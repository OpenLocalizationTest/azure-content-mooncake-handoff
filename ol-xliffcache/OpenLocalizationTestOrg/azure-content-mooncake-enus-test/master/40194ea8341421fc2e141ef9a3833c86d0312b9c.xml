{
  "nodes": [
    {
      "content": "在流分析中使用引用数据和查找表 | Azure",
      "pos": [
        28,
        51
      ]
    },
    {
      "content": "在流分析查询中使用引用数据",
      "pos": [
        71,
        84
      ]
    },
    {
      "content": "在流分析的输入流中使用引用数据或查找表",
      "pos": [
        349,
        368
      ]
    },
    {
      "content": "引用数据（也称为查找表）是一个静态的或本质上缓慢变化的有限数据集，用于执行查找或与你的数据流相关联。为了在 Azure 流分析作业中利用引用数据，你通常会在查询中使用<bpt id=\"p1\">[</bpt>引用数据联合<ept id=\"p1\">](https://msdn.microsoft.com/zh-cn/library/azure/dn949258.aspx)</ept>。流分析使用 Azure Blob 存储作为引用数据的存储层，并且通过 Azure 数据工厂，可以从",
      "pos": [
        370,
        573
      ]
    },
    {
      "content": "基于云和本地的任意数量的数据存储区",
      "pos": [
        581,
        598
      ]
    },
    {
      "content": "将引用数据转换和/或复制到 Azure Blob 存储，以用作引用数据。",
      "pos": [
        669,
        705
      ]
    },
    {
      "content": "配置引用数据",
      "pos": [
        710,
        716
      ]
    },
    {
      "pos": [
        718,
        782
      ],
      "content": "若要配置引用数据，首先需要创建一个属于<bpt id=\"p1\">**</bpt>引用数据<ept id=\"p1\">**</ept>类型的输入。下表介绍你在根据属性说明创建引用数据输入时需要提供的每个属性："
    },
    {
      "content": "属性名称",
      "pos": [
        809,
        813
      ]
    },
    {
      "content": "说明",
      "pos": [
        823,
        825
      ]
    },
    {
      "content": "输入别名",
      "pos": [
        846,
        850
      ]
    },
    {
      "content": "一个友好名称会用于作业查询，以便引用此输入。",
      "pos": [
        860,
        882
      ]
    },
    {
      "content": "存储帐户",
      "pos": [
        903,
        907
      ]
    },
    {
      "content": "存储 blob 文件的存储帐户的名称。如果其订阅与你的流分析作业相同，你可以从下拉菜单中选择它。",
      "pos": [
        917,
        965
      ]
    },
    {
      "content": "存储帐户密钥",
      "pos": [
        986,
        992
      ]
    },
    {
      "content": "与存储帐户关联的密钥。如果存储帐户的订阅与流分析作业相同，则自动填充此密钥。",
      "pos": [
        1002,
        1040
      ]
    },
    {
      "content": "存储容器",
      "pos": [
        1061,
        1065
      ]
    },
    {
      "content": "容器对存储在 Azure Blob 服务中的 blob 进行逻辑分组。将 blob 上载到 Blob 服务时，必须为该 blob 指定一个容器。",
      "pos": [
        1075,
        1147
      ]
    },
    {
      "content": "路径模式",
      "pos": [
        1168,
        1172
      ]
    },
    {
      "content": "用于对指定容器中的 blob 进行定位的文件路径。在路径中，你可以选择指定一个或多个使用以下 2 个变量的实例：",
      "pos": [
        1182,
        1238
      ]
    },
    {
      "content": "{date}、{time}",
      "pos": [
        1242,
        1255
      ]
    },
    {
      "content": "示例 1：products/{date}/{time}/product-list.csv",
      "pos": [
        1259,
        1303
      ]
    },
    {
      "content": "示例 2：products/{date}/product-list.csv",
      "pos": [
        1307,
        1344
      ]
    },
    {
      "content": "日期格式 [可选]",
      "pos": [
        1360,
        1369
      ]
    },
    {
      "content": "如果在你指定的路径模式中使用了 {date}，则可以从支持格式的下拉列表中选择组织你的文件所用的日期格式。示例：YYYY/MM/DD",
      "pos": [
        1379,
        1445
      ]
    },
    {
      "content": "时间格式 [可选]",
      "pos": [
        1466,
        1475
      ]
    },
    {
      "content": "如果在你指定的路径模式中使用了 {time}，则可以从支持格式的下拉列表中选择组织你的文件所用的时间格式。示例：HH",
      "pos": [
        1485,
        1543
      ]
    },
    {
      "content": "事件序列化格式",
      "pos": [
        1564,
        1571
      ]
    },
    {
      "content": "为确保查询按预计的方式运行，流分析需要了解你对传入数据流使用哪种序列化格式。对于引用数据，所支持的格式是 CSV 和 JSON。",
      "pos": [
        1581,
        1645
      ]
    },
    {
      "content": "编码",
      "pos": [
        1666,
        1668
      ]
    },
    {
      "content": "目前只支持 UTF-8 这种编码格式",
      "pos": [
        1678,
        1696
      ]
    },
    {
      "content": "按计划生成引用数据",
      "pos": [
        1730,
        1739
      ]
    },
    {
      "pos": [
        1741,
        2012
      ],
      "content": "如果你的引用数据是缓慢变化的数据集，则使用 {date} 和 {time} 令牌在输入配置中指定路径模式即可实现对刷新引用数据的支持。流分析将根据此路径模式选取引用数据定义。例如，使用 <ph id=\"ph1\">````\"/sample/{date}/{time}/products.csv\"````</ph> 模式时，日期格式为“YYYY-MM-DD”，时间格式为“HH:mm”，这是告诉流分析在 2015 年 4 月 16 日下午 5:30（UTC 时区）提取更新的 blob <ph id=\"ph2\">````\"/sample/2015-04-16/17:30/products.csv\"````</ph>。"
    },
    {
      "pos": [
        2016,
        2399
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>当前，流分析作业仅在计算机时间与 blob 名称中的编码时间一致时查找 blob 刷新。例如，作业将查找 2015 年 4 月 16 日 UTC 时区下午 5:30 与下午 5:30:59.9 的 /sample/2015-04-16/17:30/products.csv。当计算机时钟到达下午 5:31 时，流分析作业会停止查找 /sample/2015-04-16/17:30/products.csv，开始查找 /sample/2015-04-16/17:31/products.csv。这种情况的一个例外是，当作业需要按时重新处理数据时或第一次启动作业时。开始时，作业查找的是在指定的作业开始时间之前生成的最新 blob。这样做是为了确保在作业开始时存在非空的引用数据集。如果找不到此类数据集，则作业会失败，将向用户显示一个诊断通知。"
    },
    {
      "content": "Azure 数据工厂",
      "pos": [
        2409,
        2419
      ]
    },
    {
      "content": "可用来安排这样的任务，创建流分析更新引用数据定义所需的已更新 blob。数据工厂是一项基于云的数据集成服务，可对数据移动和转换进行安排并使其实现自动化。数据工厂支持",
      "pos": [
        2492,
        2574
      ]
    },
    {
      "content": "连接到大量基于云和本地的数据存储",
      "pos": [
        2582,
        2598
      ]
    },
    {
      "content": "以及按你指定的定期计划轻松地移动数据。有关如何将数据工厂管道设置为生成按预定义计划刷新的流分析引用数据的详细信息和分步指导，请查看此 <bpt id=\"p1\">[</bpt><ept id=\"p1\">GitHub 示例](https://github.com/Azure/Azure-DataFactory/tree/master/Samples/ReferenceDataRefreshForASAJobs)</ept>。",
      "pos": [
        2658,
        2832
      ]
    },
    {
      "content": "有关刷新引用数据的提示",
      "pos": [
        2837,
        2848
      ]
    },
    {
      "pos": [
        2856,
        2984
      ],
      "content": "覆盖引用数据 blob 不会导致流分析重新加载 blob，并且在某些情况下，它可能会导致作业失败。更改引用数据的建议方法是使用作业输入中定义的相同容器和路径模式添加新的 blob，并且使用的日期/时间<bpt id=\"p1\">**</bpt>大于<ept id=\"p1\">**</ept>序列中最后一个 blob 指定的日期/时间。"
    },
    {
      "content": "引用数据 blob 并不按 blob 的“上次修改”时间排序，而是按 blob 名称中使用 {date} 和 {time} 替换项指定的日期和时间排序。",
      "pos": [
        2989,
        3065
      ]
    },
    {
      "content": "有时，作业必须按时返回，因此不能变更或删除引用数据 blob。",
      "pos": [
        3070,
        3101
      ]
    },
    {
      "content": "获取帮助",
      "pos": [
        3106,
        3110
      ]
    },
    {
      "pos": [
        3111,
        3220
      ],
      "content": "如需进一步的帮助，请尝试我们的 <bpt id=\"p1\">[</bpt>Azure 流分析论坛<ept id=\"p1\">](https://social.msdn.microsoft.com/Forums/zh-CN/home?forum=AzureStreamAnalytics)</ept>"
    },
    {
      "content": "后续步骤",
      "pos": [
        3225,
        3229
      ]
    },
    {
      "content": "我们已经向你介绍了流分析，这是一种托管服务，适用于对物联网的数据进行流式分析。若要了解有关此服务的详细信息，请参阅：",
      "pos": [
        3230,
        3288
      ]
    },
    {
      "content": "Azure 流分析入门",
      "pos": [
        3293,
        3304
      ]
    },
    {
      "content": "缩放 Azure 流分析作业",
      "pos": [
        3363,
        3377
      ]
    },
    {
      "content": "Azure 流分析查询语言参考",
      "pos": [
        3435,
        3450
      ]
    },
    {
      "content": "Azure 流分析管理 REST API 参考",
      "pos": [
        3517,
        3540
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"在流分析中使用引用数据和查找表 | Azure\" \n    description=\"在流分析查询中使用引用数据\" \n    keywords=\"查找表, 引用数据\"\n    services=\"stream-analytics\" \n    documentationCenter=\"\" \n    authors=\"jeffstokes72\" \n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags \n    ms.service=\"stream-analytics\" \n    ms.date=\"12/04/2015\" \n    wacn.date=\"01/14/2016\"/>\n\n# 在流分析的输入流中使用引用数据或查找表\n\n引用数据（也称为查找表）是一个静态的或本质上缓慢变化的有限数据集，用于执行查找或与你的数据流相关联。为了在 Azure 流分析作业中利用引用数据，你通常会在查询中使用[引用数据联合](https://msdn.microsoft.com/zh-cn/library/azure/dn949258.aspx)。流分析使用 Azure Blob 存储作为引用数据的存储层，并且通过 Azure 数据工厂，可以从<!--[-->基于云和本地的任意数量的数据存储区<!--](/documentation/articles/data-factory-data-movement-activities)-->将引用数据转换和/或复制到 Azure Blob 存储，以用作引用数据。\n\n## 配置引用数据\n\n若要配置引用数据，首先需要创建一个属于**引用数据**类型的输入。下表介绍你在根据属性说明创建引用数据输入时需要提供的每个属性：\n\n<table>\n<tbody>\n<tr>\n<td>属性名称</td>\n<td>说明</td>\n</tr>\n<tr>\n<td>输入别名</td>\n<td>一个友好名称会用于作业查询，以便引用此输入。</td>\n</tr>\n<tr>\n<td>存储帐户</td>\n<td>存储 blob 文件的存储帐户的名称。如果其订阅与你的流分析作业相同，你可以从下拉菜单中选择它。</td>\n</tr>\n<tr>\n<td>存储帐户密钥</td>\n<td>与存储帐户关联的密钥。如果存储帐户的订阅与流分析作业相同，则自动填充此密钥。</td>\n</tr>\n<tr>\n<td>存储容器</td>\n<td>容器对存储在 Azure Blob 服务中的 blob 进行逻辑分组。将 blob 上载到 Blob 服务时，必须为该 blob 指定一个容器。</td>\n</tr>\n<tr>\n<td>路径模式</td>\n<td>用于对指定容器中的 blob 进行定位的文件路径。在路径中，你可以选择指定一个或多个使用以下 2 个变量的实例：<BR>{date}、{time}<BR>示例 1：products/{date}/{time}/product-list.csv<BR>示例 2：products/{date}/product-list.csv\n</tr>\n<tr>\n<td>日期格式 [可选]</td>\n<td>如果在你指定的路径模式中使用了 {date}，则可以从支持格式的下拉列表中选择组织你的文件所用的日期格式。示例：YYYY/MM/DD</td>\n</tr>\n<tr>\n<td>时间格式 [可选]</td>\n<td>如果在你指定的路径模式中使用了 {time}，则可以从支持格式的下拉列表中选择组织你的文件所用的时间格式。示例：HH</td>\n</tr>\n<tr>\n<td>事件序列化格式</td>\n<td>为确保查询按预计的方式运行，流分析需要了解你对传入数据流使用哪种序列化格式。对于引用数据，所支持的格式是 CSV 和 JSON。</td>\n</tr>\n<tr>\n<td>编码</td>\n<td>目前只支持 UTF-8 这种编码格式</td>\n</tr>\n</tbody>\n</table>\n\n## 按计划生成引用数据\n\n如果你的引用数据是缓慢变化的数据集，则使用 {date} 和 {time} 令牌在输入配置中指定路径模式即可实现对刷新引用数据的支持。流分析将根据此路径模式选取引用数据定义。例如，使用 ````\"/sample/{date}/{time}/products.csv\"```` 模式时，日期格式为“YYYY-MM-DD”，时间格式为“HH:mm”，这是告诉流分析在 2015 年 4 月 16 日下午 5:30（UTC 时区）提取更新的 blob ````\"/sample/2015-04-16/17:30/products.csv\"````。\n\n> [AZURE.NOTE]当前，流分析作业仅在计算机时间与 blob 名称中的编码时间一致时查找 blob 刷新。例如，作业将查找 2015 年 4 月 16 日 UTC 时区下午 5:30 与下午 5:30:59.9 的 /sample/2015-04-16/17:30/products.csv。当计算机时钟到达下午 5:31 时，流分析作业会停止查找 /sample/2015-04-16/17:30/products.csv，开始查找 /sample/2015-04-16/17:31/products.csv。这种情况的一个例外是，当作业需要按时重新处理数据时或第一次启动作业时。开始时，作业查找的是在指定的作业开始时间之前生成的最新 blob。这样做是为了确保在作业开始时存在非空的引用数据集。如果找不到此类数据集，则作业会失败，将向用户显示一个诊断通知。\n\n<!--[-->Azure 数据工厂<!--](http://azure.microsoft.com/documentation/services/data-factory/)-->可用来安排这样的任务，创建流分析更新引用数据定义所需的已更新 blob。数据工厂是一项基于云的数据集成服务，可对数据移动和转换进行安排并使其实现自动化。数据工厂支持<!--[-->连接到大量基于云和本地的数据存储<!--](/articles/data-factory-data-movement-activities.md)-->以及按你指定的定期计划轻松地移动数据。有关如何将数据工厂管道设置为生成按预定义计划刷新的流分析引用数据的详细信息和分步指导，请查看此 [GitHub 示例](https://github.com/Azure/Azure-DataFactory/tree/master/Samples/ReferenceDataRefreshForASAJobs)。\n\n## 有关刷新引用数据的提示 ##\n\n1. 覆盖引用数据 blob 不会导致流分析重新加载 blob，并且在某些情况下，它可能会导致作业失败。更改引用数据的建议方法是使用作业输入中定义的相同容器和路径模式添加新的 blob，并且使用的日期/时间**大于**序列中最后一个 blob 指定的日期/时间。\n2.  引用数据 blob 并不按 blob 的“上次修改”时间排序，而是按 blob 名称中使用 {date} 和 {time} 替换项指定的日期和时间排序。\n3.  有时，作业必须按时返回，因此不能变更或删除引用数据 blob。\n\n## 获取帮助\n如需进一步的帮助，请尝试我们的 [Azure 流分析论坛](https://social.msdn.microsoft.com/Forums/zh-CN/home?forum=AzureStreamAnalytics)\n\n## 后续步骤\n我们已经向你介绍了流分析，这是一种托管服务，适用于对物联网的数据进行流式分析。若要了解有关此服务的详细信息，请参阅：\n\n- [Azure 流分析入门](/documentation/articles/stream-analytics-get-started)\n- [缩放 Azure 流分析作业](/documentation/articles/stream-analytics-scale-jobs)\n- [Azure 流分析查询语言参考](https://msdn.microsoft.com/zh-cn/library/azure/dn834998.aspx)\n- [Azure 流分析管理 REST API 参考](https://msdn.microsoft.com/zh-cn/library/azure/dn835031.aspx)\n\n<!--Link references-->\n[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md\n[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md\n[stream.analytics.introduction]: stream-analytics-introduction.md\n[stream.analytics.get.started]: stream-analytics-get-started.md\n[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299\n[stream.analytics.rest.api.reference]: http://go.microsoft.com/fwlink/?LinkId=517301\n\n<!---HONumber=Mooncake_0104_2016-->"
}