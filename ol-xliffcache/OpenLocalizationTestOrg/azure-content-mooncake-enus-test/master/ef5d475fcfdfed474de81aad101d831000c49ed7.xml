{
  "nodes": [
    {
      "content": "了解什么是 Hive 以及如何使用 HiveQL | Azure",
      "pos": [
        27,
        59
      ]
    },
    {
      "content": "了解 Apache Hive 以及如何将它与 HDInsight 中的 Hadoop 配合使用。选择如何运行 Hive 作业，以及如何使用 HiveQL 来分析示例 Apache log4j 文件。",
      "pos": [
        78,
        177
      ]
    },
    {
      "content": "将 Hive 和 HiveQL 与 HDInsight 中的 Hadoop 配合使用以分析示例 Apache log4j 文件",
      "pos": [
        448,
        511
      ]
    },
    {
      "content": "在本教程中，你将学习如何在 HDInsight 上使用 Hadoop 中的 Apache Hive，以及选择如何运行 Hive 作业。此外，你还将了解 HiveQL 以及如何分析一个示例 Apache log4j 文件。",
      "pos": [
        591,
        701
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a id=\"why\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>什么是 Hive，为何要使用它？",
      "pos": [
        705,
        737
      ]
    },
    {
      "content": "<bpt id=\"p1\">[</bpt>Apache Hive<ept id=\"p1\">](http://hive.apache.org/)</ept> 是适用于 Hadoop 的数据仓库系统，可让你使用 HiveQL（类似于 SQL 的查询语言）来进行数据汇总、查询和数据分析。使用 Hive 能够以交互方式浏览数据，或者创建可重用的批处理作业。",
      "pos": [
        738,
        874
      ]
    },
    {
      "pos": [
        876,
        1001
      ],
      "content": "Hive 允许你在很大程度上未结构化的数据上投影结构。在定义结构后，你可以使用 Hive 来查询这些数据，而不需要具备 Java 或 MapReduce 的知识。<bpt id=\"p1\">**</bpt>HiveQL<ept id=\"p1\">**</ept>（Hive 查询语言）可让你使用类似于 T-SQL 的语句编写查询。"
    },
    {
      "pos": [
        1003,
        1269
      ],
      "content": "Hive 知道如何处理结构化和半结构化数据，例如其中的字段以特定字符分隔的文本文件。Hive 还支持对复杂或不规则的结构化数据使用自定义<bpt id=\"p1\">**</bpt>序列化程序/反序列化程序 (SerDe)<ept id=\"p1\">**</ept>。有关详细信息，请参阅<bpt id=\"p2\">[</bpt>如何将自定义 JSON SerDe 与 HDInsight 配合使用<ept id=\"p2\">](http://blogs.msdn.com/b/bigdatasupport/archive/2014/06/18/how-to-use-a-custom-json-serde-with-microsoft-azure-hdinsight.aspx)</ept>。"
    },
    {
      "pos": [
        1271,
        1359
      ],
      "content": "还可以通过<bpt id=\"p1\">**</bpt>用户定义函数的 (UDF)<ept id=\"p1\">**</ept> 扩展 Hive。UDF 允许你实现 HiveQL 中不容易建模的功能或逻辑。有关将 UDF 与 Hive 配合使用的示例，请参阅："
    },
    {
      "content": "在 HDInsight 中将 Python 与 Hive 和 Pig 配合使用",
      "pos": [
        1364,
        1403
      ]
    },
    {
      "content": "在 HDInsight 中将 C# 与 Hive 和 Pig 配合使用",
      "pos": [
        1451,
        1486
      ]
    },
    {
      "content": "如何将自定义 Hive UDF 添加到 HDInsight",
      "pos": [
        1561,
        1590
      ]
    },
    {
      "content": "Hive 内部表与外部表",
      "pos": [
        1702,
        1714
      ]
    },
    {
      "content": "以下是你需要了解的有关 Hive 内部表和外部表的一些信息：",
      "pos": [
        1716,
        1746
      ]
    },
    {
      "pos": [
        1750,
        1789
      ],
      "content": "<bpt id=\"p1\">**</bpt>CREATE TABLE<ept id=\"p1\">**</ept> 命令创建内部表。数据文件必须位于默认容器中。"
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>CREATE TABLE<ept id=\"p1\">**</ept> 命令会将数据文件移到 /hive/warehouse/",
      "pos": [
        1792,
        1836
      ]
    },
    {
      "content": "文件夹中。",
      "pos": [
        1848,
        1853
      ]
    },
    {
      "pos": [
        1856,
        1908
      ],
      "content": "<bpt id=\"p1\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p1\">**</ept> 命令创建外部表。数据文件可以位于默认容器以外的位置。"
    },
    {
      "pos": [
        1911,
        1947
      ],
      "content": "<bpt id=\"p1\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p1\">**</ept> 命令不移动数据文件。"
    },
    {
      "pos": [
        1950,
        2028
      ],
      "content": "<bpt id=\"p1\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p1\">**</ept> 命令不允许在 LOCATION 中有任何文件夹。这是本教程生成 sample.log 文件的副本的原因。"
    },
    {
      "pos": [
        2030,
        2091
      ],
      "content": "有关详细信息，请参阅 <bpt id=\"p1\">[</bpt>HDInsight：Hive 内部和外部表简介<ept id=\"p1\">][cindygross-hive-tables]</ept>。"
    },
    {
      "pos": [
        2096,
        2139
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"data\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>关于示例数据（一个 Apache log4j 文件）"
    },
    {
      "pos": [
        2141,
        2264
      ],
      "content": "本示例使用 <bpt id=\"p1\">*</bpt>log4j<ept id=\"p1\">*</ept> 示例文件，该文件存储在 Blob 存储容器的 <bpt id=\"p2\">**</bpt>/example/data/sample.log<ept id=\"p2\">**</ept> 中。该文件中的每个日志都包含一行字段，其中包含一个 <ph id=\"ph1\">`[LOG LEVEL]`</ph> 字段，用于显示类型和严重性，例如："
    },
    {
      "content": "在前面的示例中，日志级别为 ERROR。",
      "pos": [
        2345,
        2365
      ]
    },
    {
      "pos": [
        2369,
        2689
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>你也可以使用 <bpt id=\"p1\">[</bpt>Apache Log4j<ept id=\"p1\">](http://zh.wikipedia.org/wiki/Log4j)</ept> 日志记录工具生成 log4j 文件，然后将该文件上载到 Blob 容器。请参阅<bpt id=\"p2\">[</bpt>将数据上载到 HDInsight<ept id=\"p2\">](/documentation/articles/hdinsight-upload-data)</ept> 以获取相关说明。有关如何将 Azure Blob 存储与 HDInsight 配合使用的详细信息，请参阅<bpt id=\"p3\">[</bpt>将 Azure Blob 存储与 HDInsight 配合使用<ept id=\"p3\">](/documentation/articles/hdinsight-hadoop-use-blob-storage)</ept>。"
    },
    {
      "pos": [
        2691,
        2807
      ],
      "content": "示例数据存储在 HDInsight 用作默认文件系统的 Azure Blob 存储中。HDInsight 可以使用 <bpt id=\"p1\">**</bpt>wasb<ept id=\"p1\">**</ept> 前缀来访问存储在 Blob 中的文件。例如，若要访问 sample.log 文件，可使用以下语法："
    },
    {
      "pos": [
        2846,
        2934
      ],
      "content": "由于 Azure Blob 存储是 HDInsight 的默认存储，因此你也可以使用 HiveQL 中的 <bpt id=\"p1\">**</bpt>/example/data/sample.log<ept id=\"p1\">**</ept> 访问该文件。"
    },
    {
      "pos": [
        2938,
        3151
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>语法 ****wasb:///** 用于访问存储在 HDInsight 群集的默认存储容器中的文件。如果你在预配群集时指定了其他存储帐户，并且你想要访问存储在这些帐户中的文件，则可以通过指定容器名称和存储帐户地址来访问这些数据，例如：<bpt id=\"p1\">**</bpt>wasb://mycontainer@mystorage.blob.core.chinacloudapi.cn/example/data/sample.log<ept id=\"p1\">**</ept>。"
    },
    {
      "pos": [
        3155,
        3186
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"job\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>示例作业：将列投影到分隔的数据"
    },
    {
      "content": "以下 HiveQL 语句将列投影到 ****wasb:///example/data** 目录中存储的分隔数据：",
      "pos": [
        3188,
        3244
      ]
    },
    {
      "content": "在上例中，HiveQL 语句执行以下操作：",
      "pos": [
        3618,
        3639
      ]
    },
    {
      "pos": [
        3643,
        3676
      ],
      "content": "<bpt id=\"p1\">**</bpt>DROP TABLE<ept id=\"p1\">**</ept>：删除表和数据文件（如果该表已存在）。"
    },
    {
      "pos": [
        3679,
        3759
      ],
      "content": "<bpt id=\"p1\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p1\">**</ept>：在 Hive 中创建新的<bpt id=\"p2\">**</bpt>外部<ept id=\"p2\">**</ept>表。外部表只会在 Hive 中存储表定义；数据以原始格式保留在原始位置。"
    },
    {
      "pos": [
        3762,
        3815
      ],
      "content": "<bpt id=\"p1\">**</bpt>ROW FORMAT<ept id=\"p1\">**</ept>：告知 Hive 如何设置数据的格式。在此情况下，每个日志中的字段以空格分隔。"
    },
    {
      "pos": [
        3818,
        3921
      ],
      "content": "<bpt id=\"p1\">**</bpt>STORED AS TEXTFILE LOCATION<ept id=\"p1\">**</ept>：让 Hive 知道数据的存储位置（example/data 目录），并且数据已存储为文本。数据可以在一个文件中，也可以分散在目录的多个文件内。"
    },
    {
      "pos": [
        3924,
        3994
      ],
      "content": "<bpt id=\"p1\">**</bpt>SELECT<ept id=\"p1\">**</ept>：选择其列 <bpt id=\"p2\">**</bpt>t4<ept id=\"p2\">**</ept> 包含值 <bpt id=\"p3\">**</bpt>[ERROR]<ept id=\"p3\">**</ept> 的所有行计数。这应会返回值 <bpt id=\"p4\">**</bpt>3<ept id=\"p4\">**</ept>，因为有三个行包含此值。"
    },
    {
      "pos": [
        3997,
        4126
      ],
      "content": "<bpt id=\"p1\">**</bpt>INPUT\\_\\_FILE\\_\\_NAME LIKE '%.log'<ept id=\"p1\">**</ept> - 告诉 Hive，我们只应返回以 .log 结尾的文件中的数据。此项将搜索限定于包含数据的 sample.log 文件，使搜索不会返回与所定义架构不符的其他示例数据文件中的数据。"
    },
    {
      "pos": [
        4130,
        4223
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>当你预期以外部源更新基础数据（例如自动化数据上载过程），或以其他 MapReduce 操作更新基础数据，但希望 Hive 查询始终使用最新数据时，必须使用外部表。"
    },
    {
      "content": "删除外部表<bpt id=\"p1\">**</bpt>不会<ept id=\"p1\">**</ept>删除数据，只会删除表定义。",
      "pos": [
        4229,
        4253
      ]
    },
    {
      "pos": [
        4255,
        4278
      ],
      "content": "创建外部表后，使用以下语句创建<bpt id=\"p1\">**</bpt>内部<ept id=\"p1\">**</ept>表。"
    },
    {
      "content": "这些语句将执行以下操作：",
      "pos": [
        4566,
        4578
      ]
    },
    {
      "pos": [
        4582,
        4689
      ],
      "content": "<bpt id=\"p1\">**</bpt>CREATE TABLE IF NOT EXISTS<ept id=\"p1\">**</ept>：创建表（如果该表尚不存在）。由于未使用 <bpt id=\"p2\">**</bpt>EXTERNAL<ept id=\"p2\">**</ept> 关键字，因此这是一个内部表，它存储在 Hive 数据仓库中并完全受 Hive 的管理。"
    },
    {
      "pos": [
        4692,
        4754
      ],
      "content": "<bpt id=\"p1\">**</bpt>STORED AS ORC<ept id=\"p1\">**</ept>：以优化行纵栏表 (ORC) 格式存储数据。这是高度优化且有效的 Hive 数据存储格式。"
    },
    {
      "pos": [
        4757,
        4851
      ],
      "content": "<bpt id=\"p1\">**</bpt>INSERT OVERWRITE ...SELECT<ept id=\"p1\">**</ept>：从包含 <bpt id=\"p2\">**</bpt>[ERROR]<ept id=\"p2\">**</ept> 的 <bpt id=\"p3\">**</bpt>log4jLogs<ept id=\"p3\">**</ept> 表中选择行，然后将数据插入 <bpt id=\"p4\">**</bpt>errorLogs<ept id=\"p4\">**</ept> 表中。"
    },
    {
      "pos": [
        4855,
        4889
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>与外部表不同，删除内部表会同时删除基础数据。"
    },
    {
      "pos": [
        4893,
        4930
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"usetez\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>使用 Apache Tez 提高性能"
    },
    {
      "pos": [
        4932,
        5035
      ],
      "content": "<bpt id=\"p1\">[</bpt>Apache Tez<ept id=\"p1\">](http://tez.apache.org)</ept> 是可让数据密集型应用程序（例如 Hive）大规模高效运行的框架。在最新版的 HDInsight 中，Hive 支持在 Tez 上运行。"
    },
    {
      "pos": [
        5039,
        5132
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>对于基于 Windows 的 HDInsight 群集来说，Tez 目前默认处于关闭状态，必须将其启用。若要充分利用 Tez，你必须设置 Hive 查询的以下值："
    },
    {
      "pos": [
        5135,
        5321
      ],
      "content": "<p>```set hive.execution.engine=tez;```\n<p>你可为每个查询提交此值，只需将它放置在查询的开头即可。你也可以在创建群集时设置配置值，而在群集上将此值默认为打开。可以在[预配 HDInsight 群集](/documentation/articles/hdinsight-provision-clusters-v1)中找到详细信息。",
      "leadings": [
        "",
        ">"
      ],
      "nodes": [
        {
          "content": "你可为每个查询提交此值，只需将它放置在查询的开头即可。你也可以在创建群集时设置配置值，而在群集上将此值默认为打开。可以在<bpt id=\"p1\">[</bpt><ept id=\"p1\">预配 HDInsight 群集](/documentation/articles/hdinsight-provision-clusters-v1)</ept>中找到详细信息。",
          "pos": [
            43,
            185
          ]
        }
      ]
    },
    {
      "pos": [
        5323,
        5420
      ],
      "content": "<bpt id=\"p1\">[</bpt>Tez 上的 Hive 设计文档<ept id=\"p1\">](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez)</ept>包含实现选项和优化配置的详细信息。"
    },
    {
      "pos": [
        5425,
        5457
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"run\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>选择如何运行 HiveQL 作业"
    },
    {
      "content": "HDInsight 可以使用各种方法运行 HiveQL 作业。使用下表来确定哪种方法最适合你，然后按链接进行演练。",
      "pos": [
        5459,
        5516
      ]
    },
    {
      "pos": [
        5520,
        5538
      ],
      "content": "<bpt id=\"p1\">**</bpt>使用此方法<ept id=\"p1\">**</ept>，如果你想要..."
    },
    {
      "pos": [
        5541,
        5557
      ],
      "content": "...<bpt id=\"p1\">**</bpt>交互式<ept id=\"p1\">**</ept> shell"
    },
    {
      "pos": [
        5560,
        5570
      ],
      "content": "...<bpt id=\"p1\">**</bpt>批处理<ept id=\"p1\">**</ept>"
    },
    {
      "pos": [
        5573,
        5589
      ],
      "content": "...使用此<bpt id=\"p1\">**</bpt>群集操作系统<ept id=\"p1\">**</ept>"
    },
    {
      "pos": [
        5592,
        5608
      ],
      "content": "...从此<bpt id=\"p1\">**</bpt>客户端操作系统<ept id=\"p1\">**</ept>"
    },
    {
      "content": "Curl",
      "pos": [
        5841,
        5845
      ]
    },
    {
      "content": "&amp;nbsp;",
      "pos": [
        5905,
        5911
      ]
    },
    {
      "content": "✔",
      "pos": [
        5914,
        5915
      ]
    },
    {
      "content": "Windows",
      "pos": [
        5918,
        5925
      ]
    },
    {
      "content": "Windows",
      "pos": [
        5928,
        5935
      ]
    },
    {
      "content": "查询控制台",
      "pos": [
        5941,
        5946
      ]
    },
    {
      "content": "&amp;nbsp;",
      "pos": [
        6015,
        6021
      ]
    },
    {
      "content": "✔",
      "pos": [
        6024,
        6025
      ]
    },
    {
      "content": "Windows",
      "pos": [
        6028,
        6035
      ]
    },
    {
      "content": "任何（基于浏览器）",
      "pos": [
        6038,
        6047
      ]
    },
    {
      "content": "HDInsight tools for Visual Studio",
      "pos": [
        6053,
        6086
      ]
    },
    {
      "content": "&amp;nbsp;",
      "pos": [
        6155,
        6161
      ]
    },
    {
      "content": "✔",
      "pos": [
        6164,
        6165
      ]
    },
    {
      "content": "Windows",
      "pos": [
        6168,
        6175
      ]
    },
    {
      "content": "Windows",
      "pos": [
        6178,
        6185
      ]
    },
    {
      "content": "Windows PowerShell",
      "pos": [
        6191,
        6209
      ]
    },
    {
      "content": "&amp;nbsp;",
      "pos": [
        6275,
        6281
      ]
    },
    {
      "content": "✔",
      "pos": [
        6284,
        6285
      ]
    },
    {
      "content": "Windows",
      "pos": [
        6288,
        6295
      ]
    },
    {
      "content": "Windows",
      "pos": [
        6298,
        6305
      ]
    },
    {
      "content": "远程桌面",
      "pos": [
        6311,
        6315
      ]
    },
    {
      "content": "✔",
      "pos": [
        6385,
        6386
      ]
    },
    {
      "content": "✔",
      "pos": [
        6389,
        6390
      ]
    },
    {
      "content": "Windows",
      "pos": [
        6393,
        6400
      ]
    },
    {
      "content": "Windows",
      "pos": [
        6403,
        6410
      ]
    },
    {
      "content": "使用本地 SQL Server Integration Services 在 Azure HDInsight 上运行 Hive 作业",
      "pos": [
        6417,
        6483
      ]
    },
    {
      "content": "你也可以使用 SQL Server Integration Services (SSIS) 来运行 Hive 作业。Azure Feature Pack for SSIS 提供适用于 HDInsight 上的 Hive 作业的以下组件。",
      "pos": [
        6485,
        6603
      ]
    },
    {
      "content": "Azure HDInsight Hive 任务",
      "pos": [
        6609,
        6632
      ]
    },
    {
      "content": "Azure 订阅连接管理器",
      "pos": [
        6647,
        6660
      ]
    },
    {
      "pos": [
        6683,
        6737
      ],
      "content": "在<bpt id=\"p1\">[</bpt>此处<ept id=\"p1\">][ssispack]</ept>了解有关 Azure Feature Pack for SSIS 的详细信息。"
    },
    {
      "pos": [
        6742,
        6768
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>后续步骤"
    },
    {
      "content": "现在，你已了解什么是 Hive，以及如何将它与 HDInsight 中的 Hadoop 配合使用，请使用以下链接来学习 Azure HDInsight 的其他用法。",
      "pos": [
        6770,
        6852
      ]
    },
    {
      "content": "将数据上载到 HDInsight",
      "pos": [
        6858,
        6874
      ]
    },
    {
      "content": "将 Pig 与 HDInsight 配合使用",
      "pos": [
        6902,
        6924
      ]
    },
    {
      "content": "将 MapReduce 作业与 HDInsight 配合使用",
      "pos": [
        6948,
        6978
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"了解什么是 Hive 以及如何使用 HiveQL | Azure\"\n    description=\"了解 Apache Hive 以及如何将它与 HDInsight 中的 Hadoop 配合使用。选择如何运行 Hive 作业，以及如何使用 HiveQL 来分析示例 Apache log4j 文件。\"\n    keywords=\"hiveql,什么是 hive\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"Blackmist\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.date=\"12/03/2015\"\n    wacn.date=\"01/14/2016\"/>\n\n# 将 Hive 和 HiveQL 与 HDInsight 中的 Hadoop 配合使用以分析示例 Apache log4j 文件\n\n[AZURE.INCLUDE [hive-selector](../includes/hdinsight-selector-use-hive.md)]\n\n\n在本教程中，你将学习如何在 HDInsight 上使用 Hadoop 中的 Apache Hive，以及选择如何运行 Hive 作业。此外，你还将了解 HiveQL 以及如何分析一个示例 Apache log4j 文件。\n\n##<a id=\"why\"></a>什么是 Hive，为何要使用它？\n[Apache Hive](http://hive.apache.org/) 是适用于 Hadoop 的数据仓库系统，可让你使用 HiveQL（类似于 SQL 的查询语言）来进行数据汇总、查询和数据分析。使用 Hive 能够以交互方式浏览数据，或者创建可重用的批处理作业。\n\nHive 允许你在很大程度上未结构化的数据上投影结构。在定义结构后，你可以使用 Hive 来查询这些数据，而不需要具备 Java 或 MapReduce 的知识。**HiveQL**（Hive 查询语言）可让你使用类似于 T-SQL 的语句编写查询。\n\nHive 知道如何处理结构化和半结构化数据，例如其中的字段以特定字符分隔的文本文件。Hive 还支持对复杂或不规则的结构化数据使用自定义**序列化程序/反序列化程序 (SerDe)**。有关详细信息，请参阅[如何将自定义 JSON SerDe 与 HDInsight 配合使用](http://blogs.msdn.com/b/bigdatasupport/archive/2014/06/18/how-to-use-a-custom-json-serde-with-microsoft-azure-hdinsight.aspx)。\n\n还可以通过**用户定义函数的 (UDF)** 扩展 Hive。UDF 允许你实现 HiveQL 中不容易建模的功能或逻辑。有关将 UDF 与 Hive 配合使用的示例，请参阅：\n\n* [在 HDInsight 中将 Python 与 Hive 和 Pig 配合使用](/documentation/articles/hdinsight-python)\n\n* [在 HDInsight 中将 C# 与 Hive 和 Pig 配合使用](/documentation/articles/hdinsight-hadoop-hive-pig-udf-dotnet-csharp)\n\n* [如何将自定义 Hive UDF 添加到 HDInsight](http://blogs.msdn.com/b/bigdatasupport/archive/2014/01/14/how-to-add-custom-hive-udfs-to-hdinsight.aspx)\n\n\n## Hive 内部表与外部表\n\n以下是你需要了解的有关 Hive 内部表和外部表的一些信息：\n\n- **CREATE TABLE** 命令创建内部表。数据文件必须位于默认容器中。\n- **CREATE TABLE** 命令会将数据文件移到 /hive/warehouse/<TableName> 文件夹中。\n- **CREATE EXTERNAL TABLE** 命令创建外部表。数据文件可以位于默认容器以外的位置。\n- **CREATE EXTERNAL TABLE** 命令不移动数据文件。\n- **CREATE EXTERNAL TABLE** 命令不允许在 LOCATION 中有任何文件夹。这是本教程生成 sample.log 文件的副本的原因。\n\n有关详细信息，请参阅 [HDInsight：Hive 内部和外部表简介][cindygross-hive-tables]。\n\n\n##<a id=\"data\"></a>关于示例数据（一个 Apache log4j 文件）\n\n本示例使用 *log4j* 示例文件，该文件存储在 Blob 存储容器的 **/example/data/sample.log** 中。该文件中的每个日志都包含一行字段，其中包含一个 `[LOG LEVEL]` 字段，用于显示类型和严重性，例如：\n\n    2012-02-03 20:26:41 SampleClass3 [ERROR] verbose detail for id 1527353937\n\n在前面的示例中，日志级别为 ERROR。\n\n> [AZURE.NOTE]你也可以使用 [Apache Log4j](http://zh.wikipedia.org/wiki/Log4j) 日志记录工具生成 log4j 文件，然后将该文件上载到 Blob 容器。请参阅[将数据上载到 HDInsight](/documentation/articles/hdinsight-upload-data) 以获取相关说明。有关如何将 Azure Blob 存储与 HDInsight 配合使用的详细信息，请参阅[将 Azure Blob 存储与 HDInsight 配合使用](/documentation/articles/hdinsight-hadoop-use-blob-storage)。\n\n示例数据存储在 HDInsight 用作默认文件系统的 Azure Blob 存储中。HDInsight 可以使用 **wasb** 前缀来访问存储在 Blob 中的文件。例如，若要访问 sample.log 文件，可使用以下语法：\n\n    wasb:///example/data/sample.log\n\n由于 Azure Blob 存储是 HDInsight 的默认存储，因此你也可以使用 HiveQL 中的 **/example/data/sample.log** 访问该文件。\n\n> [AZURE.NOTE]语法 ****wasb:///** 用于访问存储在 HDInsight 群集的默认存储容器中的文件。如果你在预配群集时指定了其他存储帐户，并且你想要访问存储在这些帐户中的文件，则可以通过指定容器名称和存储帐户地址来访问这些数据，例如：**wasb://mycontainer@mystorage.blob.core.chinacloudapi.cn/example/data/sample.log**。\n\n##<a id=\"job\"></a>示例作业：将列投影到分隔的数据\n\n以下 HiveQL 语句将列投影到 ****wasb:///example/data** 目录中存储的分隔数据：\n\n    DROP TABLE log4jLogs;\n    CREATE EXTERNAL TABLE log4jLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string)\n    ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '\n    STORED AS TEXTFILE LOCATION 'wasb:///example/data/';\n    SELECT t4 AS sev, COUNT(*) AS count FROM log4jLogs WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log' GROUP BY t4;\n\n在上例中，HiveQL 语句执行以下操作：\n\n* **DROP TABLE**：删除表和数据文件（如果该表已存在）。\n* **CREATE EXTERNAL TABLE**：在 Hive 中创建新的**外部**表。外部表只会在 Hive 中存储表定义；数据以原始格式保留在原始位置。\n* **ROW FORMAT**：告知 Hive 如何设置数据的格式。在此情况下，每个日志中的字段以空格分隔。\n* **STORED AS TEXTFILE LOCATION**：让 Hive 知道数据的存储位置（example/data 目录），并且数据已存储为文本。数据可以在一个文件中，也可以分散在目录的多个文件内。\n* **SELECT**：选择其列 **t4** 包含值 **[ERROR]** 的所有行计数。这应会返回值 **3**，因为有三个行包含此值。\n* **INPUT\\_\\_FILE\\_\\_NAME LIKE '%.log'** - 告诉 Hive，我们只应返回以 .log 结尾的文件中的数据。此项将搜索限定于包含数据的 sample.log 文件，使搜索不会返回与所定义架构不符的其他示例数据文件中的数据。\n\n> [AZURE.NOTE]当你预期以外部源更新基础数据（例如自动化数据上载过程），或以其他 MapReduce 操作更新基础数据，但希望 Hive 查询始终使用最新数据时，必须使用外部表。\n> <p>删除外部表**不会**删除数据，只会删除表定义。\n\n创建外部表后，使用以下语句创建**内部**表。\n\n    CREATE TABLE IF NOT EXISTS errorLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string)\n    STORED AS ORC;\n    INSERT OVERWRITE TABLE errorLogs\n    SELECT t1, t2, t3, t4, t5, t6, t7 FROM log4jLogs WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log';\n\n这些语句将执行以下操作：\n\n* **CREATE TABLE IF NOT EXISTS**：创建表（如果该表尚不存在）。由于未使用 **EXTERNAL** 关键字，因此这是一个内部表，它存储在 Hive 数据仓库中并完全受 Hive 的管理。\n* **STORED AS ORC**：以优化行纵栏表 (ORC) 格式存储数据。这是高度优化且有效的 Hive 数据存储格式。\n* **INSERT OVERWRITE ...SELECT**：从包含 **[ERROR]** 的 **log4jLogs** 表中选择行，然后将数据插入 **errorLogs** 表中。\n\n> [AZURE.NOTE]与外部表不同，删除内部表会同时删除基础数据。\n\n##<a id=\"usetez\"></a>使用 Apache Tez 提高性能\n\n[Apache Tez](http://tez.apache.org) 是可让数据密集型应用程序（例如 Hive）大规模高效运行的框架。在最新版的 HDInsight 中，Hive 支持在 Tez 上运行。\n\n> [AZURE.NOTE]对于基于 Windows 的 HDInsight 群集来说，Tez 目前默认处于关闭状态，必须将其启用。若要充分利用 Tez，你必须设置 Hive 查询的以下值：\n> <p>```set hive.execution.engine=tez;```\n><p>你可为每个查询提交此值，只需将它放置在查询的开头即可。你也可以在创建群集时设置配置值，而在群集上将此值默认为打开。可以在[预配 HDInsight 群集](/documentation/articles/hdinsight-provision-clusters-v1)中找到详细信息。\n\n[Tez 上的 Hive 设计文档](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez)包含实现选项和优化配置的详细信息。\n\n\n##<a id=\"run\"></a>选择如何运行 HiveQL 作业\n\nHDInsight 可以使用各种方法运行 HiveQL 作业。使用下表来确定哪种方法最适合你，然后按链接进行演练。\n\n| **使用此方法**，如果你想要... | ...**交互式** shell | ...**批处理** | ...使用此**群集操作系统** | ...从此**客户端操作系统** |\n|:--------------------------------------------------------------------------------|:---------------------------:|:-----------------------:|:------------------------------------------|:-----------------------------------------|\n| [Curl](/documentation/articles/hdinsight-hadoop-use-hive-curl) | &nbsp; | ✔ | Windows | Windows |\n| [查询控制台](/documentation/articles/hdinsight-hadoop-use-hive-query-console) | &nbsp; | ✔ | Windows | 任何（基于浏览器） |\n| [HDInsight tools for Visual Studio](/documentation/articles/hdinsight-hadoop-use-hive-visual-studio) | &nbsp; | ✔ | Windows | Windows |\n| [Windows PowerShell](/documentation/articles/hdinsight-hadoop-use-hive-powershell) | &nbsp; | ✔ | Windows | Windows |\n| [远程桌面](/documentation/articles/hdinsight-hadoop-use-hive-remote-desktop) | ✔ | ✔ | Windows | Windows |\n\n## 使用本地 SQL Server Integration Services 在 Azure HDInsight 上运行 Hive 作业\n\n你也可以使用 SQL Server Integration Services (SSIS) 来运行 Hive 作业。Azure Feature Pack for SSIS 提供适用于 HDInsight 上的 Hive 作业的以下组件。\n\n\n- [Azure HDInsight Hive 任务][hivetask]\n- [Azure 订阅连接管理器][connectionmanager]\n\n\n在[此处][ssispack]了解有关 Azure Feature Pack for SSIS 的详细信息。\n\n\n##<a id=\"nextsteps\"></a>后续步骤\n\n现在，你已了解什么是 Hive，以及如何将它与 HDInsight 中的 Hadoop 配合使用，请使用以下链接来学习 Azure HDInsight 的其他用法。\n\n\n- [将数据上载到 HDInsight][hdinsight-upload-data]\n- [将 Pig 与 HDInsight 配合使用][hdinsight-use-pig]\n- [将 MapReduce 作业与 HDInsight 配合使用][hdinsight-use-mapreduce]\n\n[check]: ./media/hdinsight-use-hive/hdi.checkmark.png\n\n[1]: /documentation/articles/hdinsight-hadoop-visual-studio-tools-get-started\n\n[hdinsight-sdk-documentation]: http://msdn.microsoft.com/zh-cn/library/dn479185.aspx\n\n[azure-purchase-options]: /pricing/overview/\n[azure-member-offers]: /pricing/member-offers/\n[azure-trial]: /pricing/1rmb-trial/\n\n[apache-tez]: http://tez.apache.org\n[apache-hive]: http://hive.apache.org/\n[apache-log4j]: http://zh.wikipedia.org/wiki/Log4j\n[hive-on-tez-wiki]: https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez\n[import-to-excel]: /documentation/articles/hdinsight-connect-excel-power-query/\n[hivetask]: http://msdn.microsoft.com/zh-cn/library/mt146771(v=sql.120).aspx\n[connectionmanager]: http://msdn.microsoft.com/zh-cn/library/mt146773(v=sql.120).aspx\n[ssispack]: http://msdn.microsoft.com/zh-cn/library/mt146770(v=sql.120).aspx\n\n[hdinsight-use-pig]: /documentation/articles/hdinsight-use-pig\n[hdinsight-use-oozie]: /documentation/articles/hdinsight-use-oozie\n[hdinsight-analyze-flight-data]: /documentation/articles/hdinsight-analyze-flight-delay-data\n[hdinsight-use-mapreduce]: /documentation/articles/hdinsight-use-mapreduce\n\n\n[hdinsight-storage]: /documentation/articles/hdinsight-hadoop-use-blob-storage\n\n[hdinsight-provision]: /documentation/articles/hdinsight-provision-clusters-v1\n[hdinsight-submit-jobs]: /documentation/articles/hdinsight-submit-hadoop-jobs-programmatically\n[hdinsight-upload-data]: /documentation/articles/hdinsight-upload-data\n[hdinsight-get-started]: /documentation/articles/hdinsight-hadoop-tutorial-get-started-windows\n\n[Powershell-install-configure]: /documentation/articles/powershell-install-configure\n[powershell-here-strings]: http://technet.microsoft.com/zh-cn/library/ee692792.aspx\n\n[image-hdi-hive-powershell]: ./media/hdinsight-use-hive/HDI.HIVE.PowerShell.png\n[img-hdi-hive-powershell-output]: ./media/hdinsight-use-hive/HDI.Hive.PowerShell.Output.png\n[image-hdi-hive-architecture]: ./media/hdinsight-use-hive/HDI.Hive.Architecture.png\n\n\n[cindygross-hive-tables]: http://blogs.msdn.com/b/cindygross/archive/2013/02/06/hdinsight-hive-internal-and-external-tables-intro.aspx\n\n<!---HONumber=Mooncake_0104_2016-->"
}