{
  "nodes": [
    {
      "content": "在 HDInsight 中使用 Hadoop Pig | Azure",
      "pos": [
        26,
        60
      ]
    },
    {
      "content": "了解如何将 Pig 与 HDInsight 上的 Hadoop 配合使用。",
      "pos": [
        78,
        115
      ]
    },
    {
      "content": "将 Pig 与 HDInsight 上的 Hadoop 配合使用",
      "pos": [
        350,
        382
      ]
    },
    {
      "pos": [
        459,
        599
      ],
      "content": "<bpt id=\"p1\">[</bpt>Apache Pig<ept id=\"p1\">](http://pig.apache.org/)</ept> 是一个平台，可以使用名为 <bpt id=\"p2\">*</bpt>Pig Latin<ept id=\"p2\">*</ept> 的过程语言，为 Hadoop 创建程序。Pig 可以替代 Java 来创建 <bpt id=\"p3\">*</bpt>MapReduce<ept id=\"p3\">*</ept> 解决方案，已包括在 Azure HDInsight 中。"
    },
    {
      "content": "在本文中，你将了解如何将 Pig 与 hdinsight 配合使用。",
      "pos": [
        601,
        635
      ]
    },
    {
      "pos": [
        639,
        664
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"why\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>为何使用 Pig？"
    },
    {
      "content": "在 Hadoop 中使用 MapReduce 来处理数据时，其中一项难题是仅使用映射和化简函数来实现处理逻辑。进行复杂的处理时，你通常必须将处理分解成多个 MapReduce 操作，这些操作合在一起即可获得所需的结果。",
      "pos": [
        666,
        775
      ]
    },
    {
      "content": "使用 Pig，你可以将处理定义成一系列转换，相关数据经过这些转换即可生成所需的输出，不必只使用映射和化简函数。",
      "pos": [
        777,
        832
      ]
    },
    {
      "content": "使用 Pig Latin 语言，你可以通过一个或多个转换从原始输入描述数据流，以便生成所需的输出。Pig Latin 程序遵循下述常规模式：",
      "pos": [
        834,
        904
      ]
    },
    {
      "pos": [
        908,
        929
      ],
      "content": "<bpt id=\"p1\">**</bpt>加载<ept id=\"p1\">**</ept>：从文件系统中读取要操作的数据"
    },
    {
      "pos": [
        932,
        944
      ],
      "content": "<bpt id=\"p1\">**</bpt>转换<ept id=\"p1\">**</ept>：操作该数据"
    },
    {
      "pos": [
        947,
        976
      ],
      "content": "<bpt id=\"p1\">**</bpt>转储或存储<ept id=\"p1\">**</ept>：将数据输出到屏幕或将其存储后再进行处理"
    },
    {
      "content": "Pig Latin 还支持使用用户定义函数 (UDF) 来调用外部组件，以便实现难以在 Pig Latin 中建模的逻辑。",
      "pos": [
        978,
        1039
      ]
    },
    {
      "pos": [
        1041,
        1212
      ],
      "content": "有关 Pig Latin 的详细信息，请参阅 <bpt id=\"p1\">[</bpt>Pig Latin 参考手册 1<ept id=\"p1\">](http://pig.apache.org/docs/r0.7.0/piglatin_ref1.html)</ept> 和 <bpt id=\"p2\">[</bpt>Pig Latin 参考手册 2<ept id=\"p2\">](http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html)</ept>。"
    },
    {
      "content": "如需通过 Pig 使用 UDF 的示例，请参阅以下文档：",
      "pos": [
        1214,
        1242
      ]
    },
    {
      "pos": [
        1246,
        1371
      ],
      "content": "<bpt id=\"p1\">[</bpt>在 HDInsight 中通过 Pig 使用 DataFu<ept id=\"p1\">](/documentation/articles/hdinsight-hadoop-use-pig-datafu-udf)</ept> - DataFu 是由 Apache 维护的有用 UDF 的集合"
    },
    {
      "content": "在 HDInsight 中将 Python 与 Pig 和 Hive 配合使用",
      "pos": [
        1376,
        1415
      ]
    },
    {
      "content": "在 HDInsight 中将 C# 与 Hive 和 Pig 配合使用",
      "pos": [
        1463,
        1498
      ]
    },
    {
      "pos": [
        1572,
        1595
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"data\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>关于示例数据"
    },
    {
      "pos": [
        1597,
        1720
      ],
      "content": "本示例使用 <bpt id=\"p1\">*</bpt>log4j<ept id=\"p1\">*</ept> 示例文件，该文件存储在 Blob 存储容器的 <bpt id=\"p2\">**</bpt>/example/data/sample.log<ept id=\"p2\">**</ept> 中。该文件中的每个日志都包含一行字段，其中包含一个 <ph id=\"ph1\">`[LOG LEVEL]`</ph> 字段，用于显示类型和严重性，例如："
    },
    {
      "content": "在前面的示例中，日志级别为 ERROR。",
      "pos": [
        1801,
        1821
      ]
    },
    {
      "pos": [
        1825,
        2145
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>你还可以使用 <bpt id=\"p1\">[</bpt>Apache Log4j<ept id=\"p1\">](http://zh.wikipedia.org/wiki/Log4j)</ept> 日志记录工具来生成 log4j 文件，然后将该文件上载到 Blob。请参阅<bpt id=\"p2\">[</bpt>将数据上载到 HDInsight<ept id=\"p2\">](/documentation/articles/hdinsight-upload-data)</ept> 以获取相关说明。有关如何将 Azure 存储空间中的 Blob 用于 HDInsight 的详细信息，请参阅<bpt id=\"p3\">[</bpt>将 Azure Blob 存储与 HDInsight 配合使用<ept id=\"p3\">](/documentation/articles/hdinsight-hadoop-use-blob-storage)</ept>。"
    },
    {
      "pos": [
        2147,
        2279
      ],
      "content": "示例数据存储在 Azure Blob 存储中，HDInsight 可以将该存储用作 Hadoop 群集的默认文件系统。HDInsight 可以使用 <bpt id=\"p1\">**</bpt>wasb<ept id=\"p1\">**</ept> 前缀来访问存储在 Blob 中的文件。例如，若要访问 sample.log 文件，可使用以下语法："
    },
    {
      "pos": [
        2318,
        2400
      ],
      "content": "由于 WASB 是 HDInsight 的默认存储，你也可以使用 Pig Latin 中的 <bpt id=\"p1\">**</bpt>/example/data/sample.log<ept id=\"p1\">**</ept> 来访问该文件。"
    },
    {
      "pos": [
        2404,
        2619
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>语法 ****wasb:///** 用于访问存储在 HDInsight 群集的默认存储容器中的文件。如果你在预配群集时指定了其他存储帐户，并且你想要访问存储在这些帐户中的文件，则可以通过指定容器名称和存储帐户地址来访问这些数据，例如：****wasb://mycontainer@mystorage.blob.core.chinacloudapi.cn/example/data/sample.log**。"
    },
    {
      "pos": [
        2624,
        2646
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"job\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>关于示例作业"
    },
    {
      "pos": [
        2648,
        2752
      ],
      "content": "下面的 Pig Latin 作业从 HDInsight 群集的默认存储加载 <bpt id=\"p1\">**</bpt>sample.log<ept id=\"p1\">**</ept> 文件。然后，它会执行一系列转换，以便对输入数据中出现的每个日志级别进行计数。结果转储到 STDOUT。"
    },
    {
      "content": "下同详细显示了每个转换对数据的影响。",
      "pos": [
        3201,
        3219
      ]
    },
    {
      "content": "转换的图形表示形式",
      "pos": [
        3223,
        3232
      ]
    },
    {
      "pos": [
        3272,
        3303
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"run\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>运行 Pig Latin 作业"
    },
    {
      "content": "HDInsight 可以使用各种方法来运行 Pig Latin 作业。使用下表来确定哪种方法最适合你，然后按链接进行演练。",
      "pos": [
        3305,
        3366
      ]
    },
    {
      "pos": [
        3370,
        3388
      ],
      "content": "<bpt id=\"p1\">**</bpt>使用此方法<ept id=\"p1\">**</ept>，如果你想要..."
    },
    {
      "pos": [
        3391,
        3407
      ],
      "content": "...<bpt id=\"p1\">**</bpt>交互式<ept id=\"p1\">**</ept> shell"
    },
    {
      "pos": [
        3410,
        3420
      ],
      "content": "...<bpt id=\"p1\">**</bpt>批处理<ept id=\"p1\">**</ept>"
    },
    {
      "pos": [
        3423,
        3439
      ],
      "content": "...使用此<bpt id=\"p1\">**</bpt>群集操作系统<ept id=\"p1\">**</ept>"
    },
    {
      "pos": [
        3442,
        3458
      ],
      "content": "...从此<bpt id=\"p1\">**</bpt>客户端操作系统<ept id=\"p1\">**</ept>"
    },
    {
      "content": "Curl",
      "pos": [
        3673,
        3677
      ]
    },
    {
      "content": "&amp;nbsp;",
      "pos": [
        3736,
        3742
      ]
    },
    {
      "content": "✔",
      "pos": [
        3745,
        3746
      ]
    },
    {
      "content": "Windows",
      "pos": [
        3749,
        3756
      ]
    },
    {
      "content": "Windows",
      "pos": [
        3759,
        3766
      ]
    },
    {
      "content": ".NET SDK for Hadoop",
      "pos": [
        3772,
        3791
      ]
    },
    {
      "content": "&amp;nbsp;",
      "pos": [
        3859,
        3865
      ]
    },
    {
      "content": "✔",
      "pos": [
        3868,
        3869
      ]
    },
    {
      "content": "Windows",
      "pos": [
        3872,
        3879
      ]
    },
    {
      "content": "Windows（暂时）",
      "pos": [
        3882,
        3893
      ]
    },
    {
      "content": "Windows PowerShell",
      "pos": [
        3899,
        3917
      ]
    },
    {
      "content": "&amp;nbsp;",
      "pos": [
        3982,
        3988
      ]
    },
    {
      "content": "✔",
      "pos": [
        3991,
        3992
      ]
    },
    {
      "content": "Windows",
      "pos": [
        3995,
        4002
      ]
    },
    {
      "content": "Windows",
      "pos": [
        4005,
        4012
      ]
    },
    {
      "content": "远程桌面",
      "pos": [
        4018,
        4022
      ]
    },
    {
      "content": "✔",
      "pos": [
        4091,
        4092
      ]
    },
    {
      "content": "✔",
      "pos": [
        4095,
        4096
      ]
    },
    {
      "content": "Windows",
      "pos": [
        4099,
        4106
      ]
    },
    {
      "content": "Windows",
      "pos": [
        4109,
        4116
      ]
    },
    {
      "content": "使用本地 SQL Server Integration Services 在 Azure HDInsight 上运行 Pig 作业",
      "pos": [
        4124,
        4189
      ]
    },
    {
      "content": "你也可以使用 SQL Server Integration Services (SSIS) 来运行 Pig 作业。Azure Feature Pack for SSIS 提供适用于 HDInsight 上的 Pig 作业的以下组件。",
      "pos": [
        4191,
        4307
      ]
    },
    {
      "content": "Azure HDInsight Pig 任务",
      "pos": [
        4313,
        4335
      ]
    },
    {
      "content": "Azure 订阅连接管理器",
      "pos": [
        4349,
        4362
      ]
    },
    {
      "pos": [
        4385,
        4439
      ],
      "content": "在<bpt id=\"p1\">[</bpt>此处<ept id=\"p1\">][ssispack]</ept>了解有关 Azure Feature Pack for SSIS 的详细信息。"
    },
    {
      "pos": [
        4444,
        4470
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>后续步骤"
    },
    {
      "content": "现在，你已了解如何将 Pig 与 HDInsight 配合使用，请使用以下链接来学习 Azure HDInsight 的其他用法。",
      "pos": [
        4472,
        4537
      ]
    },
    {
      "content": "将数据上载到 HDInsight",
      "pos": [
        4542,
        4558
      ]
    },
    {
      "content": "将 Hive 与 HDInsight 配合使用",
      "pos": [
        4586,
        4609
      ]
    },
    {
      "content": "将 MapReduce 作业与 HDInsight 配合使用",
      "pos": [
        4634,
        4664
      ]
    }
  ],
  "content": "<properties\n   pageTitle=\"在 HDInsight 中使用 Hadoop Pig | Azure\"\n   description=\"了解如何将 Pig 与 HDInsight 上的 Hadoop 配合使用。\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.date=\"01/08/2016\"\n    wacn.date=\"03/03/2016\"/>\n\n# 将 Pig 与 HDInsight 上的 Hadoop 配合使用\n\n[AZURE.INCLUDE [pig-selector](../includes/hdinsight-selector-use-pig.md)]\n\n[Apache Pig](http://pig.apache.org/) 是一个平台，可以使用名为 *Pig Latin* 的过程语言，为 Hadoop 创建程序。Pig 可以替代 Java 来创建 *MapReduce* 解决方案，已包括在 Azure HDInsight 中。\n\n在本文中，你将了解如何将 Pig 与 hdinsight 配合使用。\n\n##<a id=\"why\"></a>为何使用 Pig？\n\n在 Hadoop 中使用 MapReduce 来处理数据时，其中一项难题是仅使用映射和化简函数来实现处理逻辑。进行复杂的处理时，你通常必须将处理分解成多个 MapReduce 操作，这些操作合在一起即可获得所需的结果。\n\n使用 Pig，你可以将处理定义成一系列转换，相关数据经过这些转换即可生成所需的输出，不必只使用映射和化简函数。\n\n使用 Pig Latin 语言，你可以通过一个或多个转换从原始输入描述数据流，以便生成所需的输出。Pig Latin 程序遵循下述常规模式：\n\n- **加载**：从文件系统中读取要操作的数据\n- **转换**：操作该数据\n- **转储或存储**：将数据输出到屏幕或将其存储后再进行处理\n\nPig Latin 还支持使用用户定义函数 (UDF) 来调用外部组件，以便实现难以在 Pig Latin 中建模的逻辑。\n\n有关 Pig Latin 的详细信息，请参阅 [Pig Latin 参考手册 1](http://pig.apache.org/docs/r0.7.0/piglatin_ref1.html) 和 [Pig Latin 参考手册 2](http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html)。\n\n如需通过 Pig 使用 UDF 的示例，请参阅以下文档：\n\n* [在 HDInsight 中通过 Pig 使用 DataFu](/documentation/articles/hdinsight-hadoop-use-pig-datafu-udf) - DataFu 是由 Apache 维护的有用 UDF 的集合\n\n* [在 HDInsight 中将 Python 与 Pig 和 Hive 配合使用](/documentation/articles/hdinsight-python)\n\n* [在 HDInsight 中将 C# 与 Hive 和 Pig 配合使用](/documentation/articles/hdinsight-hadoop-hive-pig-udf-dotnet-csharp)\n\n##<a id=\"data\"></a>关于示例数据\n\n本示例使用 *log4j* 示例文件，该文件存储在 Blob 存储容器的 **/example/data/sample.log** 中。该文件中的每个日志都包含一行字段，其中包含一个 `[LOG LEVEL]` 字段，用于显示类型和严重性，例如：\n\n    2012-02-03 20:26:41 SampleClass3 [ERROR] verbose detail for id 1527353937\n\n在前面的示例中，日志级别为 ERROR。\n\n> [AZURE.NOTE]你还可以使用 [Apache Log4j](http://zh.wikipedia.org/wiki/Log4j) 日志记录工具来生成 log4j 文件，然后将该文件上载到 Blob。请参阅[将数据上载到 HDInsight](/documentation/articles/hdinsight-upload-data) 以获取相关说明。有关如何将 Azure 存储空间中的 Blob 用于 HDInsight 的详细信息，请参阅[将 Azure Blob 存储与 HDInsight 配合使用](/documentation/articles/hdinsight-hadoop-use-blob-storage)。\n\n示例数据存储在 Azure Blob 存储中，HDInsight 可以将该存储用作 Hadoop 群集的默认文件系统。HDInsight 可以使用 **wasb** 前缀来访问存储在 Blob 中的文件。例如，若要访问 sample.log 文件，可使用以下语法：\n\n    wasb:///example/data/sample.log\n\n由于 WASB 是 HDInsight 的默认存储，你也可以使用 Pig Latin 中的 **/example/data/sample.log** 来访问该文件。\n\n> [AZURE.NOTE]语法 ****wasb:///** 用于访问存储在 HDInsight 群集的默认存储容器中的文件。如果你在预配群集时指定了其他存储帐户，并且你想要访问存储在这些帐户中的文件，则可以通过指定容器名称和存储帐户地址来访问这些数据，例如：****wasb://mycontainer@mystorage.blob.core.chinacloudapi.cn/example/data/sample.log**。\n\n\n##<a id=\"job\"></a>关于示例作业\n\n下面的 Pig Latin 作业从 HDInsight 群集的默认存储加载 **sample.log** 文件。然后，它会执行一系列转换，以便对输入数据中出现的每个日志级别进行计数。结果转储到 STDOUT。\n\n    LOGS = LOAD 'wasb:///example/data/sample.log';\n    LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\n    FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;\n    GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;\n    FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;\n    RESULT = order FREQUENCIES by COUNT desc;\n    DUMP RESULT;\n\n下同详细显示了每个转换对数据的影响。\n\n![转换的图形表示形式][image-hdi-pig-data-transformation]\n\n##<a id=\"run\"></a>运行 Pig Latin 作业\n\nHDInsight 可以使用各种方法来运行 Pig Latin 作业。使用下表来确定哪种方法最适合你，然后按链接进行演练。\n\n| **使用此方法**，如果你想要... | ...**交互式** shell | ...**批处理** | ...使用此**群集操作系统** | ...从此**客户端操作系统** |\n|:--------------------------------------------------------------|:---------------------------:|:-----------------------:|:------------------------------------------|:-----------------------------------------|\n| [Curl](/documentation/articles/hdinsight-hadoop-use-pig-curl) | &nbsp; | ✔ | Windows | Windows |\n| [.NET SDK for Hadoop](/documentation/articles/hdinsight-hadoop-use-pig-dotnet-sdk-v1) | &nbsp; | ✔ | Windows | Windows（暂时） |\n| [Windows PowerShell](/documentation/articles/hdinsight-hadoop-use-pig-powershell) | &nbsp; | ✔ | Windows | Windows |\n| [远程桌面](/documentation/articles/hdinsight-hadoop-use-pig-remote-desktop) | ✔ | ✔ | Windows | Windows |\n\n\n## 使用本地 SQL Server Integration Services 在 Azure HDInsight 上运行 Pig 作业\n\n你也可以使用 SQL Server Integration Services (SSIS) 来运行 Pig 作业。Azure Feature Pack for SSIS 提供适用于 HDInsight 上的 Pig 作业的以下组件。\n\n\n- [Azure HDInsight Pig 任务][pigtask]\n- [Azure 订阅连接管理器][connectionmanager]\n\n\n在[此处][ssispack]了解有关 Azure Feature Pack for SSIS 的详细信息。\n\n\n##<a id=\"nextsteps\"></a>后续步骤\n\n现在，你已了解如何将 Pig 与 HDInsight 配合使用，请使用以下链接来学习 Azure HDInsight 的其他用法。\n\n* [将数据上载到 HDInsight][hdinsight-upload-data]\n* [将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]\n* [将 MapReduce 作业与 HDInsight 配合使用][hdinsight-use-mapreduce]\n\n[check]: ./media/hdinsight-use-pig/hdi.checkmark.png\n\n[apachepig-home]: http://pig.apache.org/\n[putty]: http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html\n[curl]: http://curl.haxx.se/\n[pigtask]: http://msdn.microsoft.com/zh-cn/library/mt146781(v=sql.120).aspx\n[connectionmanager]: http://msdn.microsoft.com/zh-cn/library/mt146773(v=sql.120).aspx\n[ssispack]: http://msdn.microsoft.com/zh-cn/library/mt146770(v=sql.120).aspx\n\n[hdinsight-storage]: /documentation/articles/hdinsight-hadoop-use-blob-storage\n[hdinsight-upload-data]: /documentation/articles/hdinsight-upload-data\n[hdinsight-get-started]: /documentation/articles/hdinsight-hadoop-tutorial-get-started-windows-v1\n[hdinsight-admin-powershell]: /documentation/articles/hdinsight-administer-use-powershell\n\n[hdinsight-use-hive]: /documentation/articles/hdinsight-use-hive\n[hdinsight-use-mapreduce]: /documentation/articles/hdinsight-use-mapreduce\n\n[hdinsight-provision]: /documentation/articles/hdinsight-provision-clusters-v1\n[hdinsight-submit-jobs]: /documentation/articles/hdinsight-submit-hadoop-jobs-programmatically#mapreduce-sdk\n\n[Powershell-install-configure]: /documentation/articles/powershell-install-configure\n\n[powershell-start]: http://technet.microsoft.com/zh-cn/library/hh847889.aspx\n\n[image-hdi-log4j-sample]: ./media/hdinsight-use-pig/HDI.wholesamplefile.png\n[image-hdi-pig-data-transformation]: ./media/hdinsight-use-pig/HDI.DataTransformation.gif\n[image-hdi-pig-powershell]: ./media/hdinsight-use-pig/hdi.pig.powershell.png\n[image-hdi-pig-architecture]: ./media/hdinsight-use-pig/HDI.Pig.Architecture.png\n\n<!---HONumber=Mooncake_1207_2015-->"
}