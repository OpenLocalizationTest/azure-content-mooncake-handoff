{
  "nodes": [
    {
      "content": "HDInsight 的 Hadoop Emulator 入门 | Azure",
      "pos": [
        27,
        65
      ]
    },
    {
      "content": "使用安装的模拟器，参考 MapReduce 教程和其他示例来了解 Hadoop 生态系统。HDInsight Emulator 的工作原理类似于 Hadoop 沙盒。",
      "pos": [
        84,
        167
      ]
    },
    {
      "content": "使用 HDInsight Emulator（一个 Hadoop 沙盒）开始了解 Hadoop 生态系统",
      "pos": [
        455,
        506
      ]
    },
    {
      "pos": [
        508,
        779
      ],
      "content": "本指南指导你开始使用 Microsoft HDInsight Emulator for Azure（以前称作 HDInsight Server 开发者预览版）中的 Hadoop 群集。HDInsight Emulator 附带来自 Hadoop 生态系统的与 Azure HDInsight 相同的组件。有关详细信息（包括与部署的版本有关的信息），请参阅 <bpt id=\"p1\">[</bpt>Azure HDInsight 包含哪个版本的 Hadoop？<ept id=\"p1\">](/documentation/articles/hdinsight-component-versioning-v1)</ept>。"
    },
    {
      "content": "安装该模拟器后，遵照 MapReduce 教程进行单词计数，然后运行示例。",
      "pos": [
        781,
        818
      ]
    },
    {
      "pos": [
        822,
        888
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>HDInsight Emulator 只包括一个 Hadoop 群集。它不包括 HBase 或 Storm。"
    },
    {
      "content": "HDInsight Emulator 提供非常类似于 Hadoop 沙盒的本地开发环境。如果你对 Hadoop 比较熟悉，则可以开始通过 Hadoop 分布式文件系统 (HDFS) 使用 HDInsight Emulator。在 HDInsight 中，默认文件系统是 Azure Blob 存储。因此，最终你将需要使用 Azure Blob 存储开发你的作业。若要将 Azure Blob 存储与 HDInsight Emulator 配合使用，你必须对模拟器的配置进行更改。",
      "pos": [
        891,
        1130
      ]
    },
    {
      "pos": [
        1134,
        1175
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>HDInsight Emulator 只能使用单节点部署。"
    },
    {
      "content": "先决条件",
      "pos": [
        1181,
        1185
      ]
    },
    {
      "content": "在开始阅读本教程前，你必须具有：",
      "pos": [
        1186,
        1202
      ]
    },
    {
      "content": "HDInsight Emulator 需要 64 位版本的 Windows。必须满足以下要求之一：",
      "pos": [
        1206,
        1255
      ]
    },
    {
      "content": "Windows 7 Service Pack 1",
      "pos": [
        1263,
        1287
      ]
    },
    {
      "content": "Windows Server 2008 R2 Service Pack 1",
      "pos": [
        1294,
        1331
      ]
    },
    {
      "content": "Windows 8",
      "pos": [
        1338,
        1347
      ]
    },
    {
      "content": "Windows Server 2012",
      "pos": [
        1354,
        1373
      ]
    },
    {
      "pos": [
        1377,
        1487
      ],
      "content": "安装和配置 Azure PowerShell。有关说明，请参阅<bpt id=\"p1\">[</bpt>安装和配置 Azure PowerShell<ept id=\"p1\">](/documentation/articles/powershell-install-configure)</ept>。"
    },
    {
      "pos": [
        1492,
        1535
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"install\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>安装 HDInsight Emulator"
    },
    {
      "content": "Microsoft HDInsight Emulator 可通过 Microsoft Web 平台安装程序进行安装。",
      "pos": [
        1537,
        1595
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>HDInsight Emulator 目前只支持英语操作系统。如果你已安装该模拟器的先前版本，则必须先从“控制面板/程序和功能”中卸载以下两个组件，然后安装最新版本的模拟器：",
      "pos": [
        1599,
        1698
      ]
    },
    {
      "content": "Microsoft HDInsight Emulator for Azure 或 HDInsight 开发者预览版（无论安装了哪一个）",
      "pos": [
        1707,
        1774
      ]
    },
    {
      "content": "Hortonworks 数据平台",
      "pos": [
        1784,
        1800
      ]
    },
    {
      "content": "安装 HDInsight Emulator",
      "pos": [
        1816,
        1837
      ]
    },
    {
      "pos": [
        1844,
        1944
      ],
      "content": "打开 Internet Explorer，然后浏览到 <bpt id=\"p1\">[</bpt>Microsoft HDInsight Emulator for Azure 安装页<ept id=\"p1\">][hdinsight-emulator-install]</ept>。"
    },
    {
      "content": "单击“立即安装”。",
      "pos": [
        1948,
        1957
      ]
    },
    {
      "content": "在页面底部提示安装 HDINSIGHT.exe 时单击“运行”。",
      "pos": [
        1961,
        1993
      ]
    },
    {
      "content": "在弹出的“用户帐户控制”窗口中单击“是”按钮，以便完成安装。此时将显示“Web 平台安装程序”窗口。",
      "pos": [
        1997,
        2047
      ]
    },
    {
      "content": "单击页面底部的“安装”。",
      "pos": [
        2051,
        2063
      ]
    },
    {
      "content": "单击“我接受”同意许可条款。",
      "pos": [
        2067,
        2081
      ]
    },
    {
      "content": "确认 Web 平台安装程序显示“已成功安装以下产品”，然后单击“完成”。",
      "pos": [
        2085,
        2121
      ]
    },
    {
      "content": "单击“退出”以关闭“Web 平台安装程序”窗口。",
      "pos": [
        2125,
        2149
      ]
    },
    {
      "content": "验证 HDInsight Emulator 安装",
      "pos": [
        2153,
        2177
      ]
    },
    {
      "content": "该安装应该已经在你的桌面上安装了三个图标。这三个图标按如下所示进行链接：",
      "pos": [
        2181,
        2217
      ]
    },
    {
      "pos": [
        2221,
        2303
      ],
      "content": "<bpt id=\"p1\">**</bpt>Hadoop 命令行<ept id=\"p1\">**</ept> - 在 HDInsight Emulator 中从其运行 MapReduce、Pig 和 Hive 作业的 Hadoop 命令提示符。"
    },
    {
      "pos": [
        2307,
        2423
      ],
      "content": "<bpt id=\"p1\">**</bpt>Hadoop 名称节点状态<ept id=\"p1\">**</ept> - NameNode 对于 HDFS 中的所有文件维持基于树的目录。它还跟踪 Hadoop 群集中所有文件的数据位置。客户端与 NameNode 进行通信，以便确定将所有文件的数据节点存储于何处。"
    },
    {
      "pos": [
        2427,
        2478
      ],
      "content": "<bpt id=\"p1\">**</bpt>Hadoop Yarn 状态<ept id=\"p1\">**</ept> - 将 MapReduce 任务分配给群集中的节点的作业跟踪器。"
    },
    {
      "content": "该安装应该还安装了若干本地服务。下面是“服务”窗口的屏幕快照：",
      "pos": [
        2480,
        2511
      ]
    },
    {
      "content": "模拟器窗口中列出的 Hadoop 生态系统服务。",
      "pos": [
        2515,
        2539
      ]
    },
    {
      "pos": [
        2571,
        2738
      ],
      "content": "默认情况下，不会启动与 HDInsight Emulator 相关的服务。若要启动这些服务，请在 Hadoop 命令行中，在 \\\\hdp（默认位置）下运行 <bpt id=\"p1\">**</bpt>start\\_local\\_hdp\\_services.cmd<ept id=\"p1\">**</ept>。若要在重新启动计算机后自动启动这些服务，请运行 <bpt id=\"p2\">**</bpt>set-onebox-autostart.cmd<ept id=\"p2\">**</ept>。"
    },
    {
      "pos": [
        2740,
        2960
      ],
      "content": "有关安装和运行 HDInsight Emulator 的已知问题，请参阅 <bpt id=\"p1\">[</bpt>HDInsight Emulator 发行说明<ept id=\"p1\">](/documentation/articles/hdinsight-emulator-release-notes)</ept>。安装日志位于 <bpt id=\"p2\">**</bpt>C:\\HadoopFeaturePackSetup\\HadoopFeaturePackSetupTools\\gettingStarted.winpkg.install.log<ept id=\"p2\">**</ept>。"
    },
    {
      "pos": [
        2964,
        3034
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"vstools\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>在 Emulator 中使用 HDInsight Tools for Visual Studio"
    },
    {
      "pos": [
        3036,
        3273
      ],
      "content": "你可以使用 HDInsight Tools for Visual Studio 连接到 HDInsight Emulator。有关如何在 Azure 上对 HDInsight 群集使用 Visual Studio 工具的信息，请参阅 <bpt id=\"p1\">[</bpt>HDInsight Hadoop Tools for Visual Studio 入门<ept id=\"p1\">](/documentation/articles/hdinsight-hadoop-visual-studio-tools-get-started)</ept>。"
    },
    {
      "content": "安装 Emulator 的 HDInsight 工具",
      "pos": [
        3279,
        3305
      ]
    },
    {
      "pos": [
        3307,
        3439
      ],
      "content": "有关如何安装 HDInsight Visual Studio 工具的说明，请单击<bpt id=\"p1\">[</bpt>此处<ept id=\"p1\">](/documentation/articles/hdinsight-hadoop-visual-studio-tools-get-started#installation)</ept>。"
    },
    {
      "content": "连接到 HDInsight Emulator",
      "pos": [
        3445,
        3467
      ]
    },
    {
      "content": "打开 Visual Studio。",
      "pos": [
        3472,
        3489
      ]
    },
    {
      "content": "在“视图”菜单中，单击“服务器资源管理器”，以打开“服务器资源管理器”窗口。",
      "pos": [
        3493,
        3531
      ]
    },
    {
      "content": "展开“Azure”，右键单击“HDInsight”，然后单击“连接到 HDInsight Emulator”。",
      "pos": [
        3535,
        3590
      ]
    },
    {
      "content": "Visual Studio 视图：菜单中的“连接到 HDInsight Emulator”。",
      "pos": [
        3599,
        3645
      ]
    },
    {
      "content": "在“连接到 HDInsight Emulator”对话框中，检查 WebHCat、HiveServer2 和 WebHDFS 终结点的值，然后单击“下一步”。如果你没有对 Emulator 的默认配置进行任何更改，则使用默认填充的值即可。如果你做了任何更改，请更新对话框中的值，然后单击“下一步”。",
      "pos": [
        3726,
        3875
      ]
    },
    {
      "content": "包含设置的“连接到 HDInsight Emulator”对话框。",
      "pos": [
        3883,
        3916
      ]
    },
    {
      "content": "成功建立连接后，单击“完成”。现在，你应会在服务器资源管理器中看到“HDInsight Emulator”。",
      "pos": [
        4004,
        4058
      ]
    },
    {
      "content": "显示已连接到 HDInsight 本地模拟器（一个 Hadoop 沙盒）的服务器资源管理器。",
      "pos": [
        4066,
        4112
      ]
    },
    {
      "pos": [
        4192,
        4425
      ],
      "content": "成功建立连接后，可以在 Emulator 中使用 HDInsight VS 工具，就像在 Azure HDInsight 群集中使用这些工具一样。有关如何在 Azure HDInsight 群集中使用 VS 工具的说明，请参阅<bpt id=\"p1\">[</bpt>使用 HDInsight Hadoop Tools for Visual Studio<ept id=\"p1\">](/documentation/articles/hdinsight-hadoop-visual-studio-tools-get-started)</ept>。"
    },
    {
      "content": "故障排除：将 HDInsight 工具连接到 HDInsight Emulator",
      "pos": [
        4430,
        4471
      ]
    },
    {
      "pos": [
        4476,
        4769
      ],
      "content": "在连接到 HDInsight Emulator 时，即使对话框中显示 HiveServer2 已成功连接，你也必须将 Hive 配置文件（路径为 C:\\\\hdp\\\\hive-<bpt id=\"p1\">*</bpt>version<ept id=\"p1\">*</ept>\\\\conf\\\\hive-site.xml）中的 <bpt id=\"p2\">**</bpt>hive.security.authorization.enabled 属性<ept id=\"p2\">**</ept>手动设置为 <bpt id=\"p3\">**</bpt>false<ept id=\"p3\">**</ept>，然后重新启动本地 Emulator。仅当你在预览表的前 100 行时，HDInsight Tools for Visual Studio 才会连接到 HiveServer2。如果不打算使用此类查询，可将 Hive 配置保留原样。"
    },
    {
      "pos": [
        4774,
        4986
      ],
      "content": "如果你在运行 HDInsight Emulator 的计算机上使用动态 IP 分配 (DHCP)，则可能需要更新 C:\\\\hdp\\\\hadoop-<bpt id=\"p1\">*</bpt>version<ept id=\"p1\">*</ept>\\\\etc\\\\hadoop\\\\core-site.xml，并将 <bpt id=\"p2\">**</bpt>hadoop.proxyuser.hadoop.hosts<ept id=\"p2\">**</ept> 属性的值更改为 (*)。这样，Hadoop 用户便可以从所有主机进行连接，以模拟你在 Visual Studio 中输入的用户。"
    },
    {
      "content": "当 Visual Studio 尝试连接到 WebHCat 服务时，你可能会收到错误（错误: 找不到作业 job\\_XXXX\\_0001）。在这种情况下，必须重新启动 WebHCat 服务并重试。若要重新启动 WebHCat 服务，请启动“服务”MMC，右键单击“Apache Hadoop Templeton”（这是 WebHCat 服务的旧名称），然后单击“重新启动”。",
      "pos": [
        5115,
        5302
      ]
    },
    {
      "pos": [
        5306,
        5350
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"runwordcount\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>单词计数 MapReduce 教程"
    },
    {
      "content": "现在你已在工作站上配置了 HDInsight Emulator，接下来，请试着学习此 MapReduce 教程来测试安装。首先，请将一些文本文件上载到 HDFS，然后运行单词计数 MapReduce 作业以便计算特定单词在这些文件中出现的频率。",
      "pos": [
        5352,
        5474
      ]
    },
    {
      "pos": [
        5476,
        5630
      ],
      "content": "单词计数 MapReduce 程序已打包到 <bpt id=\"p1\">*</bpt>hadoop-mapreduce-examples-2.4.0.2.1.3.0-1981.jar<ept id=\"p1\">*</ept> 中。该 jar 文件位于 <bpt id=\"p2\">*</bpt>C:\\\\hdp\\\\hadoop-2.4.0.2.1.3.0-1981\\\\share\\\\hadoop\\\\mapreduce<ept id=\"p2\">*</ept> 文件夹中。"
    },
    {
      "content": "用于单词计数的 MapReduce 作业采用两个参数：",
      "pos": [
        5632,
        5659
      ]
    },
    {
      "pos": [
        5663,
        5716
      ],
      "content": "一个输入文件夹。你将使用 <bpt id=\"p1\">*</bpt>hdfs://localhost/user/HDIUser<ept id=\"p1\">*</ept> 作为输入文件夹。"
    },
    {
      "pos": [
        5719,
        5872
      ],
      "content": "一个输出文件夹。你将使用 <bpt id=\"p1\">*</bpt>hdfs://localhost/user/HDIUser/WordCount_Output<ept id=\"p1\">*</ept> 作为输出文件夹。输出文件夹不能是现有文件夹，否则 MapReduce 作业将失败。如果要第二次运行 MapReduce 作业，则必须指定一个不同的输出文件夹，或删除现有的输出文件夹。"
    },
    {
      "content": "运行单词计数 MapReduce 作业",
      "pos": [
        5876,
        5895
      ]
    },
    {
      "content": "从桌面双击“Hadoop 命令行”以打开 Hadoop 命令行窗口。当前文件夹应为：",
      "pos": [
        5902,
        5944
      ]
    },
    {
      "content": "如果不是，请运行以下命令：",
      "pos": [
        5992,
        6005
      ]
    },
    {
      "content": "运行以下 Hadoop 命令以使 HDFS 文件夹对输入和输出文件进行排序：",
      "pos": [
        6036,
        6074
      ]
    },
    {
      "content": "运行以下 Hadoop 命令以便将某些本地文本文件复制到 HDFS：",
      "pos": [
        6150,
        6184
      ]
    },
    {
      "content": "运行以下命令以便列出 /user/HDIUser 文件夹中的文件：",
      "pos": [
        6299,
        6332
      ]
    },
    {
      "content": "你应该看到以下文件：",
      "pos": [
        6375,
        6385
      ]
    },
    {
      "content": "运行以下命令来运行单词计数 MapReduce 作业：",
      "pos": [
        6844,
        6871
      ]
    },
    {
      "content": "运行以下命令以便从输出文件中列出其中含有“windows”的单词数：",
      "pos": [
        7095,
        7129
      ]
    },
    {
      "content": "输出应该是：",
      "pos": [
        7223,
        7229
      ]
    },
    {
      "pos": [
        7422,
        7482
      ],
      "content": "有关 Hadoop 命令的详细信息，请参阅 <bpt id=\"p1\">[</bpt>Hadoop 命令参考<ept id=\"p1\">][hadoop-commands-manual]</ept>。"
    },
    {
      "pos": [
        7486,
        7534
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"rungetstartedsamples\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>分析示例 web 日志数据"
    },
    {
      "content": "HDInsight Emulator 安装提供了一些示例，以便用户能够在 Windows 上开始学习基于 Apache Hadoop 的服务。这些示例涉及在处理大型数据集时通常需要的一些任务。这些示例是根据上述 MapReduce 教程制作的，可帮助你熟悉与 MapReduce 编程模型及其生态系统。",
      "pos": [
        7536,
        7687
      ]
    },
    {
      "pos": [
        7689,
        8041
      ],
      "content": "示例数据是围绕处理 IIS 万维网联盟 (W3C) 日志数据进行组织的。提供数据生成工具以便创建不同大小的数据集并将这些数据集导入到 HDFS 或 Azure Blob 存储中。（有关详细信息，请参阅<bpt id=\"p1\">[</bpt>将 Azure Blob 存储用于 HDInsight<ept id=\"p1\">](/documentation/articles/hdinsight-hadoop-use-blob-storage)</ept>）。然后，可以在 Azure PowerShell 脚本生成的数据页上运行 MapReduce、Pig 或 Hive 作业。使用的 Pig 和 Hive 脚本是基于 MapReduce 的抽象层，最终都会编译成 MapReduce 程序。你可以运行一系列作业，以便观察使用这些不同技术的影响以及数据大小对执行这些处理任务的影响。"
    },
    {
      "content": "本节内容",
      "pos": [
        8047,
        8051
      ]
    },
    {
      "content": "IIS W3C 日志数据方案",
      "pos": [
        8056,
        8070
      ]
    },
    {
      "content": "加载示例 W3C 日志数据",
      "pos": [
        8087,
        8100
      ]
    },
    {
      "content": "运行 Java MapReduce 作业",
      "pos": [
        8116,
        8136
      ]
    },
    {
      "content": "运行 Hive 作业",
      "pos": [
        8157,
        8167
      ]
    },
    {
      "content": "运行 Pig 作业",
      "pos": [
        8179,
        8188
      ]
    },
    {
      "pos": [
        8200,
        8238
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"scenarios\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>IIS W3C 日志数据方案"
    },
    {
      "content": "W3C 方案生成以下三种大小的 IIS W3C 日志数据并将这些数据导入到 HDFS 或 Azure Blob 存储中：1MB（小）、500MB（中）和 2GB（大）。它提供三种作业类型并且分别在 C#、Java、Pig 和 Hive 中实现它们。",
      "pos": [
        8240,
        8364
      ]
    },
    {
      "pos": [
        8368,
        8399
      ],
      "content": "<bpt id=\"p1\">**</bpt>totalhits<ept id=\"p1\">**</ept> - 计算针对某一给定页的请求总数。"
    },
    {
      "pos": [
        8402,
        8438
      ],
      "content": "<bpt id=\"p1\">**</bpt>avgtime<ept id=\"p1\">**</ept> - 计算每页某一请求所用的平均时间（单位为秒）。"
    },
    {
      "pos": [
        8441,
        8488
      ],
      "content": "<bpt id=\"p1\">**</bpt>errors<ept id=\"p1\">**</ept> - 计算对于其状态为 404 或 500 的请求，每页、每小时的错误数。"
    },
    {
      "content": "这些示例及其文档并未提供针对关键 Hadoop 技术的深入研究或完整实现。使用的群集只具有单个节点，因此对于此版本，无法观察添加多个节点的影响。",
      "pos": [
        8490,
        8562
      ]
    },
    {
      "pos": [
        8567,
        8603
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"loaddata\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>加载示例 W3C 日志数据"
    },
    {
      "content": "通过 Azure PowerShell 脚本 importdata.ps1 实现生成数据并且将数据导入到 HDFS。",
      "pos": [
        8605,
        8663
      ]
    },
    {
      "content": "导入示例 W3C 日志数据",
      "pos": [
        8667,
        8680
      ]
    },
    {
      "content": "从桌面打开 Hadoop 命令行。",
      "pos": [
        8687,
        8704
      ]
    },
    {
      "pos": [
        8708,
        8743
      ],
      "content": "将目录切换到 <bpt id=\"p1\">**</bpt>C:\\\\hdp\\\\GettingStarted<ept id=\"p1\">**</ept>。"
    },
    {
      "content": "运行以下命令以生成数据并且将数据导入到 HDFS：",
      "pos": [
        8747,
        8772
      ]
    },
    {
      "pos": [
        8853,
        8916
      ],
      "content": "如果要改为将数据导入到 Azure Blob 存储，请参阅<bpt id=\"p1\">[</bpt>连接到 Azure Blob 存储<ept id=\"p1\">](#blobstorage)</ept>。"
    },
    {
      "content": "从 Hadoop 命令行运行以下命令以便列出 HDFS 上导入的文件：",
      "pos": [
        8921,
        8956
      ]
    },
    {
      "content": "输出应如下所示：",
      "pos": [
        8993,
        9001
      ]
    },
    {
      "content": "如果你要验证文件内容，请运行以下命令，以在控制台窗口中显示一个数据文件：",
      "pos": [
        9687,
        9723
      ]
    },
    {
      "content": "现在，你已创建数据文件并已将其导入到 HDFS。你可以开始运行其他 Hadoop 作业。",
      "pos": [
        9785,
        9829
      ]
    },
    {
      "pos": [
        9834,
        9882
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"javamapreduce\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>运行 Java MapReduce 作业"
    },
    {
      "content": "MapReduce 是针对 Hadoop 的基本计算引擎。默认情况下，它是在 Java 中实现的，但也有利用采用 C# 的 .NET 和 Hadoop Streaming 的示例。运行 MapReduce 作业的语法是：",
      "pos": [
        9884,
        9994
      ]
    },
    {
      "content": "jar 文件和源文件位于 C:\\\\Hadoop\\\\GettingStarted\\\\Java 文件夹中。",
      "pos": [
        10070,
        10121
      ]
    },
    {
      "content": "运行 MapReduce 作业以便计算网页点击数",
      "pos": [
        10125,
        10149
      ]
    },
    {
      "content": "打开 Hadoop 命令行。",
      "pos": [
        10156,
        10170
      ]
    },
    {
      "pos": [
        10174,
        10209
      ],
      "content": "将目录切换到 <bpt id=\"p1\">**</bpt>C:\\\\hdp\\\\GettingStarted<ept id=\"p1\">**</ept>。"
    },
    {
      "content": "运行以下命令以便在该文件夹存在时删除输出目录。如果输出文件夹已存在，该 MapReduce 作业将失败。",
      "pos": [
        10213,
        10265
      ]
    },
    {
      "content": "运行以下命令：",
      "pos": [
        10308,
        10315
      ]
    },
    {
      "content": "下表介绍了该命令的元素：",
      "pos": [
        10458,
        10470
      ]
    },
    {
      "content": "参数",
      "pos": [
        10497,
        10499
      ]
    },
    {
      "content": "备注",
      "pos": [
        10508,
        10510
      ]
    },
    {
      "content": "w3c\\_scenarios.jar",
      "pos": [
        10529,
        10547
      ]
    },
    {
      "content": "该 jar 文件位于 C:\\\\hdp\\\\GettingStarted\\\\Java 文件夹中。",
      "pos": [
        10556,
        10602
      ]
    },
    {
      "content": "microsoft.hadoop.w3c.TotalHitsForPage",
      "pos": [
        10621,
        10658
      ]
    },
    {
      "content": "可使用以下项之一替代该类型：",
      "pos": [
        10667,
        10681
      ]
    },
    {
      "content": "microsoft.hadoop.w3c.AverageTimeTaken",
      "pos": [
        10690,
        10727
      ]
    },
    {
      "content": "microsoft.hadoop.w3c.ErrorsByPage",
      "pos": [
        10737,
        10770
      ]
    },
    {
      "content": "/w3c/input/small/data\\_w3c\\_small.txt",
      "pos": [
        10800,
        10837
      ]
    },
    {
      "content": "可使用以下项之一替代该输入文件：",
      "pos": [
        10846,
        10862
      ]
    },
    {
      "content": "/w3c/input/medium/data\\_w3c\\_medium.txt",
      "pos": [
        10871,
        10910
      ]
    },
    {
      "content": "/w3c/input/large/data\\_w3c\\_large.txt",
      "pos": [
        10920,
        10957
      ]
    },
    {
      "content": "/w3c/output",
      "pos": [
        10987,
        10998
      ]
    },
    {
      "content": "这是输出文件夹名称。",
      "pos": [
        11007,
        11017
      ]
    },
    {
      "content": "运行以下命令以显示输出文件：",
      "pos": [
        11041,
        11055
      ]
    },
    {
      "content": "输出应如下所示：",
      "pos": [
        11108,
        11116
      ]
    },
    {
      "content": "Default.aspx 页得到了 3360 次点击，依此类推。尝试根据上表中的建议替换值并再次运行这些命令，然后观察输出如何根据作业类型和数据大小发生变化。",
      "pos": [
        11281,
        11360
      ]
    },
    {
      "pos": [
        11366,
        11395
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"hive\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>运行 Hive 作业"
    },
    {
      "content": "精通结构化查询语言 (SQL) 的分析人士对于 Hive 查询引擎可能感到很熟悉。它提供与 SQL 类似的界面以及用于 HDFS 的关系数据模型。Hive 使用一种称作 HiveQL 的语言，这与 SQL 十分类似。Hive 在基于 Java 的 MapReduce 框架上提供了一个抽象层，Hive 查询在运行时将编译成 MapReduce。",
      "pos": [
        11396,
        11568
      ]
    },
    {
      "content": "运行 Hive 作业",
      "pos": [
        11572,
        11582
      ]
    },
    {
      "content": "打开 Hadoop 命令行。",
      "pos": [
        11589,
        11603
      ]
    },
    {
      "pos": [
        11607,
        11642
      ],
      "content": "将目录切换到 <bpt id=\"p1\">**</bpt>C:\\\\hdp\\\\GettingStarted<ept id=\"p1\">**</ept>。"
    },
    {
      "pos": [
        11646,
        11711
      ],
      "content": "运行以下命令以便在 <bpt id=\"p1\">**</bpt>/w3c/hive/input<ept id=\"p1\">**</ept> 文件夹存在时删除该文件夹。如果该文件夹存在，则 hive 作业将失败。"
    },
    {
      "pos": [
        11756,
        11814
      ],
      "content": "运行以下命令以便创建 <bpt id=\"p1\">**</bpt>/w3c/hive/input<ept id=\"p1\">**</ept> 文件夹，然后将数据文件复制到 /hive/input："
    },
    {
      "pos": [
        11971,
        12038
      ],
      "content": "运行以下命令以执行 <bpt id=\"p1\">**</bpt>w3ccreate.hql<ept id=\"p1\">**</ept> 脚本文件。该脚本将创建一个 Hive 表，并且将数据导入到该 Hive 表中："
    },
    {
      "pos": [
        12046,
        12289
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>在此阶段，你也可以使用 HDInsight Visual Studio 工具来运行 Hive 查询。打开 Visual Studio，创建一个新项目，然后从 HDInsight 模板中选择“Hive 应用程序”。打开该项目后，将查询添加为新项。该查询位于 <bpt id=\"p1\">**</bpt>C:/hdp/GettingStarted/Hive/w3c<ept id=\"p1\">**</ept> 中。将查询添加到项目后，将 <bpt id=\"p2\">**</bpt>${hiveconf:input}<ept id=\"p2\">**</ept> 替换为 <bpt id=\"p3\">**</bpt>/w3c/hive/input<ept id=\"p3\">**</ept>，然后按“提交”。"
    },
    {
      "content": "输出将如下所示：",
      "pos": [
        12430,
        12438
      ]
    },
    {
      "pos": [
        12960,
        13010
      ],
      "content": "运行以下命令以便运行 <bpt id=\"p1\">**</bpt>w3ctotalhitsbypage.hql<ept id=\"p1\">**</ept> HiveQL 脚本文件："
    },
    {
      "pos": [
        13018,
        13074
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>如前所述，你也可以使用 HDInsight Visual Studio 工具运行此查询。"
    },
    {
      "content": "下表描述了该命令的元素：",
      "pos": [
        13171,
        13183
      ]
    },
    {
      "content": "文件",
      "pos": [
        13210,
        13212
      ]
    },
    {
      "content": "说明",
      "pos": [
        13221,
        13223
      ]
    },
    {
      "content": "C:\\\\hdp\\\\hive-0.13.0.2.1.3.0-1981\\\\bin\\\\hive.cmd",
      "pos": [
        13242,
        13290
      ]
    },
    {
      "content": "Hive 命令脚本。",
      "pos": [
        13299,
        13309
      ]
    },
    {
      "content": "C:\\\\hdp\\\\GettingStarted\\\\Hive\\\\w3c\\\\w3ctotalhitsbypage.hql",
      "pos": [
        13328,
        13386
      ]
    },
    {
      "content": "你可以使用以下文件之一替代该 Hive 脚本文件：",
      "pos": [
        13396,
        13421
      ]
    },
    {
      "content": "C:\\\\hdp\\\\GettingStarted\\\\Hive\\\\w3c\\\\w3caveragetimetaken.hql",
      "pos": [
        13430,
        13489
      ]
    },
    {
      "content": "C:\\\\hdp\\\\GettingStarted\\\\Hive\\\\w3c\\\\w3cerrorsbypage.hql",
      "pos": [
        13499,
        13554
      ]
    },
    {
      "content": "w3ctotalhitsbypage.hql HiveQL 脚本是：",
      "pos": [
        13596,
        13630
      ]
    },
    {
      "content": "输出的末尾将如下所示：",
      "pos": [
        13835,
        13846
      ]
    },
    {
      "content": "请注意，作为每个作业的第一步，将创建一个表并将之前创建的文件中的数据加载到该表中。你可以通过使用以下命令查看 HDFS 中的 /Hive 节点，浏览已创建的文件：",
      "pos": [
        14305,
        14386
      ]
    },
    {
      "pos": [
        14424,
        14451
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"pig\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>运行 Pig 作业"
    },
    {
      "pos": [
        14453,
        14552
      ],
      "content": "Pig 处理使用称作 <bpt id=\"p1\">*</bpt>Pig Latin<ept id=\"p1\">*</ept> 的数据流语言。Pig Latin 抽象提供了比 MapReduce 更丰富的数据结构，并为 Hadoop 执行 SQL 对关系数据库管理系统执行的操作。"
    },
    {
      "content": "运行 pig 作业",
      "pos": [
        14557,
        14566
      ]
    },
    {
      "content": "打开 Hadoop 命令行。",
      "pos": [
        14573,
        14587
      ]
    },
    {
      "pos": [
        14591,
        14630
      ],
      "content": "将目录切换为 <bpt id=\"p1\">**</bpt>C:\\\\hdp\\\\GettingStarted<ept id=\"p1\">**</ept> 文件夹。"
    },
    {
      "content": "运行以下命令来提交 Pig 作业：",
      "pos": [
        14634,
        14651
      ]
    },
    {
      "pos": [
        14792,
        15300
      ],
      "content": "下表显示了该命令的元素：\n <table border=\"1\">\n <tr><td>文件</td><td>说明</td></tr>\n <tr><td>C:\\\\hdp\\\\pig-0.12.1.2.1.3.0-1981\\\\bin\\\\pig.cmd</td><td>Pig 命令脚本。</td></tr>\n <tr><td>C:\\\\hdp\\\\GettingStarted\\\\Pig\\\\w3c\\\\TotalHitsForPage.pig</td><td>你可以使用以下文件之一替代该 Pig Latin 脚本文件：\n <ul>\n <li>C:\\\\hdp\\\\GettingStarted\\\\Pig\\\\w3c\\\\AverageTimeTaken.pig</li>\n <li>C:\\\\hdp\\\\GettingStarted\\\\Pig\\\\w3c\\\\ErrorsByPage.pig</li>\n </ul>\n </td></tr>\n <tr><td>/w3c/input/small/data\\_w3c\\_small.txt</td><td>你可以使用更大的文件替换该参数：",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "下表显示了该命令的元素：",
          "pos": [
            0,
            12
          ]
        },
        {
          "content": "文件",
          "pos": [
            42,
            44
          ]
        },
        {
          "content": "说明",
          "pos": [
            53,
            55
          ]
        },
        {
          "content": "C:\\\\hdp\\\\pig-0.12.1.2.1.3.0-1981\\\\bin\\\\pig.cmd",
          "pos": [
            75,
            121
          ]
        },
        {
          "content": "Pig 命令脚本。",
          "pos": [
            130,
            139
          ]
        },
        {
          "content": "C:\\\\hdp\\\\GettingStarted\\\\Pig\\\\w3c\\\\TotalHitsForPage.pig",
          "pos": [
            159,
            214
          ]
        },
        {
          "content": "你可以使用以下文件之一替代该 Pig Latin 脚本文件：",
          "pos": [
            223,
            253
          ]
        },
        {
          "content": "C:\\\\hdp\\\\GettingStarted\\\\Pig\\\\w3c\\\\AverageTimeTaken.pig",
          "pos": [
            265,
            320
          ]
        },
        {
          "content": "C:\\\\hdp\\\\GettingStarted\\\\Pig\\\\w3c\\\\ErrorsByPage.pig",
          "pos": [
            331,
            382
          ]
        },
        {
          "content": "/w3c/input/small/data\\_w3c\\_small.txt",
          "pos": [
            416,
            453
          ]
        },
        {
          "content": "你可以使用更大的文件替换该参数：",
          "pos": [
            462,
            478
          ]
        }
      ]
    },
    {
      "pos": [
        15306,
        15439
      ],
      "content": "<ul>\n <li>/w3c/input/medium/data_w3c_medium.txt</li>\n <li>/w3c/input/large/data_w3c_large.txt</li>\n </ul></td></tr> </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "/w3c/input/medium/data_w3c_medium.txt",
          "pos": [
            10,
            47
          ]
        },
        {
          "content": "/w3c/input/large/data_w3c_large.txt",
          "pos": [
            58,
            93
          ]
        }
      ]
    },
    {
      "content": "输出应如下所示：",
      "pos": [
        15445,
        15453
      ]
    },
    {
      "content": "请注意，因为 Pig 脚本编译到 MapReduce 作业，并且可能会编译到多个此类作业，所以，你在处理 Pig 作业的过程中可能会看到多个 MapReduce 作业正在执行。",
      "pos": [
        15539,
        15627
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"blobstorage\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>连接到 Azure Blob 存储",
      "pos": [
        16080,
        16123
      ]
    },
    {
      "content": "HDInsight Emulator 使用 HDFS 作为默认文件系统。但是，Azure HDInsight 使用 Azure Blob 存储作为默认文件系统。可以将 HDInsight Emulator 配置为使用 Azure Blob 存储而不是本地存储。遵照以下说明在 Azure 中创建存储容器，然后将它连接到 HDInsight Emulator。",
      "pos": [
        16124,
        16303
      ]
    },
    {
      "pos": [
        16306,
        16451
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>有关 HDInsight 如何使用 Azure Blob 存储的详细信息，请参阅<bpt id=\"p1\">[</bpt>将 Azure Blob 存储与 HDInsight 配合使用<ept id=\"p1\">](/documentation/articles/hdinsight-hadoop-use-blob-storage)</ept>。"
    },
    {
      "pos": [
        16453,
        16551
      ],
      "content": "在开始遵照下面的说明之前，你必须创建存储帐户。有关说明，请参阅<bpt id=\"p1\">[</bpt>如何创建存储帐户<ept id=\"p1\">](/documentation/articles/storage-create-storage-account)</ept>。"
    },
    {
      "content": "创建容器",
      "pos": [
        16555,
        16559
      ]
    },
    {
      "pos": [
        16566,
        16606
      ],
      "content": "登录到 <bpt id=\"p1\">[</bpt>Azure 门户<ept id=\"p1\">][azure-management-portal]</ept>。"
    },
    {
      "content": "单击左侧的“存储”。此时将显示订阅下的存储帐户列表。",
      "pos": [
        16610,
        16636
      ]
    },
    {
      "content": "单击要从该列表创建容器的存储帐户。",
      "pos": [
        16640,
        16657
      ]
    },
    {
      "content": "单击页面顶部的“容器”。",
      "pos": [
        16661,
        16673
      ]
    },
    {
      "content": "单击页面底部的“添加”。",
      "pos": [
        16677,
        16689
      ]
    },
    {
      "content": "输入“名称”并选择“访问”。你可以使用三种访问级别中的任何一种。默认级别为“私有”。",
      "pos": [
        16693,
        16735
      ]
    },
    {
      "content": "单击“确定”以保存更改。现在，门户上会列出新的容器。",
      "pos": [
        16739,
        16765
      ]
    },
    {
      "content": "必须先将帐户名称和帐户密钥添加到配置文件，然后才能访问 Azure 存储帐户。",
      "pos": [
        16767,
        16806
      ]
    },
    {
      "content": "配置与 Azure 存储帐户的连接",
      "pos": [
        16810,
        16827
      ]
    },
    {
      "pos": [
        16834,
        16909
      ],
      "content": "在记事本中打开 <bpt id=\"p1\">**</bpt>C:\\\\hdp\\\\hadoop-2.4.0.2.1.3.0-1981\\\\etc\\\\hadoop\\\\core-site.xml<ept id=\"p1\">**</ept>。"
    },
    {
      "content": "将以下",
      "pos": [
        16913,
        16916
      ]
    },
    {
      "content": "标记添加到其他",
      "pos": [
        16928,
        16935
      ]
    },
    {
      "content": "标记旁：",
      "pos": [
        16947,
        16951
      ]
    },
    {
      "content": "你必须使用与你的存储帐户信息匹配的值替代",
      "pos": [
        17138,
        17158
      ]
    },
    {
      "content": "和",
      "pos": [
        17180,
        17181
      ]
    },
    {
      "content": "。",
      "pos": [
        17201,
        17202
      ]
    },
    {
      "content": "保存更改。你无需重新启动 Hadoop 服务。",
      "pos": [
        17207,
        17230
      ]
    },
    {
      "content": "使用以下语法可访问该存储帐户：",
      "pos": [
        17232,
        17247
      ]
    },
    {
      "content": "例如：",
      "pos": [
        17326,
        17329
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"powershell\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>运行 Azure PowerShell",
      "pos": [
        17410,
        17454
      ]
    },
    {
      "content": "HDInsight Emulator 也支持某些 Azure PowerShell cmdlet。这些 cmdlet 包括：",
      "pos": [
        17455,
        17517
      ]
    },
    {
      "content": "HDInsight 作业定义 cmdlet",
      "pos": [
        17521,
        17542
      ]
    },
    {
      "content": "New-AzureHDInsightSqoopJobDefinition",
      "pos": [
        17550,
        17586
      ]
    },
    {
      "content": "New-AzureHDInsightStreamingMapReduceJobDefinition",
      "pos": [
        17593,
        17642
      ]
    },
    {
      "content": "New-AzureHDInsightPigJobDefinition",
      "pos": [
        17649,
        17683
      ]
    },
    {
      "content": "New-AzureHDInsightHiveJobDefinition",
      "pos": [
        17690,
        17725
      ]
    },
    {
      "content": "New-AzureHDInsightMapReduceJobDefinition",
      "pos": [
        17732,
        17772
      ]
    },
    {
      "content": "Start-AzureHDInsightJob",
      "pos": [
        17779,
        17802
      ]
    },
    {
      "content": "Get-AzureHDInsightJob",
      "pos": [
        17809,
        17830
      ]
    },
    {
      "content": "Wait-AzureHDInsightJob",
      "pos": [
        17837,
        17859
      ]
    },
    {
      "content": "下面是用于提交 Hadoop 作业的示例：",
      "pos": [
        17861,
        17882
      ]
    },
    {
      "pos": [
        18104,
        18205
      ],
      "content": "在调用 Get-Credential 时系统将会向你显示一个提示。你必须将 <bpt id=\"p1\">**</bpt>hadoop<ept id=\"p1\">**</ept> 用作用户名。密码可以是任意字符串。群集名称始终是 <bpt id=\"p2\">**</bpt>http://localhost:50111<ept id=\"p2\">**</ept>。"
    },
    {
      "pos": [
        18207,
        18427
      ],
      "content": "有关提交 Hadoop 作业的详细信息，请参阅<bpt id=\"p1\">[</bpt>以编程方式提交 Hadoop 作业<ept id=\"p1\">](/documentation/articles/hdinsight-submit-hadoop-jobs-programmatically)</ept>。有关适用于 HDInsight 的 Azure Powershell cmdlet 的详细信息，请参阅 <bpt id=\"p2\">[</bpt>HDInsight cmdlet 参考<ept id=\"p2\">][hdinsight-powershell-reference]</ept>。"
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"remove\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>删除 HDInsight Emulator",
      "pos": [
        18432,
        18474
      ]
    },
    {
      "content": "在安装模拟器的计算机上打开控制面板，然后在“程序”下面单击“卸载程序”。从已安装程序的列表中，右键单击“Microsoft HDInsight Emulator for Azure”，然后单击“卸载”。",
      "pos": [
        18475,
        18576
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a name=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>后续步骤",
      "pos": [
        18581,
        18609
      ]
    },
    {
      "content": "在本 MapReduce 教程中，你安装了 HDInsight Emulator - 一个 Hadoop 沙盒 - 并运行了一些 Hadoop 作业。若要了解更多信息，请参阅下列文章：",
      "pos": [
        18610,
        18702
      ]
    },
    {
      "content": "Azure HDInsight 入门",
      "pos": [
        18707,
        18725
      ]
    },
    {
      "content": "为 HDInsight 开发 Java MapReduce 程序",
      "pos": [
        18804,
        18836
      ]
    },
    {
      "content": "为 HDInsight 开发 C# Hadoop 流式处理 MapReduce 程序",
      "pos": [
        18906,
        18948
      ]
    },
    {
      "content": "HDInsight Emulator 发行说明",
      "pos": [
        19025,
        19048
      ]
    },
    {
      "content": "讨论 HDInsight 的 MSDN 论坛",
      "pos": [
        19111,
        19133
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"HDInsight 的 Hadoop Emulator 入门 | Azure\"\n    description=\"使用安装的模拟器，参考 MapReduce 教程和其他示例来了解 Hadoop 生态系统。HDInsight Emulator 的工作原理类似于 Hadoop 沙盒。\"\n    keywords=\"emulator,hadoop ecosystem,hadoop sandbox,mapreduce tutorial\"\n    editor=\"cgronlun\"\n    manager=\"paulettm\"\n    services=\"hdinsight\"\n    authors=\"nitinme\"\n    documentationCenter=\"\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.date=\"11/29/2015\"\n    wacn.date=\"01/14/2016\"/>\n# 使用 HDInsight Emulator（一个 Hadoop 沙盒）开始了解 Hadoop 生态系统\n\n本指南指导你开始使用 Microsoft HDInsight Emulator for Azure（以前称作 HDInsight Server 开发者预览版）中的 Hadoop 群集。HDInsight Emulator 附带来自 Hadoop 生态系统的与 Azure HDInsight 相同的组件。有关详细信息（包括与部署的版本有关的信息），请参阅 [Azure HDInsight 包含哪个版本的 Hadoop？](/documentation/articles/hdinsight-component-versioning-v1)。\n\n安装该模拟器后，遵照 MapReduce 教程进行单词计数，然后运行示例。\n\n> [AZURE.NOTE]HDInsight Emulator 只包括一个 Hadoop 群集。它不包括 HBase 或 Storm。\n\n\nHDInsight Emulator 提供非常类似于 Hadoop 沙盒的本地开发环境。如果你对 Hadoop 比较熟悉，则可以开始通过 Hadoop 分布式文件系统 (HDFS) 使用 HDInsight Emulator。在 HDInsight 中，默认文件系统是 Azure Blob 存储。因此，最终你将需要使用 Azure Blob 存储开发你的作业。若要将 Azure Blob 存储与 HDInsight Emulator 配合使用，你必须对模拟器的配置进行更改。\n\n> [AZURE.NOTE]HDInsight Emulator 只能使用单节点部署。\n\n\n## 先决条件\n在开始阅读本教程前，你必须具有：\n\n- HDInsight Emulator 需要 64 位版本的 Windows。必须满足以下要求之一：\n\n    - Windows 7 Service Pack 1\n    - Windows Server 2008 R2 Service Pack 1\n    - Windows 8\n    - Windows Server 2012\n\n- 安装和配置 Azure PowerShell。有关说明，请参阅[安装和配置 Azure PowerShell](/documentation/articles/powershell-install-configure)。\n\n\n##<a name=\"install\"></a>安装 HDInsight Emulator\n\nMicrosoft HDInsight Emulator 可通过 Microsoft Web 平台安装程序进行安装。\n\n> [AZURE.NOTE]HDInsight Emulator 目前只支持英语操作系统。如果你已安装该模拟器的先前版本，则必须先从“控制面板/程序和功能”中卸载以下两个组件，然后安装最新版本的模拟器：<ul> <li>Microsoft HDInsight Emulator for Azure 或 HDInsight 开发者预览版（无论安装了哪一个）</li> <li>Hortonworks 数据平台</li> </ul>\n\n\n**安装 HDInsight Emulator**\n\n1. 打开 Internet Explorer，然后浏览到 [Microsoft HDInsight Emulator for Azure 安装页][hdinsight-emulator-install]。\n2. 单击“立即安装”。\n3. 在页面底部提示安装 HDINSIGHT.exe 时单击“运行”。\n4. 在弹出的“用户帐户控制”窗口中单击“是”按钮，以便完成安装。此时将显示“Web 平台安装程序”窗口。\n6. 单击页面底部的“安装”。\n7. 单击“我接受”同意许可条款。\n8. 确认 Web 平台安装程序显示“已成功安装以下产品”，然后单击“完成”。\n9. 单击“退出”以关闭“Web 平台安装程序”窗口。\n\n**验证 HDInsight Emulator 安装**\n\n该安装应该已经在你的桌面上安装了三个图标。这三个图标按如下所示进行链接：\n\n- **Hadoop 命令行** - 在 HDInsight Emulator 中从其运行 MapReduce、Pig 和 Hive 作业的 Hadoop 命令提示符。\n\n- **Hadoop 名称节点状态** - NameNode 对于 HDFS 中的所有文件维持基于树的目录。它还跟踪 Hadoop 群集中所有文件的数据位置。客户端与 NameNode 进行通信，以便确定将所有文件的数据节点存储于何处。\n\n- **Hadoop Yarn 状态** - 将 MapReduce 任务分配给群集中的节点的作业跟踪器。\n\n该安装应该还安装了若干本地服务。下面是“服务”窗口的屏幕快照：\n\n![模拟器窗口中列出的 Hadoop 生态系统服务。][image-hdi-emulator-services]\n\n默认情况下，不会启动与 HDInsight Emulator 相关的服务。若要启动这些服务，请在 Hadoop 命令行中，在 \\\\hdp（默认位置）下运行 **start\\_local\\_hdp\\_services.cmd**。若要在重新启动计算机后自动启动这些服务，请运行 **set-onebox-autostart.cmd**。\n\n有关安装和运行 HDInsight Emulator 的已知问题，请参阅 [HDInsight Emulator 发行说明](/documentation/articles/hdinsight-emulator-release-notes)。安装日志位于 **C:\\HadoopFeaturePackSetup\\HadoopFeaturePackSetupTools\\gettingStarted.winpkg.install.log**。\n\n##<a name=\"vstools\"></a>在 Emulator 中使用 HDInsight Tools for Visual Studio\n\n你可以使用 HDInsight Tools for Visual Studio 连接到 HDInsight Emulator。有关如何在 Azure 上对 HDInsight 群集使用 Visual Studio 工具的信息，请参阅 [HDInsight Hadoop Tools for Visual Studio 入门](/documentation/articles/hdinsight-hadoop-visual-studio-tools-get-started)。\n\n### 安装 Emulator 的 HDInsight 工具\n\n有关如何安装 HDInsight Visual Studio 工具的说明，请单击[此处](/documentation/articles/hdinsight-hadoop-visual-studio-tools-get-started#installation)。\n\n### 连接到 HDInsight Emulator\n\n1. 打开 Visual Studio。\n2. 在“视图”菜单中，单击“服务器资源管理器”，以打开“服务器资源管理器”窗口。\n3. 展开“Azure”，右键单击“HDInsight”，然后单击“连接到 HDInsight Emulator”。\n\n     ![Visual Studio 视图：菜单中的“连接到 HDInsight Emulator”。](./media/hdinsight-hadoop-emulator-get-started/hdi.emulator.connect.vs.png)\n\n4. 在“连接到 HDInsight Emulator”对话框中，检查 WebHCat、HiveServer2 和 WebHDFS 终结点的值，然后单击“下一步”。如果你没有对 Emulator 的默认配置进行任何更改，则使用默认填充的值即可。如果你做了任何更改，请更新对话框中的值，然后单击“下一步”。\n\n    ![包含设置的“连接到 HDInsight Emulator”对话框。](./media/hdinsight-hadoop-emulator-get-started/hdi.emulator.connect.vs.dialog.png)\n\n5. 成功建立连接后，单击“完成”。现在，你应会在服务器资源管理器中看到“HDInsight Emulator”。\n\n    ![显示已连接到 HDInsight 本地模拟器（一个 Hadoop 沙盒）的服务器资源管理器。](./media/hdinsight-hadoop-emulator-get-started/hdi.emulator.vs.connected.png)\n\n成功建立连接后，可以在 Emulator 中使用 HDInsight VS 工具，就像在 Azure HDInsight 群集中使用这些工具一样。有关如何在 Azure HDInsight 群集中使用 VS 工具的说明，请参阅[使用 HDInsight Hadoop Tools for Visual Studio](/documentation/articles/hdinsight-hadoop-visual-studio-tools-get-started)。\n\n## 故障排除：将 HDInsight 工具连接到 HDInsight Emulator\n\n1. 在连接到 HDInsight Emulator 时，即使对话框中显示 HiveServer2 已成功连接，你也必须将 Hive 配置文件（路径为 C:\\\\hdp\\\\hive-*version*\\\\conf\\\\hive-site.xml）中的 **hive.security.authorization.enabled 属性**手动设置为 **false**，然后重新启动本地 Emulator。仅当你在预览表的前 100 行时，HDInsight Tools for Visual Studio 才会连接到 HiveServer2。如果不打算使用此类查询，可将 Hive 配置保留原样。\n\n2. 如果你在运行 HDInsight Emulator 的计算机上使用动态 IP 分配 (DHCP)，则可能需要更新 C:\\\\hdp\\\\hadoop-*version*\\\\etc\\\\hadoop\\\\core-site.xml，并将 **hadoop.proxyuser.hadoop.hosts** 属性的值更改为 (*)。这样，Hadoop 用户便可以从所有主机进行连接，以模拟你在 Visual Studio 中输入的用户。\n\n        <property>\n            <name>hadoop.proxyuser.hadoop.hosts</name>\n            <value>*</value>\n        </property>\n\n3. 当 Visual Studio 尝试连接到 WebHCat 服务时，你可能会收到错误（错误: 找不到作业 job\\_XXXX\\_0001）。在这种情况下，必须重新启动 WebHCat 服务并重试。若要重新启动 WebHCat 服务，请启动“服务”MMC，右键单击“Apache Hadoop Templeton”（这是 WebHCat 服务的旧名称），然后单击“重新启动”。\n\n##<a name=\"runwordcount\"></a>单词计数 MapReduce 教程\n\n现在你已在工作站上配置了 HDInsight Emulator，接下来，请试着学习此 MapReduce 教程来测试安装。首先，请将一些文本文件上载到 HDFS，然后运行单词计数 MapReduce 作业以便计算特定单词在这些文件中出现的频率。\n\n单词计数 MapReduce 程序已打包到 *hadoop-mapreduce-examples-2.4.0.2.1.3.0-1981.jar* 中。该 jar 文件位于 *C:\\\\hdp\\\\hadoop-2.4.0.2.1.3.0-1981\\\\share\\\\hadoop\\\\mapreduce* 文件夹中。\n\n用于单词计数的 MapReduce 作业采用两个参数：\n\n- 一个输入文件夹。你将使用 *hdfs://localhost/user/HDIUser* 作为输入文件夹。\n- 一个输出文件夹。你将使用 *hdfs://localhost/user/HDIUser/WordCount_Output* 作为输出文件夹。输出文件夹不能是现有文件夹，否则 MapReduce 作业将失败。如果要第二次运行 MapReduce 作业，则必须指定一个不同的输出文件夹，或删除现有的输出文件夹。\n\n**运行单词计数 MapReduce 作业**\n\n1. 从桌面双击“Hadoop 命令行”以打开 Hadoop 命令行窗口。当前文件夹应为：\n\n        c:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\n\n    如果不是，请运行以下命令：\n\n        cd %hadoop_home%\n\n2. 运行以下 Hadoop 命令以使 HDFS 文件夹对输入和输出文件进行排序：\n\n        hadoop fs -mkdir /user\n        hadoop fs -mkdir /user/HDIUser\n\n3. 运行以下 Hadoop 命令以便将某些本地文本文件复制到 HDFS：\n\n        hadoop fs -copyFromLocal C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\doc\\hadoop\\common*.txt /user/HDIUser\n\n4. 运行以下命令以便列出 /user/HDIUser 文件夹中的文件：\n\n        hadoop fs -ls /user/HDIUser\n\n    你应该看到以下文件：\n\n        C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981>hadoop fs -ls /user/HDIUser\n        Found 4 items\n        -rw-r--r--   1 username hdfs     574261 2014-09-08 12:56 /user/HDIUser/CHANGES.txt\n        -rw-r--r--   1 username hdfs      15748 2014-09-08 12:56 /user/HDIUser/LICENSE.txt\n        -rw-r--r--   1 username hdfs        103 2014-09-08 12:56 /user/HDIUser/NOTICE.txt\n        -rw-r--r--   1 username hdfs       1397 2014-09-08 12:56 /user/HDIUser/README.txt\n\n5. 运行以下命令来运行单词计数 MapReduce 作业：\n\n        C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981>hadoop jar C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.4.0.2.1.3.0-1981.jar wordcount /user/HDIUser/*.txt /user/HDIUser/WordCount_Output\n\n6. 运行以下命令以便从输出文件中列出其中含有“windows”的单词数：\n\n        hadoop fs -cat /user/HDIUser/WordCount_Output/part-r-00000 | findstr \"windows\"\n\n    输出应该是：\n\n        C:\\hdp\\hadoop-2.4.0.2.1.3.0-1981>hadoop fs -cat /user/HDIUser/WordCount_Output/part-r-00000 | findstr \"windows\"\n        windows 4\n        windows.        2\n        windows/cygwin. 1\n\n有关 Hadoop 命令的详细信息，请参阅 [Hadoop 命令参考][hadoop-commands-manual]。\n\n##<a name=\"rungetstartedsamples\"></a>分析示例 web 日志数据\n\nHDInsight Emulator 安装提供了一些示例，以便用户能够在 Windows 上开始学习基于 Apache Hadoop 的服务。这些示例涉及在处理大型数据集时通常需要的一些任务。这些示例是根据上述 MapReduce 教程制作的，可帮助你熟悉与 MapReduce 编程模型及其生态系统。\n\n示例数据是围绕处理 IIS 万维网联盟 (W3C) 日志数据进行组织的。提供数据生成工具以便创建不同大小的数据集并将这些数据集导入到 HDFS 或 Azure Blob 存储中。（有关详细信息，请参阅[将 Azure Blob 存储用于 HDInsight](/documentation/articles/hdinsight-hadoop-use-blob-storage)）。然后，可以在 Azure PowerShell 脚本生成的数据页上运行 MapReduce、Pig 或 Hive 作业。使用的 Pig 和 Hive 脚本是基于 MapReduce 的抽象层，最终都会编译成 MapReduce 程序。你可以运行一系列作业，以便观察使用这些不同技术的影响以及数据大小对执行这些处理任务的影响。\n\n### 本节内容\n\n- [IIS W3C 日志数据方案](#scenarios)\n- [加载示例 W3C 日志数据](#loaddata)\n- [运行 Java MapReduce 作业](#javamapreduce)\n- [运行 Hive 作业](#hive)\n- [运行 Pig 作业](#pig)\n\n###<a name=\"scenarios\"></a>IIS W3C 日志数据方案\n\nW3C 方案生成以下三种大小的 IIS W3C 日志数据并将这些数据导入到 HDFS 或 Azure Blob 存储中：1MB（小）、500MB（中）和 2GB（大）。它提供三种作业类型并且分别在 C#、Java、Pig 和 Hive 中实现它们。\n\n- **totalhits** - 计算针对某一给定页的请求总数。\n- **avgtime** - 计算每页某一请求所用的平均时间（单位为秒）。\n- **errors** - 计算对于其状态为 404 或 500 的请求，每页、每小时的错误数。\n\n这些示例及其文档并未提供针对关键 Hadoop 技术的深入研究或完整实现。使用的群集只具有单个节点，因此对于此版本，无法观察添加多个节点的影响。\n\n###<a name=\"loaddata\"></a>加载示例 W3C 日志数据\n\n通过 Azure PowerShell 脚本 importdata.ps1 实现生成数据并且将数据导入到 HDFS。\n\n**导入示例 W3C 日志数据**\n\n1. 从桌面打开 Hadoop 命令行。\n2. 将目录切换到 **C:\\\\hdp\\\\GettingStarted**。\n3. 运行以下命令以生成数据并且将数据导入到 HDFS：\n\n        powershell -File importdata.ps1 w3c -ExecutionPolicy unrestricted\n\n    如果要改为将数据导入到 Azure Blob 存储，请参阅[连接到 Azure Blob 存储](#blobstorage)。\n\n4. 从 Hadoop 命令行运行以下命令以便列出 HDFS 上导入的文件：\n\n        hadoop fs -ls -R /w3c\n\n    输出应如下所示：\n\n        C:\\hdp\\GettingStarted>hadoop fs -ls -R /w3c\n        drwxr-xr-x   - username hdfs          0 2014-09-08 15:40 /w3c/input\n        drwxr-xr-x   - username hdfs          0 2014-09-08 15:41 /w3c/input/large\n        -rw-r--r--   1 username hdfs  543683503 2014-09-08 15:41 /w3c/input/large/data_w3c_large.txt\n        drwxr-xr-x   - username hdfs          0 2014-09-08 15:40 /w3c/input/medium\n        -rw-r--r--   1 username hdfs  272435159 2014-09-08 15:40 /w3c/input/medium/data_w3c_medium.txt\n        drwxr-xr-x   - username hdfs          0 2014-09-08 15:39 /w3c/input/small\n        -rw-r--r--   1 username hdfs    1058423 2014-09-08 15:39 /w3c/input/small/data_w3c_small.txt\n\n5. 如果你要验证文件内容，请运行以下命令，以在控制台窗口中显示一个数据文件：\n\n        hadoop fs -cat /w3c/input/small/data_w3c_small.txt\n\n现在，你已创建数据文件并已将其导入到 HDFS。你可以开始运行其他 Hadoop 作业。\n\n###<a name=\"javamapreduce\"></a>运行 Java MapReduce 作业\n\nMapReduce 是针对 Hadoop 的基本计算引擎。默认情况下，它是在 Java 中实现的，但也有利用采用 C# 的 .NET 和 Hadoop Streaming 的示例。运行 MapReduce 作业的语法是：\n\n    hadoop jar <jarFileName>.jar <className> <inputFiles> <outputFolder>\n\njar 文件和源文件位于 C:\\\\Hadoop\\\\GettingStarted\\\\Java 文件夹中。\n\n**运行 MapReduce 作业以便计算网页点击数**\n\n1. 打开 Hadoop 命令行。\n2. 将目录切换到 **C:\\\\hdp\\\\GettingStarted**。\n3. 运行以下命令以便在该文件夹存在时删除输出目录。如果输出文件夹已存在，该 MapReduce 作业将失败。\n\n        hadoop fs -rm -r /w3c/output\n\n3. 运行以下命令：\n\n        hadoop jar .\\Java\\w3c_scenarios.jar \"microsoft.hadoop.w3c.TotalHitsForPage\" \"/w3c/input/small/data_w3c_small.txt\" \"/w3c/output\"\n\n    下表介绍了该命令的元素：<table border=\"1\"> <tr><td>参数</td><td>备注</td></tr> <tr><td>w3c\\_scenarios.jar</td><td>该 jar 文件位于 C:\\\\hdp\\\\GettingStarted\\\\Java 文件夹中。</td></tr> <tr><td>microsoft.hadoop.w3c.TotalHitsForPage</td><td>可使用以下项之一替代该类型：<ul> <li>microsoft.hadoop.w3c.AverageTimeTaken</li> <li>microsoft.hadoop.w3c.ErrorsByPage</li> </ul></td></tr> <tr><td>/w3c/input/small/data\\_w3c\\_small.txt</td><td>可使用以下项之一替代该输入文件：<ul> <li>/w3c/input/medium/data\\_w3c\\_medium.txt</li> <li>/w3c/input/large/data\\_w3c\\_large.txt</li> </ul></td></tr> <tr><td>/w3c/output</td><td>这是输出文件夹名称。</td></tr> </table>\n\n4. 运行以下命令以显示输出文件：\n\n        hadoop fs -cat /w3c/output/part-00000\n\n    输出应如下所示：\n\n        c:\\Hadoop\\GettingStarted>hadoop fs -cat /w3c/output/part-00000\n        /Default.aspx   3380\n        /Info.aspx      1135\n        /UserService    1126\n\n    Default.aspx 页得到了 3360 次点击，依此类推。尝试根据上表中的建议替换值并再次运行这些命令，然后观察输出如何根据作业类型和数据大小发生变化。\n\n### <a name=\"hive\"></a>运行 Hive 作业\n精通结构化查询语言 (SQL) 的分析人士对于 Hive 查询引擎可能感到很熟悉。它提供与 SQL 类似的界面以及用于 HDFS 的关系数据模型。Hive 使用一种称作 HiveQL 的语言，这与 SQL 十分类似。Hive 在基于 Java 的 MapReduce 框架上提供了一个抽象层，Hive 查询在运行时将编译成 MapReduce。\n\n**运行 Hive 作业**\n\n1. 打开 Hadoop 命令行。\n2. 将目录切换到 **C:\\\\hdp\\\\GettingStarted**。\n3. 运行以下命令以便在 **/w3c/hive/input** 文件夹存在时删除该文件夹。如果该文件夹存在，则 hive 作业将失败。\n\n        hadoop fs -rmr /w3c/hive/input\n\n4. 运行以下命令以便创建 **/w3c/hive/input** 文件夹，然后将数据文件复制到 /hive/input：\n\n        hadoop fs -mkdir /w3c/hive\n        hadoop fs -mkdir /w3c/hive/input\n\n        hadoop fs -cp /w3c/input/small/data_w3c_small.txt /w3c/hive/input\n\n5. 运行以下命令以执行 **w3ccreate.hql** 脚本文件。该脚本将创建一个 Hive 表，并且将数据导入到该 Hive 表中：\n\n    > [AZURE.NOTE]在此阶段，你也可以使用 HDInsight Visual Studio 工具来运行 Hive 查询。打开 Visual Studio，创建一个新项目，然后从 HDInsight 模板中选择“Hive 应用程序”。打开该项目后，将查询添加为新项。该查询位于 **C:/hdp/GettingStarted/Hive/w3c** 中。将查询添加到项目后，将 **${hiveconf:input}** 替换为 **/w3c/hive/input**，然后按“提交”。\n\n        C:\\hdp\\hive-0.13.0.2.1.3.0-1981\\bin\\hive.cmd -f ./Hive/w3c/w3ccreate.hql -hiveconf \"input=/w3c/hive/input/data_w3c_small.txt\"\n\n    输出将如下所示：\n\n        Logging initialized using configuration in file:/C:/hdp/hive-0.13.0.2.1.3.0-1981    /conf/hive-log4j.properties\n        OK\n        Time taken: 1.137 seconds\n        OK\n        Time taken: 4.403 seconds\n        Loading data to table default.w3c\n        Moved: 'hdfs://HDINSIGHT02:8020/hive/warehouse/w3c' to trash at: hdfs://HDINSIGHT02:8020/user/<username>/.Trash/Current\n        Table default.w3c stats: [numFiles=1, numRows=0, totalSize=1058423, rawDataSize=0]\n        OK\n        Time taken: 2.881 seconds\n\n6. 运行以下命令以便运行 **w3ctotalhitsbypage.hql** HiveQL 脚本文件：\n\n    > [AZURE.NOTE]如前所述，你也可以使用 HDInsight Visual Studio 工具运行此查询。\n\n        C:\\hdp\\hive-0.13.0.2.1.3.0-1981\\bin\\hive.cmd -f ./Hive/w3c/w3ctotalhitsbypage.hql\n\n    下表描述了该命令的元素：<table border=\"1\"> <tr><td>文件</td><td>说明</td></tr> <tr><td>C:\\\\hdp\\\\hive-0.13.0.2.1.3.0-1981\\\\bin\\\\hive.cmd</td><td>Hive 命令脚本。</td></tr> <tr><td>C:\\\\hdp\\\\GettingStarted\\\\Hive\\\\w3c\\\\w3ctotalhitsbypage.hql</td><td> 你可以使用以下文件之一替代该 Hive 脚本文件：<ul> <li>C:\\\\hdp\\\\GettingStarted\\\\Hive\\\\w3c\\\\w3caveragetimetaken.hql</li> <li>C:\\\\hdp\\\\GettingStarted\\\\Hive\\\\w3c\\\\w3cerrorsbypage.hql</li> </ul> </td></tr>\n\n    </table>\n\n    w3ctotalhitsbypage.hql HiveQL 脚本是：\n\n        SELECT filtered.cs_uri_stem,COUNT(*)\n        FROM (\n          SELECT logdate,cs_uri_stem from w3c WHERE logdate NOT RLIKE '.*#.*'\n        ) filtered\n        GROUP BY (filtered.cs_uri_stem);\n\n    输出的末尾将如下所示：\n\n        MapReduce Total cumulative CPU time: 5 seconds 391 msec\n        Ended Job = job_1410201800143_0008\n        MapReduce Jobs Launched:\n        Job 0: Map: 1  Reduce: 1   Cumulative CPU: 5.391 sec   HDFS Read: 1058638 HDFS Write: 53 SUCCESS\n        Total MapReduce CPU Time Spent: 5 seconds 391 msec\n        OK\n        /Default.aspx   3380\n        /Info.aspx      1135\n        /UserService    1126\n        Time taken: 49.304 seconds, Fetched: 3 row(s)\n\n请注意，作为每个作业的第一步，将创建一个表并将之前创建的文件中的数据加载到该表中。你可以通过使用以下命令查看 HDFS 中的 /Hive 节点，浏览已创建的文件：\n\n    hadoop fs -lsr /apps/hive/\n\n### <a name=\"pig\"></a>运行 Pig 作业\n\nPig 处理使用称作 *Pig Latin* 的数据流语言。Pig Latin 抽象提供了比 MapReduce 更丰富的数据结构，并为 Hadoop 执行 SQL 对关系数据库管理系统执行的操作。\n\n\n**运行 pig 作业**\n\n1. 打开 Hadoop 命令行。\n2. 将目录切换为 **C:\\\\hdp\\\\GettingStarted** 文件夹。\n3. 运行以下命令来提交 Pig 作业：\n\n        C:\\hdp\\pig-0.12.1.2.1.3.0-1981\\bin\\pig.cmd -f \".\\Pig\\w3c\\TotalHitsForPage.pig\" -p \"input=/w3c/input/small/data_w3c_small.txt\"\n\n    下表显示了该命令的元素：\n    <table border=\"1\">\n    <tr><td>文件</td><td>说明</td></tr>\n    <tr><td>C:\\\\hdp\\\\pig-0.12.1.2.1.3.0-1981\\\\bin\\\\pig.cmd</td><td>Pig 命令脚本。</td></tr>\n    <tr><td>C:\\\\hdp\\\\GettingStarted\\\\Pig\\\\w3c\\\\TotalHitsForPage.pig</td><td>你可以使用以下文件之一替代该 Pig Latin 脚本文件：\n    <ul>\n    <li>C:\\\\hdp\\\\GettingStarted\\\\Pig\\\\w3c\\\\AverageTimeTaken.pig</li>\n    <li>C:\\\\hdp\\\\GettingStarted\\\\Pig\\\\w3c\\\\ErrorsByPage.pig</li>\n    </ul>\n    </td></tr>\n    <tr><td>/w3c/input/small/data\\_w3c\\_small.txt</td><td>你可以使用更大的文件替换该参数：\n\n    <ul>\n    <li>/w3c/input/medium/data_w3c_medium.txt</li>\n    <li>/w3c/input/large/data_w3c_large.txt</li>\n    </ul></td></tr> </table>\n\n    输出应如下所示：\n\n        (/Info.aspx,1135)\n        (/UserService,1126)\n        (/Default.aspx,3380)\n\n请注意，因为 Pig 脚本编译到 MapReduce 作业，并且可能会编译到多个此类作业，所以，你在处理 Pig 作业的过程中可能会看到多个 MapReduce 作业正在执行。\n\n<!---\n### <a name=\"rebuild\"></a>Rebuild the samples\nThe samples currently contain all the required binaries, so building is not required. If you'd like to make changes to the Java or .NET samples, you can rebuild them by using either the Microsoft Build Engine (MSBuild) or the included Azure PowerShell script.\n\n\n**To rebuild the samples**\n\n1. Open a Hadoop command line.\n2. Run the following command:\n\n        powershell -F buildsamples.ps1\n--->\n\n##<a name=\"blobstorage\"></a>连接到 Azure Blob 存储\nHDInsight Emulator 使用 HDFS 作为默认文件系统。但是，Azure HDInsight 使用 Azure Blob 存储作为默认文件系统。可以将 HDInsight Emulator 配置为使用 Azure Blob 存储而不是本地存储。遵照以下说明在 Azure 中创建存储容器，然后将它连接到 HDInsight Emulator。\n\n>[AZURE.NOTE]有关 HDInsight 如何使用 Azure Blob 存储的详细信息，请参阅[将 Azure Blob 存储与 HDInsight 配合使用](/documentation/articles/hdinsight-hadoop-use-blob-storage)。\n\n在开始遵照下面的说明之前，你必须创建存储帐户。有关说明，请参阅[如何创建存储帐户](/documentation/articles/storage-create-storage-account)。\n\n**创建容器**\n\n1. 登录到 [Azure 门户][azure-management-portal]。\n2. 单击左侧的“存储”。此时将显示订阅下的存储帐户列表。\n3. 单击要从该列表创建容器的存储帐户。\n4. 单击页面顶部的“容器”。\n5. 单击页面底部的“添加”。\n6. 输入“名称”并选择“访问”。你可以使用三种访问级别中的任何一种。默认级别为“私有”。\n7. 单击“确定”以保存更改。现在，门户上会列出新的容器。\n\n必须先将帐户名称和帐户密钥添加到配置文件，然后才能访问 Azure 存储帐户。\n\n**配置与 Azure 存储帐户的连接**\n\n1. 在记事本中打开 **C:\\\\hdp\\\\hadoop-2.4.0.2.1.3.0-1981\\\\etc\\\\hadoop\\\\core-site.xml**。\n2. 将以下 <property> 标记添加到其他 <property> 标记旁：\n\n        <property>\n            <name>fs.azure.account.key.<StorageAccountName>.blob.core.chinacloudapi.cn</name>\n            <value><StorageAccountKey></value>\n        </property>\n\n    你必须使用与你的存储帐户信息匹配的值替代 <StorageAccountName> 和 <StorageAccountKey>。\n\n3. 保存更改。你无需重新启动 Hadoop 服务。\n\n使用以下语法可访问该存储帐户：\n\n    wasb://<ContainerName>@<StorageAccountName>.blob.core.chinacloudapi.cn/\n\n例如：\n\n    hadoop fs -ls wasb://myContainer@myStorage.blob.core.chinacloudapi.cn/\n\n\n##<a name=\"powershell\"></a>运行 Azure PowerShell\nHDInsight Emulator 也支持某些 Azure PowerShell cmdlet。这些 cmdlet 包括：\n\n- HDInsight 作业定义 cmdlet\n\n    - New-AzureHDInsightSqoopJobDefinition\n    - New-AzureHDInsightStreamingMapReduceJobDefinition\n    - New-AzureHDInsightPigJobDefinition\n    - New-AzureHDInsightHiveJobDefinition\n    - New-AzureHDInsightMapReduceJobDefinition\n    - Start-AzureHDInsightJob\n    - Get-AzureHDInsightJob\n    - Wait-AzureHDInsightJob\n\n下面是用于提交 Hadoop 作业的示例：\n\n    $creds = Get-Credential (hadoop as username, password can be anything)\n    $hdinsightJob = <JobDefinition>\n    Start-AzureHDInsightJob -Cluster http://localhost:50111 -Credential $creds -JobDefinition $hdinsightJob\n\n在调用 Get-Credential 时系统将会向你显示一个提示。你必须将 **hadoop** 用作用户名。密码可以是任意字符串。群集名称始终是 **http://localhost:50111**。\n\n有关提交 Hadoop 作业的详细信息，请参阅[以编程方式提交 Hadoop 作业](/documentation/articles/hdinsight-submit-hadoop-jobs-programmatically)。有关适用于 HDInsight 的 Azure Powershell cmdlet 的详细信息，请参阅 [HDInsight cmdlet 参考][hdinsight-powershell-reference]。\n\n\n##<a name=\"remove\"></a>删除 HDInsight Emulator\n在安装模拟器的计算机上打开控制面板，然后在“程序”下面单击“卸载程序”。从已安装程序的列表中，右键单击“Microsoft HDInsight Emulator for Azure”，然后单击“卸载”。\n\n\n##<a name=\"nextsteps\"></a>后续步骤\n在本 MapReduce 教程中，你安装了 HDInsight Emulator - 一个 Hadoop 沙盒 - 并运行了一些 Hadoop 作业。若要了解更多信息，请参阅下列文章：\n\n- [Azure HDInsight 入门](/documentation/articles/hdinsight-hadoop-tutorial-get-started-windows-v1)\n- [为 HDInsight 开发 Java MapReduce 程序](/documentation/articles/hdinsight-develop-deploy-java-mapreduce)\n- [为 HDInsight 开发 C# Hadoop 流式处理 MapReduce 程序](/documentation/articles/hdinsight-hadoop-develop-deploy-streaming-jobs)\n- [HDInsight Emulator 发行说明](/documentation/articles/hdinsight-emulator-release-notes)\n- [讨论 HDInsight 的 MSDN 论坛](https://social.msdn.microsoft.com/Forums/zh-cn/home)\n\n\n\n[azure-sdk]: /downloads/\n[azure-create-storage-account]: /documentation/articles/storage-create-storage-account\n[azure-management-portal]: https://manage.windowsazure.cn/\n[netstat-url]: http://technet.microsoft.com/zh-cn/library/ff961504.aspx\n\n[hdinsight-develop-mapreduce]: /documentation/articles/hdinsight-develop-deploy-java-mapreduce\n[hdinsight-emulator-install]: http://www.microsoft.com/web/gallery/install.aspx?appid=HDINSIGHT\n[hdinsight-emulator-release-notes]: /documentation/articles/hdinsight-emulator-release-notes\n[hdinsight-storage]: /documentation/articles/hdinsight-hadoop-use-blob-storage\n[hdinsight-submit-jobs]: /documentation/articles/hdinsight-submit-hadoop-jobs-programmatically\n[hdinsight-powershell-reference]: http://msdn.microsoft.com/zh-cn/library/azure/dn479228.aspx\n[hdinsight-get-started]: /documentation/articles/hdinsight-get-started\n[hdinsight-develop-deploy-streaming]: /documentation/articles/hdinsight-hadoop-develop-deploy-streaming-jobs\n[hdinsight-versions]: /documentation/articles/hdinsight-component-versioning-v1\n[Powershell-install-configure]: /documentation/articles/powershell-install-configure\n\n[hadoop-commands-manual]: http://hadoop.apache.org/docs/r1.1.1/commands_manual.html\n\n[image-hdi-emulator-services]: ./media/hdinsight-hadoop-emulator-get-started/HDI.Emulator.Services.png\n\n<!---HONumber=74-->"
}