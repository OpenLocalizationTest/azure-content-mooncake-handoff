{
  "nodes": [
    {
      "content": "使用 Mahout 和基于 WIndows 的 HDInsight 生成推荐 | Azure",
      "pos": [
        27,
        73
      ]
    },
    {
      "content": "了解如何使用 Apache Mahout 机器学习库通过基于 Windows 的 HDInsight (Hadoop) 生成电影推荐。",
      "pos": [
        92,
        159
      ]
    },
    {
      "content": "将 Apache Mahout 与 HDInsight 中的 Hadoop 配合使用以生成电影推荐",
      "pos": [
        398,
        447
      ]
    },
    {
      "pos": [
        526,
        606
      ],
      "content": "了解如何使用 <bpt id=\"p1\">[</bpt>Apache Mahout<ept id=\"p1\">](http://mahout.apache.org)</ept> 机器学习库通过 Azure HDInsight 生成电影推荐。"
    },
    {
      "pos": [
        611,
        635
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"learn\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>学习内容"
    },
    {
      "pos": [
        637,
        768
      ],
      "content": "Mahout 是适用于 Apache Hadoop 的<bpt id=\"p1\">[</bpt>计算机学习<ept id=\"p1\">][ml]</ept>库。Mahout 包含用于处理数据的算法，例如筛选、分类和群集。在本文中，你将使用推荐引擎生成基于你的朋友看过的电影的电影推荐。你还将学习如何使用决策林执行分类。本文将为你传授以下知识："
    },
    {
      "content": "如何通过使用 Windows PowerShell 运行 Mahout 作业",
      "pos": [
        772,
        810
      ]
    },
    {
      "content": "如何从 Hadoop 命令行运行 Mahout 作业",
      "pos": [
        814,
        840
      ]
    },
    {
      "content": "如何在 HDInsight 3.0 和 HDInsight 2.0 群集上安装 Mahout",
      "pos": [
        844,
        890
      ]
    },
    {
      "pos": [
        898,
        1001
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Mahout 是随 HDInsight 3.1 版本的群集一起提供的。如果你使用的是早期版本的 HDInsight，请在继续操作之前参阅<bpt id=\"p1\">[</bpt>安装 Mahout<ept id=\"p1\">](#install)</ept>。"
    },
    {
      "content": "先决条件",
      "pos": [
        1005,
        1009
      ]
    },
    {
      "pos": [
        1013,
        1104
      ],
      "content": "<bpt id=\"p1\">**</bpt>HDInsight 中基于 Windows 的 Hadoop 群集<ept id=\"p1\">**</ept>。有关创建该群集的信息，请参阅<bpt id=\"p2\">[</bpt>开始使用 HDInsight 中的 Hadoop<ept id=\"p2\">][getstarted]</ept>。"
    },
    {
      "pos": [
        1107,
        1272
      ],
      "content": "<bpt id=\"p1\">**</bpt>配备 Azure PowerShell 的工作站<ept id=\"p1\">**</ept>。请参阅<bpt id=\"p2\">[</bpt>安装 Azure PowerShell 1.0 和更高版本<ept id=\"p2\">](/documentation/articles/hdinsight-administer-use-powershell#install-azure-powershell-10-and-greater)</ept>。"
    },
    {
      "pos": [
        1277,
        1333
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"recommendations\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>使用 Windows PowerShell 生成推荐"
    },
    {
      "pos": [
        1337,
        1518
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> 尽管在本部分中使用的作业使用 Windows PowerShell 执行，但是，随 Mahout 一起提供的很多类当前不使用 Windows PowerShell，并且它们必须通过使用 Hadoop 命令行来运行。有关不使用 Windows PowerShell 的类的列表，请参阅<bpt id=\"p1\">[</bpt>故障排除<ept id=\"p1\">](#troubleshooting)</ept>部分。"
    },
    {
      "content": "有关使用 Hadoop 命令行运行 Mahout 作业的示例，请参阅<bpt id=\"p1\">[</bpt><ept id=\"p1\">使用 Hadoop 命令行对数据分类](#classify)</ept>。",
      "pos": [
        1524,
        1590
      ]
    },
    {
      "pos": [
        1592,
        1744
      ],
      "content": "由 Mahout 提供的功能之一是推荐引擎。此引擎接受 <ph id=\"ph1\">`userID`</ph>、<ph id=\"ph2\">`itemId`</ph> 和 <ph id=\"ph3\">`prefValue`</ph> 格式（项的用户首选项）的数据。然后，Mahout 将执行共现分析，以确定：<bpt id=\"p1\">_</bpt>偏好某个项的用户也偏好其他类似项<ept id=\"p1\">_</ept>。Mahout 然后确定拥有类似项首选项的用户，这些首选项可用于做出推荐。"
    },
    {
      "content": "下面是使用电影的极其简单的示例：",
      "pos": [
        1746,
        1762
      ]
    },
    {
      "pos": [
        1766,
        1846
      ],
      "content": "<bpt id=\"p1\">__</bpt>共现<ept id=\"p1\">__</ept>：Joe、Alice 和 Bob 都喜欢电影<bpt id=\"p2\">_</bpt>星球大战<ept id=\"p2\">_</ept>、<bpt id=\"p3\">_</bpt>帝国反击战<ept id=\"p3\">_</ept>和<bpt id=\"p4\">_</bpt>绝地大反击<ept id=\"p4\">_</ept>。Mahout 可确定喜欢以上电影之一的用户也喜欢其他两部。"
    },
    {
      "pos": [
        1850,
        1929
      ],
      "content": "<bpt id=\"p1\">__</bpt>共现<ept id=\"p1\">__</ept>：Bob 和 Alice 还喜欢电影<bpt id=\"p2\">_</bpt>幽灵的威胁<ept id=\"p2\">_</ept>、<bpt id=\"p3\">_</bpt>克隆人的进攻<ept id=\"p3\">_</ept>和<bpt id=\"p4\">_</bpt>西斯的复仇<ept id=\"p4\">_</ept>。Mahout 可确定喜欢前面三部电影的用户也喜欢这三部电影。"
    },
    {
      "pos": [
        1933,
        2046
      ],
      "content": "<bpt id=\"p1\">__</bpt>类似性推荐<ept id=\"p1\">__</ept>：由于 Joe 喜欢前三部电影，Mahout 会查看具有类似首选项的其他人喜欢的电影，但是 Joe 还未观看过（喜欢/评价）。在这种情况下，Mahout 推荐《幽灵的威胁》、《克隆人的进攻》和《西斯的复仇》。"
    },
    {
      "content": "加载数据",
      "pos": [
        2051,
        2055
      ]
    },
    {
      "pos": [
        2057,
        2112
      ],
      "content": "为方便起见，<bpt id=\"p1\">[</bpt>GroupLens 研究<ept id=\"p1\">][movielens]</ept>以兼容 Mahout 的格式提供电影的评价数据。"
    },
    {
      "pos": [
        2117,
        2185
      ],
      "content": "下载 <bpt id=\"p1\">[</bpt>MovieLens 100k<ept id=\"p1\">][100k]</ept> 存档，其中包含 1000 名用户对 1700 部电影给出的 100,000 项评分。"
    },
    {
      "content": "提取该存档。该存档应包含 <bpt id=\"p1\">__</bpt>ml-100k<ept id=\"p1\">__</ept> 目录，其中又包含很多前缀为 <bpt id=\"p2\">__</bpt>u.<ept id=\"p2\">__</ept>",
      "pos": [
        2190,
        2235
      ]
    },
    {
      "content": "的数据文件。Mahout 要分析的文件为 <bpt id=\"p1\">__</bpt>u.data<ept id=\"p1\">__</ept>。此文件的数据结构为 <ph id=\"ph1\">`userID`</ph>、<ph id=\"ph2\">`movieID`</ph>、<ph id=\"ph3\">`userRating`</ph> 和 <ph id=\"ph4\">`timestamp`</ph>。下面是数据的示例：",
      "pos": [
        2236,
        2333
      ]
    },
    {
      "pos": [
        2492,
        2708
      ],
      "content": "将 <bpt id=\"p1\">__</bpt>u.data<ept id=\"p1\">__</ept> 文件上载到 HDInsight 群集中的 <bpt id=\"p2\">__</bpt>example/data/u.data<ept id=\"p2\">__</ept>。如果你安装了 <bpt id=\"p3\">[</bpt>Azure PowerShell<ept id=\"p3\">][aps]</ept>，则可以使用 <bpt id=\"p4\">[</bpt>HDInsight-Tools<ept id=\"p4\">][tools]</ept> 模块上载该文件。有关上载文件的其他方法，请参阅<bpt id=\"p5\">[</bpt>在 HDInsight 中上载 Hadoop 作业的数据<ept id=\"p5\">][upload]</ept>。以下命令使用 <ph id=\"ph1\">`Add-HDInsightFile`</ph> 来上载该文件："
    },
    {
      "pos": [
        2853,
        2980
      ],
      "content": "这样就会将 <bpt id=\"p1\">__</bpt>u.data<ept id=\"p1\">__</ept> 文件上载到群集的默认存储中的 <bpt id=\"p2\">__</bpt>example/data/u.data<ept id=\"p2\">__</ept>。然后，你可以通过使用 \\_\\_<bpt id=\"p3\">__</bpt>wasb:///example/data/u.data<ept id=\"p3\">__</ept> URI 从 HDInsight 作业访问此数据。"
    },
    {
      "content": "运行作业",
      "pos": [
        2985,
        2989
      ]
    },
    {
      "pos": [
        2991,
        3060
      ],
      "content": "使用以下 Windows PowerShell 脚本来运行作业，以将 Mahout 推荐引擎用于你以前上载的 <bpt id=\"p1\">__</bpt>u.data<ept id=\"p1\">__</ept> 文件："
    },
    {
      "pos": [
        4809,
        4887
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Mahout 作业不删除在处理作业时创建的临时数据。在示例作业中指定 <ph id=\"ph2\">`--tempDir`</ph> 参数，以将临时文件隔离到特定目录中。"
    },
    {
      "pos": [
        4889,
        4991
      ],
      "content": "Mahout 作业不会将输出返回到 STDOUT。而是会将其作为 <bpt id=\"p1\">__</bpt>part-r-00000<ept id=\"p1\">__</ept> 存储在指定的输出目录中。该脚本将此文件下载到你工作站上的当前目录中的 <bpt id=\"p2\">__</bpt>output.txt<ept id=\"p2\">__</ept> 中。"
    },
    {
      "content": "下面是此文件内容的示例：",
      "pos": [
        4993,
        5005
      ]
    },
    {
      "pos": [
        5408,
        5468
      ],
      "content": "第一列是 <ph id=\"ph1\">`userID`</ph>。“[”和“]”中包含的值为 <ph id=\"ph2\">`movieId`</ph>:<ph id=\"ph3\">`recommendationScore`</ph>。"
    },
    {
      "content": "查看输出",
      "pos": [
        5473,
        5477
      ]
    },
    {
      "pos": [
        5479,
        5591
      ],
      "content": "尽管生成的输出也许能够正常地在应用程序中使用，但它的用户可读性并不太好。以前提取到 <bpt id=\"p1\">__</bpt>ml-100k<ept id=\"p1\">__</ept> 文件夹的其他某些文件可用于将 <ph id=\"ph1\">`movieId`</ph> 解析为电影名称，这是以下 PowerShell 脚本所执行的操作："
    },
    {
      "pos": [
        8463,
        8507
      ],
      "content": "若要使用此脚本，你必须之前已提取 <bpt id=\"p1\">__</bpt>ml-100k<ept id=\"p1\">__</ept> 文件夹。下面是运行脚本的示例："
    },
    {
      "content": "输出应如下所示：",
      "pos": [
        8648,
        8656
      ]
    },
    {
      "pos": [
        9796,
        9841
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"classify\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>通过使用 Hadoop 命令行对数据进行分类"
    },
    {
      "pos": [
        9843,
        10011
      ],
      "content": "Mahout 提供的分类方法之一是生成<bpt id=\"p1\">[</bpt>随机林<ept id=\"p1\">][forest]</ept>。这是一个多步骤过程，涉及到使用训练数据来生成决策树，然后使用决策树对数据进行分类。此过程使用 Mahout 提供的 <bpt id=\"p2\">__</bpt>org.apache.mahout.classifier.df.tools.Describe<ept id=\"p2\">__</ept> 类。它当前必须通过使用 Hadoop 命令行来运行。"
    },
    {
      "content": "加载数据",
      "pos": [
        10016,
        10020
      ]
    },
    {
      "pos": [
        10025,
        10114
      ],
      "content": "从 <bpt id=\"p1\">[</bpt>NSL-KDD 数据集<ept id=\"p1\">](http://www.unb.ca/research/iscx/dataset/iscx-NSL-KDD-dataset.html)</ept>下载以下文件。"
    },
    {
      "content": "KDDTrain+.ARFF：训练文件",
      "pos": [
        10120,
        10139
      ]
    },
    {
      "content": "KDDTest+.ARFF：测试数据",
      "pos": [
        10145,
        10163
      ]
    },
    {
      "content": "打开每个文件，删除顶部以“@”开头的行，然后保存文件。如果未删除这些行，则你在 Mahout 中使用数据时将会收到错误消息。",
      "pos": [
        10168,
        10230
      ]
    },
    {
      "pos": [
        10235,
        10325
      ],
      "content": "将文件上载到 <bpt id=\"p1\">__</bpt>example/data<ept id=\"p1\">__</ept>。你可以通过使用 <bpt id=\"p2\">[</bpt>HDInsight-Tools<ept id=\"p2\">][tools]</ept> 模块中的 <ph id=\"ph1\">`Add-HDInsightFile`</ph> 函数执行此操作。"
    },
    {
      "content": "运行作业",
      "pos": [
        10330,
        10334
      ]
    },
    {
      "pos": [
        10339,
        10492
      ],
      "content": "此作业需要 Hadoop 命令行。为 HDInsight 群集启用远程桌面，然后按照<bpt id=\"p1\">[</bpt>使用 RDP 连接到 HDInsight 群集<ept id=\"p1\">](/documentation/articles/hdinsight-administer-use-management-portal-v1#rdp)</ept>中的说明连接到该群集。"
    },
    {
      "content": "建立连接后，使用“Hadoop 命令行”图标打开 Hadoop 命令行：",
      "pos": [
        10497,
        10533
      ]
    },
    {
      "content": "hadoop cli",
      "pos": [
        10541,
        10551
      ]
    },
    {
      "pos": [
        10568,
        10617
      ],
      "content": "使用以下命令生成文件描述符 (<bpt id=\"p1\">__</bpt>KDDTrain+.info<ept id=\"p1\">__</ept>)，该描述符使用 Mahout。"
    },
    {
      "pos": [
        10904,
        10960
      ],
      "content": "<ph id=\"ph1\">`N 3 C 2 N C 4 N C 8 N 2 C 19 N L`</ph> 描述文件中数据的属性。例如，L 指示标签。"
    },
    {
      "content": "通过使用以下命令生成决策树的林：",
      "pos": [
        10965,
        10981
      ]
    },
    {
      "pos": [
        11296,
        11441
      ],
      "content": "此操作的输出存储在 <bpt id=\"p1\">__</bpt>nsl-forest<ept id=\"p1\">__</ept> 目录中，该目录位于 HDInsight 群集的存储中的 <bpt id=\"p2\">__</bpt>wasb://user/&amp;lt;username&gt;/nsl-forest/nsl-forest.seq<ept id=\"p2\">__</ept> 处。&amp;lt;用户名&gt; 是你用于远程桌面会话的用户名。此文件对用户不可读。"
    },
    {
      "pos": [
        11446,
        11487
      ],
      "content": "通过为 <bpt id=\"p1\">__</bpt>KDDTest+.arff<ept id=\"p1\">__</ept> 数据集分类来测试该林。请使用以下命令："
    },
    {
      "content": "此命令返回如下有关分类过程的摘要信息：",
      "pos": [
        11795,
        11814
      ]
    },
    {
      "pos": [
        12922,
        13004
      ],
      "content": "此作业还将生成位于 <bpt id=\"p1\">__</bpt>wasb:///example/data/predictions/KDDTest+.arff.out<ept id=\"p1\">__</ept> 的文件。但是，此文件对用户不可读。"
    },
    {
      "pos": [
        13008,
        13066
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph> Mahout 作业不会覆盖文件。如果要再次运行这些作业，则必须删除由以前的作业创建的文件。"
    },
    {
      "pos": [
        13070,
        13104
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"troubleshooting\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>故障排除"
    },
    {
      "pos": [
        13109,
        13140
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"install\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>安装 Mahout"
    },
    {
      "content": "Mahout 安装在 HDInsight 3.1 群集上，它可以通过使用以下步骤手动安装在 HDInsight 3.0 或 HDInsight 2.1 群集上：",
      "pos": [
        13142,
        13222
      ]
    },
    {
      "pos": [
        13227,
        13304
      ],
      "content": "要使用的 Mahout 版本取决于群集的 HDInsight 版本。你可以通过使用以下 <bpt id=\"p1\">[</bpt>Azure PowerShell<ept id=\"p1\">][aps]</ept> 命令查找群集版本："
    },
    {
      "pos": [
        13392,
        13541
      ],
      "content": "<bpt id=\"p1\">__</bpt>对于 HDInsight 2.1<ept id=\"p1\">__</ept>，可以下载包含 <bpt id=\"p2\">[</bpt>Mahout 0.9<ept id=\"p2\">](http://repo2.maven.org/maven2/org/apache/mahout/mahout-core/0.9/mahout-core-0.9-job.jar)</ept> 的 Java 存档 (JAR) 文件。"
    },
    {
      "pos": [
        13547,
        13663
      ],
      "content": "<bpt id=\"p1\">__</bpt>对于 HDInsight 3.0<ept id=\"p1\">__</ept>，必须<bpt id=\"p2\">[</bpt>从源生成 Mahout<ept id=\"p2\">][build]</ept> 并指定 HDInsight 提供的 Hadoop 版本。安装构建页上列出的必备组件，并下载源，然后使用以下命令创建 Mahout jar 文件："
    },
    {
      "pos": [
        13986,
        14084
      ],
      "content": "将该 jar 文件上载到群集默认存储的 <bpt id=\"p1\">__</bpt>example/jars<ept id=\"p1\">__</ept> 中。以下示例使用 <bpt id=\"p2\">[</bpt>HDInsight-Tools<ept id=\"p2\">][tools]</ept> 中的 add-hdinsightfile 来上载文件："
    },
    {
      "content": "无法覆盖文件",
      "pos": [
        14261,
        14267
      ]
    },
    {
      "content": "Mahout 作业不清理在处理期间创建的临时文件。此外，作业将不会覆盖现有的输出文件。",
      "pos": [
        14269,
        14312
      ]
    },
    {
      "content": "若要避免运行 Mahout 作业时出错，请在每次运行作业之前删除临时文件和输出文件，或者使用唯一的临时目录名称和输出目录名称。",
      "pos": [
        14314,
        14377
      ]
    },
    {
      "content": "找不到 JAR 文件",
      "pos": [
        14382,
        14392
      ]
    },
    {
      "content": "HDInsight 3.1 群集提供 Mahout。路径和文件名包括在群集上安装的 Mahout 的版本号。本教程中的 Windows PowerShell 示例脚本使用的路径的有效截止期为 2014 年 7 月，但是，将来对 HDInsight 做出更新后，版本号将发生更改。若要确定群集的 Mahout JAR 文件的当前路径，请使用以下 Windows PowerShell 命令，然后修改脚本以引用返回的文件路径：",
      "pos": [
        14394,
        14605
      ]
    },
    {
      "pos": [
        14769,
        14822
      ],
      "content": "<ph id=\"ph1\">&lt;a name=\"nopowershell\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>不适用于 Windows PowerShell 的类"
    },
    {
      "content": "Mahout 作业如果使用以下类，则从 Windows PowerShell 中使用这些类时，将返回各种错误消息：",
      "pos": [
        14824,
        14881
      ]
    },
    {
      "content": "org.apache.mahout.utils.clustering.ClusterDumper",
      "pos": [
        14885,
        14933
      ]
    },
    {
      "content": "org.apache.mahout.utils.SequenceFileDumper",
      "pos": [
        14936,
        14978
      ]
    },
    {
      "content": "org.apache.mahout.utils.vectors.lucene.Driver",
      "pos": [
        14981,
        15026
      ]
    },
    {
      "content": "org.apache.mahout.utils.vectors.arff.Driver",
      "pos": [
        15029,
        15072
      ]
    },
    {
      "content": "org.apache.mahout.text.WikipediaToSequenceFile",
      "pos": [
        15075,
        15121
      ]
    },
    {
      "content": "org.apache.mahout.clustering.streaming.tools.ResplitSequenceFiles",
      "pos": [
        15124,
        15189
      ]
    },
    {
      "content": "org.apache.mahout.clustering.streaming.tools.ClusterQualitySummarizer",
      "pos": [
        15192,
        15261
      ]
    },
    {
      "content": "org.apache.mahout.classifier.sgd.TrainLogistic",
      "pos": [
        15264,
        15310
      ]
    },
    {
      "content": "org.apache.mahout.classifier.sgd.RunLogistic",
      "pos": [
        15313,
        15357
      ]
    },
    {
      "content": "org.apache.mahout.classifier.sgd.TrainAdaptiveLogistic",
      "pos": [
        15360,
        15414
      ]
    },
    {
      "content": "org.apache.mahout.classifier.sgd.ValidateAdaptiveLogistic",
      "pos": [
        15417,
        15474
      ]
    },
    {
      "content": "org.apache.mahout.classifier.sgd.RunAdaptiveLogistic",
      "pos": [
        15477,
        15529
      ]
    },
    {
      "content": "org.apache.mahout.classifier.sequencelearning.hmm.BaumWelchTrainer",
      "pos": [
        15532,
        15598
      ]
    },
    {
      "content": "org.apache.mahout.classifier.sequencelearning.hmm.ViterbiEvaluator",
      "pos": [
        15601,
        15667
      ]
    },
    {
      "content": "org.apache.mahout.classifier.sequencelearning.hmm.RandomSequenceGenerator",
      "pos": [
        15670,
        15743
      ]
    },
    {
      "content": "org.apache.mahout.classifier.df.tools.Describe",
      "pos": [
        15746,
        15792
      ]
    },
    {
      "pos": [
        15794,
        15889
      ],
      "content": "若要运行使用这些类的作业，请连接到 HDInsight 群集，然后通过使用 Hadoop 命令行运行这些作业。有关示例，请参阅<bpt id=\"p1\">[</bpt>使用 Hadoop 命令行对数据分类<ept id=\"p1\">](#classify)</ept>。"
    },
    {
      "content": "后续步骤",
      "pos": [
        15893,
        15897
      ]
    },
    {
      "content": "现在，你已经学习了如何使用 Mahout，因此可以探索通过其他方式来使用 HDInsight 上的数据：",
      "pos": [
        15899,
        15951
      ]
    },
    {
      "content": "Hive 和 HDInsight",
      "pos": [
        15956,
        15972
      ]
    },
    {
      "content": "Pig 和 HDInsight",
      "pos": [
        16039,
        16054
      ]
    },
    {
      "content": "MapReduce 和 HDInsight",
      "pos": [
        16120,
        16141
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"使用 Mahout 和基于 WIndows 的 HDInsight 生成推荐 | Azure\"\n    description=\"了解如何使用 Apache Mahout 机器学习库通过基于 Windows 的 HDInsight (Hadoop) 生成电影推荐。\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"Blackmist\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.date=\"01/08/2016\"\n    wacn.date=\"02/26/2016\"/>\n\n#将 Apache Mahout 与 HDInsight 中的 Hadoop 配合使用以生成电影推荐\n\n[AZURE.INCLUDE [mahout-selector](../includes/hdinsight-selector-mahout.md)]\n\n了解如何使用 [Apache Mahout](http://mahout.apache.org) 机器学习库通过 Azure HDInsight 生成电影推荐。\n\n\n##<a name=\"learn\"></a>学习内容\n\nMahout 是适用于 Apache Hadoop 的[计算机学习][ml]库。Mahout 包含用于处理数据的算法，例如筛选、分类和群集。在本文中，你将使用推荐引擎生成基于你的朋友看过的电影的电影推荐。你还将学习如何使用决策林执行分类。本文将为你传授以下知识：\n\n* 如何通过使用 Windows PowerShell 运行 Mahout 作业\n\n* 如何从 Hadoop 命令行运行 Mahout 作业\n\n* 如何在 HDInsight 3.0 和 HDInsight 2.0 群集上安装 Mahout\n\n    > [AZURE.NOTE] Mahout 是随 HDInsight 3.1 版本的群集一起提供的。如果你使用的是早期版本的 HDInsight，请在继续操作之前参阅[安装 Mahout](#install)。\n\n##先决条件\n\n- **HDInsight 中基于 Windows 的 Hadoop 群集**。有关创建该群集的信息，请参阅[开始使用 HDInsight 中的 Hadoop][getstarted]。\n- **配备 Azure PowerShell 的工作站**。请参阅[安装 Azure PowerShell 1.0 和更高版本](/documentation/articles/hdinsight-administer-use-powershell#install-azure-powershell-10-and-greater)。\n\n\n##<a name=\"recommendations\"></a>使用 Windows PowerShell 生成推荐\n\n> [AZURE.NOTE] 尽管在本部分中使用的作业使用 Windows PowerShell 执行，但是，随 Mahout 一起提供的很多类当前不使用 Windows PowerShell，并且它们必须通过使用 Hadoop 命令行来运行。有关不使用 Windows PowerShell 的类的列表，请参阅[故障排除](#troubleshooting)部分。\n> <p>有关使用 Hadoop 命令行运行 Mahout 作业的示例，请参阅[使用 Hadoop 命令行对数据分类](#classify)。\n\n由 Mahout 提供的功能之一是推荐引擎。此引擎接受 `userID`、`itemId` 和 `prefValue` 格式（项的用户首选项）的数据。然后，Mahout 将执行共现分析，以确定：_偏好某个项的用户也偏好其他类似项_。Mahout 然后确定拥有类似项首选项的用户，这些首选项可用于做出推荐。\n\n下面是使用电影的极其简单的示例：\n\n* __共现__：Joe、Alice 和 Bob 都喜欢电影_星球大战_、_帝国反击战_和_绝地大反击_。Mahout 可确定喜欢以上电影之一的用户也喜欢其他两部。\n\n* __共现__：Bob 和 Alice 还喜欢电影_幽灵的威胁_、_克隆人的进攻_和_西斯的复仇_。Mahout 可确定喜欢前面三部电影的用户也喜欢这三部电影。\n\n* __类似性推荐__：由于 Joe 喜欢前三部电影，Mahout 会查看具有类似首选项的其他人喜欢的电影，但是 Joe 还未观看过（喜欢/评价）。在这种情况下，Mahout 推荐《幽灵的威胁》、《克隆人的进攻》和《西斯的复仇》。\n\n###加载数据\n\n为方便起见，[GroupLens 研究][movielens]以兼容 Mahout 的格式提供电影的评价数据。\n\n1. 下载 [MovieLens 100k][100k] 存档，其中包含 1000 名用户对 1700 部电影给出的 100,000 项评分。\n\n2. 提取该存档。该存档应包含 __ml-100k__ 目录，其中又包含很多前缀为 __u.__ 的数据文件。Mahout 要分析的文件为 __u.data__。此文件的数据结构为 `userID`、`movieID`、`userRating` 和 `timestamp`。下面是数据的示例：\n\n\n        196 242 3   881250949\n        186 302 3   891717742\n        22  377 1   878887116\n        244 51  2   880606923\n        166 346 1   886397596\n\n\n3.  将 __u.data__ 文件上载到 HDInsight 群集中的 __example/data/u.data__。如果你安装了 [Azure PowerShell][aps]，则可以使用 [HDInsight-Tools][tools] 模块上载该文件。有关上载文件的其他方法，请参阅[在 HDInsight 中上载 Hadoop 作业的数据][upload]。以下命令使用 `Add-HDInsightFile` 来上载该文件：\n\n        PS C:\\> Add-HDInsightFile -LocalPath \"path\\to\\u.data\" -DestinationPath \"example/data/u.data\" -ClusterName \"your cluster name\"\n    \n    这样就会将 __u.data__ 文件上载到群集的默认存储中的 __example/data/u.data__。然后，你可以通过使用 \\_\\___wasb:///example/data/u.data__ URI 从 HDInsight 作业访问此数据。\n\n###运行作业\n\n使用以下 Windows PowerShell 脚本来运行作业，以将 Mahout 推荐引擎用于你以前上载的 __u.data__ 文件：\n\n    # The HDInsight cluster name.\n    $clusterName = \"the cluster name\"\n    # NOTE: The version number portion of the file path\n    # may change in future versions of HDInsight.\n    # So dynamically grab it using Hive.\n    $mahoutPath = Invoke-Hive -Query '!${env:COMSPEC} /c dir /b /s ${env:MAHOUT_HOME}\\examples\\target*-job.jar' | where {$_.startswith(\"C:\\apps\\dist\")}\n    $noCRLF = $mahoutPath -replace \"`r`n\", \"\"\n    $cleanedPath = $noCRLF -replace \"\\\", \"/\"\n    $jarFile = \"file:///$cleanedPath\"\n    #\n    # If you are using an earlier version of HDInsight,\n    # set $jarFile to the jar file you\n    # uploaded.\n    # For example,\n    # $jarFile = \"wasb:///example/jars/mahout-core-0.9-job.jar\"\n\n    # The arguments for this job\n    # * input - the path to the data uploaded to HDInsight\n    # * output - the path to store output data\n    # * tempDir - the directory for temp files\n    $jobArguments = \"-s\", \"SIMILARITY_COOCCURRENCE\",\n                    \"--input\", \"wasb:///example/data/u.data\",\n                    \"--output\", \"wasb:///example/out\",\n                    \"--tempDir\", \"wasb:///temp/mahout\"\n\n    # Create the job definition\n    $jobDefinition = New-AzureHDInsightMapReduceJobDefinition `\n      -JarFile $jarFile `\n      -ClassName \"org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\" `\n      -Arguments $jobArguments\n\n    # Start the job\n    $job = Start-AzureHDInsightJob -Cluster $clusterName -JobDefinition $jobDefinition\n\n    # Wait on the job to complete\n    Write-Host \"Wait for the job to complete ...\" -ForegroundColor Green\n    Wait-AzureHDInsightJob -Job $job\n\n    # Write out any error information\n    Write-Host \"STDERR\"\n    Get-AzureHDInsightJobOutput -Cluster $clusterName -JobId $job.JobId -StandardError\n\n> [AZURE.NOTE] Mahout 作业不删除在处理作业时创建的临时数据。在示例作业中指定 `--tempDir` 参数，以将临时文件隔离到特定目录中。\n\nMahout 作业不会将输出返回到 STDOUT。而是会将其作为 __part-r-00000__ 存储在指定的输出目录中。该脚本将此文件下载到你工作站上的当前目录中的 __output.txt__ 中。\n\n下面是此文件内容的示例：\n\n    1   [234:5.0,347:5.0,237:5.0,47:5.0,282:5.0,275:5.0,88:5.0,515:5.0,514:5.0,121:5.0]\n    2   [282:5.0,210:5.0,237:5.0,234:5.0,347:5.0,121:5.0,258:5.0,515:5.0,462:5.0,79:5.0]\n    3   [284:5.0,285:4.828125,508:4.7543354,845:4.75,319:4.705128,124:4.7045455,150:4.6938777,311:4.6769233,248:4.65625,272:4.649266]\n    4   [690:5.0,12:5.0,234:5.0,275:5.0,121:5.0,255:5.0,237:5.0,895:5.0,282:5.0,117:5.0]\n\n第一列是 `userID`。“[”和“]”中包含的值为 `movieId`:`recommendationScore`。\n\n###查看输出\n\n尽管生成的输出也许能够正常地在应用程序中使用，但它的用户可读性并不太好。以前提取到 __ml-100k__ 文件夹的其他某些文件可用于将 `movieId` 解析为电影名称，这是以下 PowerShell 脚本所执行的操作：\n\n    <#\n    .SYNOPSIS\n        Displays recommendations for movies.\n    .DESCRIPTION\n        Displays recommendations generated by Mahout\n        with HDInsight example in a human readable format.\n    .EXAMPLE\n        .\\Show-Recommendation -userId 4\n            -userDataFile \"u.data\"\n            -movieFile \"u.item\"\n            -recommendationFile \"output.txt\"\n    #>\n\n    [CmdletBinding(SupportsShouldProcess = $true)]\n    param(\n        #The user ID\n        [Parameter(Mandatory = $true)]\n        [String]$userId,\n\n        [Parameter(Mandatory = $true)]\n        [String]$userDataFile,\n\n        [Parameter(Mandatory = $true)]\n        [String]$movieFile,\n\n        [Parameter(Mandatory = $true)]\n        [String]$recommendationFile\n    )\n    # Read movie ID & description into hash table\n    Write-Host \"Reading movies descriptions\" -ForegroundColor Green\n    $movieById = @{}\n    foreach($line in Get-Content $movieFile)\n    {\n        $tokens = $line.Split(\"|\")\n        $movieById[$tokens[0]] = $tokens[1]\n    }\n    # Load movies user has already seen (rated)\n    # into a hash table\n    Write-Host \"Reading rated movies\" -ForegroundColor Green\n    $ratedMovieIds = @{}\n    foreach($line in Get-Content $userDataFile)\n    {\n        $tokens = $line.Split(\"`t\")\n        if($tokens[0] -eq $userId)\n        {\n            # Resolve the ID to the movie name\n            $ratedMovieIds[$movieById[$tokens[1]]] = $tokens[2]\n        }\n    }\n    # Read recommendations generated by Mahout\n    Write-Host \"Reading recommendations\" -ForegroundColor Green\n    $recommendations = @{}\n    foreach($line in get-content $recommendationFile)\n    {\n        $tokens = $line.Split(\"`t\")\n        if($tokens[0] -eq $userId)\n        {\n            #Trim leading/treailing [] and split at ,\n            $movieIdAndScores = $tokens[1].TrimStart(\"[\").TrimEnd(\"]\").Split(\",\")\n            foreach($movieIdAndScore in $movieIdAndScores)\n            {\n                #Split at : and store title and score in a hash table\n                $idAndScore = $movieIdAndScore.Split(\":\")\n                $recommendations[$movieById[$idAndScore[0]]] = $idAndScore[1]\n            }\n            break\n        }\n    }\n\n    Write-Host \"Rated movies\" -ForegroundColor Green\n    Write-Host \"---------------------------\" -ForegroundColor Green\n    $ratedFormat = @{Expression={$_.Name};Label=\"Movie\";Width=40}, `\n                   @{Expression={$_.Value};Label=\"Rating\"}\n    $ratedMovieIds | format-table $ratedFormat\n    Write-Host \"---------------------------\" -ForegroundColor Green\n\n    write-host \"Recommended movies\" -ForegroundColor Green\n    Write-Host \"---------------------------\" -ForegroundColor Green\n    $recommendationFormat = @{Expression={$_.Name};Label=\"Movie\";Width=40}, `\n                            @{Expression={$_.Value};Label=\"Score\"}\n    $recommendations | format-table $recommendationFormat\n\n若要使用此脚本，你必须之前已提取 __ml-100k__ 文件夹。下面是运行脚本的示例：\n\n    PS C:\\> show-recommendation.ps1 -userId 4 -userDataFile .\\ml-100k\\u.data -movieFile .\\ml-100k\\u.item -recommendationFile .\\output.txt\n\n输出应如下所示：\n\n    Reading movies descriptions\n    Reading rated movies\n    Reading recommendations\n    Rated movies\n    ---------------------------\n    Movie                                    Rating\n    -----                                    ------\n    Devil's Own, The (1997)                  1\n    Alien: Resurrection (1997)               3\n    187 (1997)                               2\n    (lines ommitted)\n\n    ---------------------------\n    Recommended movies\n    ---------------------------\n\n    Movie                                    Score\n    -----                                    -----\n    Good Will Hunting (1997)                 4.6504064\n    Swingers (1996)                          4.6862745\n    Wings of the Dove, The (1997)            4.6666665\n    People vs. Larry Flynt, The (1996)       4.834559\n    Everyone Says I Love You (1996)          4.707071\n    Secrets & Lies (1996)                    4.818182\n    That Thing You Do! (1996)                4.75\n    Grosse Pointe Blank (1997)               4.8235292\n    Donnie Brasco (1997)                     4.6792455\n    Lone Star (1996)                         4.7099237  \n\n##<a name=\"classify\"></a>通过使用 Hadoop 命令行对数据进行分类\n\nMahout 提供的分类方法之一是生成[随机林][forest]。这是一个多步骤过程，涉及到使用训练数据来生成决策树，然后使用决策树对数据进行分类。此过程使用 Mahout 提供的 __org.apache.mahout.classifier.df.tools.Describe__ 类。它当前必须通过使用 Hadoop 命令行来运行。\n\n###加载数据\n\n1. 从 [NSL-KDD 数据集](http://www.unb.ca/research/iscx/dataset/iscx-NSL-KDD-dataset.html)下载以下文件。\n\n  * KDDTrain+.ARFF：训练文件\n\n  * KDDTest+.ARFF：测试数据\n\n2. 打开每个文件，删除顶部以“@”开头的行，然后保存文件。如果未删除这些行，则你在 Mahout 中使用数据时将会收到错误消息。\n\n2. 将文件上载到 __example/data__。你可以通过使用 [HDInsight-Tools][tools] 模块中的 `Add-HDInsightFile` 函数执行此操作。\n\n###运行作业\n\n1. 此作业需要 Hadoop 命令行。为 HDInsight 群集启用远程桌面，然后按照[使用 RDP 连接到 HDInsight 群集](/documentation/articles/hdinsight-administer-use-management-portal-v1#rdp)中的说明连接到该群集。\n\n3. 建立连接后，使用“Hadoop 命令行”图标打开 Hadoop 命令行：\n\n    ![hadoop cli][hadoopcli]\n\n3. 使用以下命令生成文件描述符 (__KDDTrain+.info__)，该描述符使用 Mahout。\n\n        hadoop jar \"c:/apps/dist/mahout-0.9.0.2.2.7.1-37/examples/target/mahout-examples-0.9.0.2.2.7.1-37-job.jar\" org.apache.mahout.classifier.df.tools.Describe -p \"wasb:///example/data/KDDTrain+.arff\" -f \"wasb:///example/data/KDDTrain+.info\" -d N 3 C 2 N C 4 N C 8 N 2 C 19 N L\n\n    `N 3 C 2 N C 4 N C 8 N 2 C 19 N L` 描述文件中数据的属性。例如，L 指示标签。\n\n4. 通过使用以下命令生成决策树的林：\n\n        hadoop jar c:/apps/dist/mahout-0.9.0.2.2.7.1-37/examples/target/mahout-examples-0.9.0.2.2.7.1-37-job.jar org.apache.mahout.classifier.df.mapreduce.BuildForest -Dmapred.max.split.size=1874231 -d wasb:///example/data/KDDTrain+.arff -ds wasb:///example/data/KDDTrain+.info -sl 5 -p -t 100 -o nsl-forest\n\n    此操作的输出存储在 __nsl-forest__ 目录中，该目录位于 HDInsight 群集的存储中的 __wasb://user/&lt;username>/nsl-forest/nsl-forest.seq__ 处。&lt;用户名> 是你用于远程桌面会话的用户名。此文件对用户不可读。\n\n5. 通过为 __KDDTest+.arff__ 数据集分类来测试该林。请使用以下命令：\n\n        hadoop jar c:/apps/dist/mahout-0.9.0.2.2.7.1-37/examples/target/mahout-examples-0.9.0.2.2.7.1-37-job.jar org.apache.mahout.classifier.df.mapreduce.TestForest -i wasb:///example/data/KDDTest+.arff -ds wasb:///example/data/KDDTrain+.info -m nsl-forest -a -mr -o wasb:///example/data/predictions\n\n    此命令返回如下有关分类过程的摘要信息：\n\n        14/07/02 14:29:28 INFO mapreduce.TestForest:\n\n        =======================================================\n        Summary\n        -------------------------------------------------------\n        Correctly Classified Instances          :      17560       77.8921%\n        Incorrectly Classified Instances        :       4984       22.1079%\n        Total Classified Instances              :      22544\n\n        =======================================================\n        Confusion Matrix\n        -------------------------------------------------------\n        a       b       <--Classified as\n        9437    274      |  9711        a     = normal\n        4710    8123     |  12833       b     = anomaly\n\n        =======================================================\n        Statistics\n        -------------------------------------------------------\n        Kappa                                       0.5728\n        Accuracy                                   77.8921%\n        Reliability                                53.4921%\n        Reliability (standard deviation)            0.4933\n\n  此作业还将生成位于 __wasb:///example/data/predictions/KDDTest+.arff.out__ 的文件。但是，此文件对用户不可读。\n\n> [AZURE.NOTE] Mahout 作业不会覆盖文件。如果要再次运行这些作业，则必须删除由以前的作业创建的文件。\n\n##<a name=\"troubleshooting\"></a>故障排除\n\n###<a name=\"install\"></a>安装 Mahout\n\nMahout 安装在 HDInsight 3.1 群集上，它可以通过使用以下步骤手动安装在 HDInsight 3.0 或 HDInsight 2.1 群集上：\n\n1. 要使用的 Mahout 版本取决于群集的 HDInsight 版本。你可以通过使用以下 [Azure PowerShell][aps] 命令查找群集版本：\n\n        PS C:\\> Get-AzureHDInsightCluster -Name YourClusterName | Select version\n\n  * __对于 HDInsight 2.1__，可以下载包含 [Mahout 0.9](http://repo2.maven.org/maven2/org/apache/mahout/mahout-core/0.9/mahout-core-0.9-job.jar) 的 Java 存档 (JAR) 文件。\n\n  * __对于 HDInsight 3.0__，必须[从源生成 Mahout][build] 并指定 HDInsight 提供的 Hadoop 版本。安装构建页上列出的必备组件，并下载源，然后使用以下命令创建 Mahout jar 文件：\n\n            mvn -Dhadoop2.version=2.2.0 -DskipTests clean package\n\n        After the build completes, you can find the JAR file at __mahout\\mrlegacy\\target\\mahout-mrlegacy-1.0-SNAPSHOT-job.jar__.\n\n        > [AZURE.NOTE] When Mahout 1.0 is released, you should be able to use the prebuilt packages with HDInsight 3.0.\n\n2. 将该 jar 文件上载到群集默认存储的 __example/jars__ 中。以下示例使用 [HDInsight-Tools][tools] 中的 add-hdinsightfile 来上载文件：\n\n        PS C:\\> .\\Add-HDInsightFile -LocalPath \"path\\to\\mahout-core-0.9-job.jar\" -DestinationPath \"example/jars/mahout-core-0.9-job.jar\" -ClusterName \"your cluster name\"\n\n\n###无法覆盖文件\n\nMahout 作业不清理在处理期间创建的临时文件。此外，作业将不会覆盖现有的输出文件。\n\n若要避免运行 Mahout 作业时出错，请在每次运行作业之前删除临时文件和输出文件，或者使用唯一的临时目录名称和输出目录名称。\n\n###找不到 JAR 文件\n\nHDInsight 3.1 群集提供 Mahout。路径和文件名包括在群集上安装的 Mahout 的版本号。本教程中的 Windows PowerShell 示例脚本使用的路径的有效截止期为 2014 年 7 月，但是，将来对 HDInsight 做出更新后，版本号将发生更改。若要确定群集的 Mahout JAR 文件的当前路径，请使用以下 Windows PowerShell 命令，然后修改脚本以引用返回的文件路径：\n\n    Use-AzureHDInsightCluster -Name $clusterName\n    $jarFile = Invoke-Hive -Query '!${env:COMSPEC} /c dir /b /s ${env:MAHOUT_HOME}\\examples\\target*-job.jar'\n\n###<a name=\"nopowershell\"></a>不适用于 Windows PowerShell 的类\n\nMahout 作业如果使用以下类，则从 Windows PowerShell 中使用这些类时，将返回各种错误消息：\n\n* org.apache.mahout.utils.clustering.ClusterDumper\n* org.apache.mahout.utils.SequenceFileDumper\n* org.apache.mahout.utils.vectors.lucene.Driver\n* org.apache.mahout.utils.vectors.arff.Driver\n* org.apache.mahout.text.WikipediaToSequenceFile\n* org.apache.mahout.clustering.streaming.tools.ResplitSequenceFiles\n* org.apache.mahout.clustering.streaming.tools.ClusterQualitySummarizer\n* org.apache.mahout.classifier.sgd.TrainLogistic\n* org.apache.mahout.classifier.sgd.RunLogistic\n* org.apache.mahout.classifier.sgd.TrainAdaptiveLogistic\n* org.apache.mahout.classifier.sgd.ValidateAdaptiveLogistic\n* org.apache.mahout.classifier.sgd.RunAdaptiveLogistic\n* org.apache.mahout.classifier.sequencelearning.hmm.BaumWelchTrainer\n* org.apache.mahout.classifier.sequencelearning.hmm.ViterbiEvaluator\n* org.apache.mahout.classifier.sequencelearning.hmm.RandomSequenceGenerator\n* org.apache.mahout.classifier.df.tools.Describe\n\n若要运行使用这些类的作业，请连接到 HDInsight 群集，然后通过使用 Hadoop 命令行运行这些作业。有关示例，请参阅[使用 Hadoop 命令行对数据分类](#classify)。\n\n##后续步骤\n\n现在，你已经学习了如何使用 Mahout，因此可以探索通过其他方式来使用 HDInsight 上的数据：\n\n* [Hive 和 HDInsight](/documentation/articles/hdinsight-hadoop-use-hive-powershell)\n* [Pig 和 HDInsight](/documentation/articles/hdinsight-hadoop-use-pig-powershell)\n* [MapReduce 和 HDInsight](/documentation/articles/hdinsight-hadoop-use-mapreduce-powershell)\n\n[build]: http://mahout.apache.org/developers/buildingmahout.html\n[aps]: /documentation/articles/powershell-install-configure\n[movielens]: http://grouplens.org/datasets/movielens/\n[100k]: http://files.grouplens.org/datasets/movielens/ml-100k.zip\n[getstarted]: /documentation/articles/hdinsight-hadoop-tutorial-get-started-windows-v1\n[upload]: /documentation/articles/hdinsight-upload-data\n[ml]: http://en.wikipedia.org/wiki/Machine_learning\n[forest]: http://en.wikipedia.org/wiki/Random_forest\n[management]: https://manage.windowsazure.cn/\n[enableremote]: ./media/hdinsight-mahout/enableremote.png\n[connect]: ./media/hdinsight-mahout/connect.png\n[hadoopcli]: ./media/hdinsight-mahout/hadoopcli.png\n[tools]: https://github.com/Blackmist/hdinsight-tools\n \n\n<!---HONumber=Mooncake_0215_2016-->"
}