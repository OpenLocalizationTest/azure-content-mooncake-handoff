{
  "nodes": [
    {
      "content": "使用 HDInsight 中的 Hadoop 分析航班延误数据 | Azure",
      "pos": [
        27,
        66
      ]
    },
    {
      "content": "了解如何使用一个 Windows PowerShell 脚本来创建 HDInsight 群集、运行 Hive 作业、运行 Sqoop 作业和删除群集。",
      "pos": [
        85,
        160
      ]
    },
    {
      "content": "使用 HDInsight 中的 Hive 分析航班延误数据",
      "pos": [
        372,
        401
      ]
    },
    {
      "pos": [
        403,
        503
      ],
      "content": "Hive 提供了通过类似 SQL 的脚本语言（称为 <bpt id=\"p1\">*</bpt><bpt id=\"p2\">[</bpt>HiveQL<ept id=\"p2\">][hadoop-hiveql]</ept><ept id=\"p1\">*</ept>）运行 Hadoop MapReduce 作业的方法，此方法可用于对大量数据进行汇总、查询和分析。"
    },
    {
      "content": "Azure HDInsight 的主要优势之一就是隔离数据存储和计算。HDInsight 将 Azure Blob 存储用于数据存储。典型的作业包含 3 部分：",
      "pos": [
        505,
        585
      ]
    },
    {
      "pos": [
        590,
        682
      ],
      "content": "<bpt id=\"p1\">**</bpt>将数据存储在 Azure Blob 存储中。<ept id=\"p1\">**</ept> 这可能是一个连续的过程。例如，将天气数据、传感器数据、Web 日志以及此示例中的航班延误数据保存到 Azure Blob 存储中。"
    },
    {
      "pos": [
        686,
        829
      ],
      "content": "<bpt id=\"p1\">**</bpt>运行作业。<ept id=\"p1\">**</ept> 该处理数据时，你可以运行 Windows PowerShell 脚本（或客户端应用程序）以创建 HDInsight 群集、运行作业，然后删除该群集。作业将输出数据保存到 Azure Blob 存储。甚至在删除该群集后，输出数据也会保留。这样，你仅为已使用的内容付费。"
    },
    {
      "pos": [
        833,
        884
      ],
      "content": "<bpt id=\"p1\">**</bpt>从 Azure Blob 存储检索输出<ept id=\"p1\">**</ept>，或在此教程中将数据导出到 Azure SQL 数据库。"
    },
    {
      "content": "下图演示了本教程的方案和结构：",
      "pos": [
        886,
        901
      ]
    },
    {
      "content": "HDI.FlightDelays.flow",
      "pos": [
        905,
        926
      ]
    },
    {
      "pos": [
        956,
        1003
      ],
      "content": "<bpt id=\"p1\">**</bpt>注意<ept id=\"p1\">**</ept>：图中的编号对应于章节标题。<bpt id=\"p2\">**</bpt>M<ept id=\"p2\">**</ept> 代表主进程。<bpt id=\"p3\">**</bpt>A<ept id=\"p3\">**</ept> 代表附录中的内容。"
    },
    {
      "content": "教程的主要部分说明如何使用一个 PowerShell 脚本来执行以下操作：",
      "pos": [
        1005,
        1042
      ]
    },
    {
      "content": "创建 HDInsight 群集。",
      "pos": [
        1046,
        1062
      ]
    },
    {
      "content": "在群集上运行 Hive 作业，以计算机场的平均延迟。航班延误数据会存储在 Azure Blob 存储帐户中。",
      "pos": [
        1065,
        1119
      ]
    },
    {
      "content": "运行 Sqoop 作业将 Hive 作业输出导出至 Azure SQL 数据库。",
      "pos": [
        1122,
        1162
      ]
    },
    {
      "content": "删除 HDInsight 群集。",
      "pos": [
        1165,
        1181
      ]
    },
    {
      "content": "在附录中，你可以找到有关上载航班延误数据、创建/上载 Hive 查询字符串和针对 Sqoop 作业准备 Azure SQL 数据库的说明。",
      "pos": [
        1183,
        1252
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a id=\"prerequisite\"&gt;</ph> <ph id=\"ph2\">&lt;/a&gt;</ph>",
      "pos": [
        1253,
        1279
      ]
    },
    {
      "content": "先决条件",
      "pos": [
        1283,
        1287
      ]
    },
    {
      "content": "在开始阅读本教程前，你必须具有：",
      "pos": [
        1289,
        1305
      ]
    },
    {
      "pos": [
        1309,
        1365
      ],
      "content": "<bpt id=\"p1\">**</bpt>一个 Azure 订阅<ept id=\"p1\">**</ept>。请参阅<bpt id=\"p2\">[</bpt>获取 Azure 试用版<ept id=\"p2\">](/pricing/1rmb-trial/)</ept>。"
    },
    {
      "pos": [
        1369,
        1480
      ],
      "content": "<bpt id=\"p1\">**</bpt>配备 Azure PowerShell 的工作站<ept id=\"p1\">**</ept>。请参阅<bpt id=\"p2\">[</bpt>安装和使用 Azure PowerShell<ept id=\"p2\">](/documentation/articles/powershell-install-configure)</ept>。"
    },
    {
      "content": "本教程中使用的文件",
      "pos": [
        1484,
        1493
      ]
    },
    {
      "pos": [
        1497,
        1767
      ],
      "content": "本教程将使用来自<bpt id=\"p1\">[</bpt>美国研究与技术创新管理部门 - 运输统计局或 RITA<ept id=\"p1\">][rita-website]</ept> 的航班准时表现数据。数据的副本已上载至具有公共 Blob 访问权限的 Azure Blob 存储容器。PowerShell 脚本的一部分将数据从公共 blob 容器复制到你的群集的默认 blob 容器。HiveQL 脚本也会复制到同一 Blob 容器。如果想要了解如何将数据获取/上载到你自己的存储帐户，以及如何创建/上载 HiveQL 脚本文件，请参阅<bpt id=\"p2\">[</bpt>附录 A<ept id=\"p2\">](#appendix-a)</ept> 和<bpt id=\"p3\">[</bpt>附录 B<ept id=\"p3\">](#appendix-b)</ept>。"
    },
    {
      "content": "下表列出了本教程中使用的文件：",
      "pos": [
        1769,
        1784
      ]
    },
    {
      "content": "文件",
      "pos": [
        1813,
        1815
      ]
    },
    {
      "content": "说明",
      "pos": [
        1824,
        1826
      ]
    },
    {
      "content": "wasb://flightdelay@hditutorialdata.blob.core.windows.net/flightdelays.hql",
      "pos": [
        1845,
        1918
      ]
    },
    {
      "content": "你要运行的 Hive 作业所用的 HiveQL 脚本文件。此脚本已上载到具有公共访问权限的 Azure Blob 存储帐户。<ph id=\"ph1\">&lt;a href=\"#appendix-b\"&gt;</ph>附录 B<ph id=\"ph2\">&lt;/a&gt;</ph> 提供了有关准备此文件以及将其上载到你自己的 Azure Blob 存储帐户的说明。",
      "pos": [
        1927,
        2061
      ]
    },
    {
      "content": "wasb://flightdelay@hditutorialdata.blob.core.windows.net/2013Data",
      "pos": [
        2080,
        2145
      ]
    },
    {
      "content": "Hive 作业的输入的数据。这些数据已上载到具有公共访问权限的 Azure Blob 存储帐户。<ph id=\"ph1\">&lt;a href=\"#appendix-a\"&gt;</ph>附录 A<ph id=\"ph2\">&lt;/a&gt;</ph> 提供了有关获取数据以及将数据上载到你自己的 Azure Blob 存储帐户的说明。",
      "pos": [
        2154,
        2274
      ]
    },
    {
      "content": "\\tutorials\\flightdelays\\output",
      "pos": [
        2293,
        2323
      ]
    },
    {
      "content": "Hive 作业的输出路径。默认容器用于存储输出数据。",
      "pos": [
        2332,
        2358
      ]
    },
    {
      "content": "\\tutorials\\flightdelays\\jobstatus",
      "pos": [
        2377,
        2410
      ]
    },
    {
      "content": "默认容器上的 Hive 作业状态文件夹。",
      "pos": [
        2419,
        2439
      ]
    },
    {
      "content": "创建群集并运行 Hive/Sqoop 作业",
      "pos": [
        2463,
        2484
      ]
    },
    {
      "pos": [
        2486,
        2696
      ],
      "content": "Hadoop MapReduce 属于批处理。运行 Hive 作业时，最具成本效益的方法是为作业创建群集，并在作业完成之后删除作业。以下脚本覆盖了整个过程。有关创建 HDInsight 群集和运行 Hive 作业的详细信息，请参阅<bpt id=\"p1\">[</bpt>在 HDInsight 中创建 Hadoop 群集<ept id=\"p1\">][hdinsight-provision]</ept>和<bpt id=\"p2\">[</bpt>将 Hive 与 HDInsight 配合使用<ept id=\"p2\">][hdinsight-use-hive]</ept>。"
    },
    {
      "content": "使用 Azure PowerShell 运行 Hive 查询",
      "pos": [
        2700,
        2730
      ]
    },
    {
      "pos": [
        2737,
        2795
      ],
      "content": "按照<bpt id=\"p1\">[</bpt>附录 C<ept id=\"p1\">](#appendix-c)</ept> 中的说明，为 Sqoop 作业输出创建 Azure SQL 数据库和表。"
    },
    {
      "content": "准备参数：",
      "pos": [
        2799,
        2804
      ]
    },
    {
      "content": "变量名",
      "pos": [
        2837,
        2840
      ]
    },
    {
      "content": "说明",
      "pos": [
        2849,
        2851
      ]
    },
    {
      "content": "$hdinsightClusterName",
      "pos": [
        2870,
        2891
      ]
    },
    {
      "content": "HDInsight 群集名称。如果群集不存在，脚本会使用输入的名称创建一个群集。",
      "pos": [
        2900,
        2940
      ]
    },
    {
      "content": "$storageAccountName",
      "pos": [
        2959,
        2978
      ]
    },
    {
      "content": "将用作默认存储帐户的 Azure 存储帐户。只有在脚本需要创建 HDInsight 群集时才需要此值。如果你已为 $hdinsightClusterName 指定了现有的 HDInsight 群集名称，请保留空白。如果具有输入值的存储帐户不存在，则脚本会使用该名称创建帐户。",
      "pos": [
        2987,
        3124
      ]
    },
    {
      "content": "$blobContainerName",
      "pos": [
        3143,
        3161
      ]
    },
    {
      "content": "将用于默认文件系统的 Blob 容器。如果你将它保留空白，将使用 $hdinsightClusterName 值。",
      "pos": [
        3170,
        3227
      ]
    },
    {
      "content": "$sqlDatabaseServerName",
      "pos": [
        3246,
        3268
      ]
    },
    {
      "content": "Azure SQL 数据库服务器名称。必须是现有的服务器。有关创建服务器的信息，请参阅<ph id=\"ph1\">&lt;a href=\"#appendix-c\"&gt;</ph>附录 C<ph id=\"ph2\">&lt;/a&gt;</ph>。",
      "pos": [
        3277,
        3351
      ]
    },
    {
      "content": "$sqlDatabaseUsername",
      "pos": [
        3370,
        3390
      ]
    },
    {
      "content": "Azure SQL 数据库服务器登录名。",
      "pos": [
        3399,
        3419
      ]
    },
    {
      "content": "$sqlDatabasePassword",
      "pos": [
        3438,
        3458
      ]
    },
    {
      "content": "Azure SQL 数据库服务器登录密码。",
      "pos": [
        3467,
        3488
      ]
    },
    {
      "content": "$sqlDatabaseName",
      "pos": [
        3507,
        3523
      ]
    },
    {
      "content": "Sqoop 将数据导出到的 SQL 数据库。默认名称是 HDISqoop。Sqooop 作业输出的表名称为 AvgDelays。",
      "pos": [
        3532,
        3596
      ]
    },
    {
      "content": "<ph id=\"ph1\">\n3.</ph> 打开 Windows PowerShell 集成脚本环境 (ISE)。\n4.",
      "pos": [
        3615,
        3657
      ]
    },
    {
      "content": "将以下脚本复制并粘贴到脚本窗格中：",
      "pos": [
        3658,
        3675
      ]
    },
    {
      "pos": [
        16678,
        16700
      ],
      "content": "按 <bpt id=\"p1\">**</bpt>F5<ept id=\"p1\">**</ept> 运行脚本。输出应如下所示："
    },
    {
      "content": "HDI.FlightDelays.RunHiveJob.output",
      "pos": [
        16708,
        16742
      ]
    },
    {
      "content": "连接到 SQL 数据库，并在 AvgDelays 表中按城市查看平均航班延迟：",
      "pos": [
        16790,
        16829
      ]
    },
    {
      "content": "HDI.FlightDelays.AvgDelays.Dataset",
      "pos": [
        16837,
        16871
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a id=\"appendix-a\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>附录 A - 将航班延误数据上载到 Azure Blob 存储",
      "pos": [
        16924,
        16978
      ]
    },
    {
      "content": "上载数据文件和 HiveQL 脚本文件（请参阅<bpt id=\"p1\">[</bpt>附录 B<ept id=\"p1\">](#appendix-b)</ept>）需要进行规划。思路是在创建 HDInsight 群集和运行 Hive 作业之前存储数据文件和 HiveQL 文件。可以使用两个选项：",
      "pos": [
        16979,
        17088
      ]
    },
    {
      "pos": [
        17092,
        17178
      ],
      "content": "<bpt id=\"p1\">**</bpt>使用将由 HDInsight 群集用作默认文件系统的同一 Azure 存储帐户。<ept id=\"p1\">**</ept> 由于 HDInsight 群集将具有存储帐户访问密钥，因此你无需进行任何其他更改。"
    },
    {
      "pos": [
        17181,
        17413
      ],
      "content": "<bpt id=\"p1\">**</bpt>使用与 HDInsight 群集默认文件系统不同的 Azure 存储帐户。<ept id=\"p1\">**</ept> 如果选择了此项，你必须修改<bpt id=\"p2\">[</bpt>创建 HDInsight 群集和运行 Hive/Sqoop 作业<ept id=\"p2\">](#runjob)</ept>中的 Windows PowerShell 脚本的创建部分，以链接该存储帐户作为额外的存储帐户。有关说明，请参阅<bpt id=\"p3\">[</bpt>在 HDInsight 中创建 Hadoop 群集<ept id=\"p3\">][hdinsight-provision]</ept>。这样，HDInsight 群集就会知道存储帐户的访问密钥。"
    },
    {
      "pos": [
        17416,
        17474
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>数据文件的 WASB 路径会在 HiveQL 脚本文件中进行硬编码。你必须相应地更新该路径。"
    },
    {
      "content": "下载航班数据",
      "pos": [
        17478,
        17484
      ]
    },
    {
      "pos": [
        17491,
        17532
      ],
      "content": "浏览到<bpt id=\"p1\">[</bpt>美国研究与技术创新管理部门 - 运输统计局<ept id=\"p1\">][rita-website]</ept>。"
    },
    {
      "content": "在该页面上，选择以下值：",
      "pos": [
        17536,
        17548
      ]
    },
    {
      "pos": [
        17554,
        18035
      ],
      "content": "<table border=\"1\">\n <tr><th>Name</th><th>值</th></tr>\n <tr><td>筛选年份</td><td>2013 </td></tr>\n <tr><td>筛选期间</td><td>1 月</td></tr>\n <tr><td>字段</td><td>*Year*、*FlightDate*、*UniqueCarrier*、*Carrier*、*FlightNum*、*OriginAirportID*、*Origin*、*OriginCityName*、*OriginState*、*DestAirportID*、*Dest*、*DestCityName*、*DestState*、*DepDelayMinutes*、*ArrDelay*、*ArrDelayMinutes*、*CarrierDelay*、*WeatherDelay*、*NASDelay*、*SecurityDelay*、*LateAircraftDelay*（清除其他所有字段）</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "Name",
          "pos": [
            28,
            32
          ]
        },
        {
          "content": "值",
          "pos": [
            41,
            42
          ]
        },
        {
          "content": "筛选年份",
          "pos": [
            62,
            66
          ]
        },
        {
          "content": "2013",
          "pos": [
            75,
            79
          ]
        },
        {
          "content": "筛选期间",
          "pos": [
            100,
            104
          ]
        },
        {
          "content": "1 月",
          "pos": [
            113,
            116
          ]
        },
        {
          "content": "字段",
          "pos": [
            136,
            138
          ]
        },
        {
          "content": "<bpt id=\"p1\">*</bpt>Year<ept id=\"p1\">*</ept>、<bpt id=\"p2\">*</bpt>FlightDate<ept id=\"p2\">*</ept>、<bpt id=\"p3\">*</bpt>UniqueCarrier<ept id=\"p3\">*</ept>、<bpt id=\"p4\">*</bpt>Carrier<ept id=\"p4\">*</ept>、<bpt id=\"p5\">*</bpt>FlightNum<ept id=\"p5\">*</ept>、<bpt id=\"p6\">*</bpt>OriginAirportID<ept id=\"p6\">*</ept>、<bpt id=\"p7\">*</bpt>Origin<ept id=\"p7\">*</ept>、<bpt id=\"p8\">*</bpt>OriginCityName<ept id=\"p8\">*</ept>、<bpt id=\"p9\">*</bpt>OriginState<ept id=\"p9\">*</ept>、<bpt id=\"p10\">*</bpt>DestAirportID<ept id=\"p10\">*</ept>、<bpt id=\"p11\">*</bpt>Dest<ept id=\"p11\">*</ept>、<bpt id=\"p12\">*</bpt>DestCityName<ept id=\"p12\">*</ept>、<bpt id=\"p13\">*</bpt>DestState<ept id=\"p13\">*</ept>、<bpt id=\"p14\">*</bpt>DepDelayMinutes<ept id=\"p14\">*</ept>、<bpt id=\"p15\">*</bpt>ArrDelay<ept id=\"p15\">*</ept>、<bpt id=\"p16\">*</bpt>ArrDelayMinutes<ept id=\"p16\">*</ept>、<bpt id=\"p17\">*</bpt>CarrierDelay<ept id=\"p17\">*</ept>、<bpt id=\"p18\">*</bpt>WeatherDelay<ept id=\"p18\">*</ept>、<bpt id=\"p19\">*</bpt>NASDelay<ept id=\"p19\">*</ept>、<bpt id=\"p20\">*</bpt>SecurityDelay<ept id=\"p20\">*</ept>、<bpt id=\"p21\">*</bpt>LateAircraftDelay<ept id=\"p21\">*</ept>（清除其他所有字段）",
          "pos": [
            147,
            446
          ]
        }
      ]
    },
    {
      "content": "单击“下载”。",
      "pos": [
        18040,
        18047
      ]
    },
    {
      "pos": [
        18051,
        18125
      ],
      "content": "将文件解压缩到 <bpt id=\"p1\">**</bpt>C:\\\\Tutorials\\\\FlightDelays\\\\Data<ept id=\"p1\">**</ept> 文件夹。每个文件均为 CSV 文件且大小约为 60GB。"
    },
    {
      "pos": [
        18130,
        18187
      ],
      "content": "将文件重命名为其包含的数据所对应的月份的名称。例如，将包含 1 月份数据的文件命名为 <bpt id=\"p1\">*</bpt>January.csv<ept id=\"p1\">*</ept>。"
    },
    {
      "content": "重复步骤 2 和步骤 5 为 2013 年中的 12 个月分别下载一个对应的文件。完成本教程到少要有一个文件。",
      "pos": [
        18191,
        18246
      ]
    },
    {
      "content": "将航班延迟数据上载到 Azure Blob 存储",
      "pos": [
        18252,
        18276
      ]
    },
    {
      "content": "准备参数：",
      "pos": [
        18283,
        18288
      ]
    },
    {
      "pos": [
        18294,
        18500
      ],
      "content": "<table border=\"1\">\n <tr><th>变量名</th><th>说明</th></tr>\n <tr><td>$storageAccountName</td><td>要将数据上载到的 Azure 存储帐户。</td></tr>\n <tr><td>$blobContainerName</td><td>要将数据上载到的 Blob 容器。</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "变量名",
          "pos": [
            28,
            31
          ]
        },
        {
          "content": "说明",
          "pos": [
            40,
            42
          ]
        },
        {
          "content": "$storageAccountName",
          "pos": [
            62,
            81
          ]
        },
        {
          "content": "要将数据上载到的 Azure 存储帐户。",
          "pos": [
            90,
            110
          ]
        },
        {
          "content": "$blobContainerName",
          "pos": [
            130,
            148
          ]
        },
        {
          "content": "要将数据上载到的 Blob 容器。",
          "pos": [
            157,
            174
          ]
        }
      ]
    },
    {
      "content": "打开 Azure PowerShell ISE。",
      "pos": [
        18504,
        18528
      ]
    },
    {
      "content": "将以下脚本粘贴到脚本窗格中：",
      "pos": [
        18532,
        18546
      ]
    },
    {
      "pos": [
        21271,
        21285
      ],
      "content": "按 <bpt id=\"p1\">**</bpt>F5<ept id=\"p1\">**</ept> 运行脚本。"
    },
    {
      "content": "如果你选择使用其他方法上载文件，请确保文件路径是 tutorials/flightdelay/data。用于访问文件的语法是：",
      "pos": [
        21287,
        21350
      ]
    },
    {
      "content": "路径 tutorials/flightdelay/data 是你在上载文件时创建的虚拟文件夹。验证是否有 12 个文件，每个月对应一个文件。",
      "pos": [
        21455,
        21525
      ]
    },
    {
      "pos": [
        21528,
        21562
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>你必须更新 Hive 查询，以从新位置读取。"
    },
    {
      "content": "你必须配置容器访问权限，使其成为公用，或者将存储帐户绑定到 HDInsight 群集。否则，Hive 查询字符串将无法访问数据文件。",
      "pos": [
        21566,
        21632
      ]
    },
    {
      "pos": [
        21640,
        21685
      ],
      "content": "<ph id=\"ph1\">&lt;a id=\"appendix-b\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>附录 B - 创建并上载 HiveQL 脚本"
    },
    {
      "content": "使用 Azure PowerShell，你可以一次运行多个 HiveQL 语句，或者将 HiveQL 语句打包到一个脚本文件中。本部分说明如何创建 HiveQL 脚本，以及使用 Azure PowerShell 将脚本上载到 Azure Blob 存储。Hive 要求 HiveQL 脚本必须存储在 Azure Blob 存储中。",
      "pos": [
        21687,
        21852
      ]
    },
    {
      "content": "HiveQL 脚本将执行以下操作：",
      "pos": [
        21854,
        21871
      ]
    },
    {
      "pos": [
        21876,
        21906
      ],
      "content": "<bpt id=\"p1\">**</bpt>删除 delays\\_raw 表<ept id=\"p1\">**</ept>（如果该表已存在）。"
    },
    {
      "pos": [
        21910,
        22125
      ],
      "content": "<bpt id=\"p1\">**</bpt>创建 delays\\_raw 外部 Hive 表<ept id=\"p1\">**</ept>，并将该表指向航班延误文件所在的 Blob 存储位置。此查询指定用“,”分隔字段并用“\\\\n”终止行。这在字段值包含逗号时将导致出现问题，因为 Hive 无法区分逗号是字段分隔符还是字段值的一部分（在 ORIGIN\\_CITY\\_NAME 和 DEST\\_CITY\\_NAME 的字段值中属于此情况）。为了解决此问题，此查询将创建 TEMP 列来保存未正确拆分到列中的数据。"
    },
    {
      "pos": [
        22131,
        22156
      ],
      "content": "<bpt id=\"p1\">**</bpt>删除 delays 表<ept id=\"p1\">**</ept>（如果该表已存在）。"
    },
    {
      "pos": [
        22160,
        22283
      ],
      "content": "<bpt id=\"p1\">**</bpt>创建 delays 表<ept id=\"p1\">**</ept>。这适用于在进一步处理前清理数据。此查询将从 delays\\_raw 表创建一个新表 <bpt id=\"p2\">*</bpt>delays<ept id=\"p2\">*</ept>。请注意，将不会复制 TEMP 列（如前所述），并且将使用 <bpt id=\"p3\">**</bpt>substring<ept id=\"p3\">**</ept> 函数从数据中删除引号标记。"
    },
    {
      "pos": [
        22287,
        22428
      ],
      "content": "<bpt id=\"p1\">**</bpt>计算平均天气延迟，并按城市名对结果进行分组。<ept id=\"p1\">**</ept> 它还会将结果输出到 Blob 存储。请注意，查询将从数据中删除撇号，并且将排除 <bpt id=\"p2\">**</bpt>weather\\_delay<ept id=\"p2\">**</ept> 的值为 null 的行。由于本教程中稍后使用的 Sqoop 在默认情况下无法适当地处理这些值，因此这是必要的。"
    },
    {
      "pos": [
        22430,
        22501
      ],
      "content": "如需 HiveQL 命令的完整列表，请参阅 <bpt id=\"p1\">[</bpt>Hive 数据定义语言<ept id=\"p1\">][hadoop-hiveql]</ept>。每条 HiveQL 命令必须以分号结尾。"
    },
    {
      "content": "创建 HiveQL 脚本文件",
      "pos": [
        22505,
        22519
      ]
    },
    {
      "content": "准备参数：",
      "pos": [
        22526,
        22531
      ]
    },
    {
      "pos": [
        22537,
        22759
      ],
      "content": "<table border=\"1\">\n <tr><th>变量名</th><th>说明</th></tr>\n <tr><td>$storageAccountName</td><td>要将 HiveQL 脚本上载到的 Azure 存储帐户。</td></tr>\n <tr><td>$blobContainerName</td><td>要将 HiveQL 脚本上载到的 Blob 容器。</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "变量名",
          "pos": [
            28,
            31
          ]
        },
        {
          "content": "说明",
          "pos": [
            40,
            42
          ]
        },
        {
          "content": "$storageAccountName",
          "pos": [
            62,
            81
          ]
        },
        {
          "content": "要将 HiveQL 脚本上载到的 Azure 存储帐户。",
          "pos": [
            90,
            118
          ]
        },
        {
          "content": "$blobContainerName",
          "pos": [
            138,
            156
          ]
        },
        {
          "content": "要将 HiveQL 脚本上载到的 Blob 容器。",
          "pos": [
            165,
            190
          ]
        }
      ]
    },
    {
      "content": "打开 Azure PowerShell ISE。",
      "pos": [
        22763,
        22787
      ]
    },
    {
      "content": "将以下脚本复制并粘贴到脚本窗格中：",
      "pos": [
        22792,
        22809
      ]
    },
    {
      "content": "该脚本中使用了以下变量：",
      "pos": [
        29351,
        29363
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>$hqlLocalFileName<ept id=\"p1\">**</ept> - 该脚本会先将 HiveQL 脚本文件保存在本地，然后才上载到 Blob 存储。这是文件名。默认值是",
      "pos": [
        29371,
        29444
      ]
    },
    {
      "content": "C:\\\\tutorials\\\\flightdelays\\\\flightdelays.hql",
      "pos": [
        29448,
        29493
      ]
    },
    {
      "content": "。",
      "pos": [
        29497,
        29498
      ]
    },
    {
      "pos": [
        29505,
        29685
      ],
      "content": "<bpt id=\"p1\">**</bpt>$hqlBlobName<ept id=\"p1\">**</ept> - 这是 Azure Blob 存储中使用的 HiveQL 脚本文件 Blob 名称。默认值是 tutorials/flightdelay/flightdelays.hql。因为文件会直接写入 Azure Blob 存储，所以 Blob 名称的开头不是“/”。如果你要从 Blob 存储访问文件，必须在文件名的开头添加“/”。"
    },
    {
      "pos": [
        29692,
        29803
      ],
      "content": "**$srcDataFolder** 和 **$dstDataFolder** - = \"tutorials/flightdelay/data\" \n= \"tutorials/flightdelays/output\"",
      "leadings": [
        "",
        "    "
      ],
      "nodes": [
        {
          "content": "<bpt id=\"p1\">**</bpt>$srcDataFolder<ept id=\"p1\">**</ept> 和 <bpt id=\"p2\">**</bpt>$dstDataFolder<ept id=\"p2\">**</ept> - = \"tutorials/flightdelay/data\"",
          "pos": [
            0,
            72
          ]
        },
        {
          "content": "= \"tutorials/flightdelays/output\"",
          "pos": [
            74,
            107
          ]
        }
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a id=\"appendix-c\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>附录 C - 针对 Sqoop 作业输出准备 Azure SQL 数据库",
      "pos": [
        29812,
        29871
      ]
    },
    {
      "content": "准备 SQL 数据库（将此部分与 Sqoop 脚本合并）",
      "pos": [
        29874,
        29902
      ]
    },
    {
      "content": "准备参数：",
      "pos": [
        29909,
        29914
      ]
    },
    {
      "pos": [
        29920,
        30499
      ],
      "content": "<table border=\"1\">\n <tr><th>变量名</th><th>说明</th></tr>\n <tr><td>$sqlDatabaseServerName</td><td>Azure SQL 数据库服务器的名称。不输入任何值会创建新的服务器。</td></tr>\n <tr><td>$sqlDatabaseUsername</td><td>Azure SQL 数据库服务器登录名。如果 $sqlDatabaseServerName 是现有的服务器，登录名和登录密码将用来向服务器进行身份验证。否则会创建新的服务器。</td></tr>\n <tr><td>$sqlDatabasePassword</td><td>Azure SQL 数据库服务器登录密码。</td></tr>\n <tr><td>$sqlDatabaseLocation</td><td>只有在创建新的 Azure 数据库服务器时才会使用此值。</td></tr>\n <tr><td>$sqlDatabaseName</td><td>Sqoop 作业的 AvgDelays 表的 SQL 数据库。保留空白会创建名为 HDISqoop 的数据库。Sqooop 作业输出的表名称为 AvgDelays。</td></tr>\n </table>",
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "nodes": [
        {
          "content": "变量名",
          "pos": [
            28,
            31
          ]
        },
        {
          "content": "说明",
          "pos": [
            40,
            42
          ]
        },
        {
          "content": "$sqlDatabaseServerName",
          "pos": [
            62,
            84
          ]
        },
        {
          "content": "Azure SQL 数据库服务器的名称。不输入任何值会创建新的服务器。",
          "pos": [
            93,
            128
          ]
        },
        {
          "content": "$sqlDatabaseUsername",
          "pos": [
            148,
            168
          ]
        },
        {
          "content": "Azure SQL 数据库服务器登录名。如果 $sqlDatabaseServerName 是现有的服务器，登录名和登录密码将用来向服务器进行身份验证。否则会创建新的服务器。",
          "pos": [
            177,
            264
          ]
        },
        {
          "content": "$sqlDatabasePassword",
          "pos": [
            284,
            304
          ]
        },
        {
          "content": "Azure SQL 数据库服务器登录密码。",
          "pos": [
            313,
            334
          ]
        },
        {
          "content": "$sqlDatabaseLocation",
          "pos": [
            354,
            374
          ]
        },
        {
          "content": "只有在创建新的 Azure 数据库服务器时才会使用此值。",
          "pos": [
            383,
            411
          ]
        },
        {
          "content": "$sqlDatabaseName",
          "pos": [
            431,
            447
          ]
        },
        {
          "content": "Sqoop 作业的 AvgDelays 表的 SQL 数据库。保留空白会创建名为 HDISqoop 的数据库。Sqooop 作业输出的表名称为 AvgDelays。",
          "pos": [
            456,
            538
          ]
        }
      ]
    },
    {
      "content": "打开 Azure PowerShell ISE。",
      "pos": [
        30503,
        30527
      ]
    },
    {
      "content": "将以下脚本复制并粘贴到脚本窗格中：",
      "pos": [
        30531,
        30548
      ]
    },
    {
      "pos": [
        36581,
        36687
      ],
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>该脚本使用具象状态传输 (REST) 服务 http://bot.whatismyipaddress.com 来检索外部 IP 地址。IP 地址用于创建 SQL 数据库服务器的防火墙规则。"
    },
    {
      "content": "该脚本中使用的某些变量：",
      "pos": [
        36693,
        36705
      ]
    },
    {
      "pos": [
        36713,
        36911
      ],
      "content": "<bpt id=\"p1\">**</bpt>$ipAddressRestService<ept id=\"p1\">**</ept> - 默认值为 http://bot.whatismyipaddress.com。这是用来获取外部 IP 地址的公共 IP 地址 REST 服务。如果需要，你可以使用其他服务。使用此服务检索的外部 IP 地址将用于创建 Azure SQL 数据库服务器的防火墙规则，使你能够从工作站访问数据库（通过 Windows PowerShell 脚本）。"
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>$fireWallRuleName<ept id=\"p1\">**</ept> - 这是 Azure SQL 数据库服务器的防火墙规则名称。默认名称为",
      "pos": [
        36918,
        36975
      ]
    },
    {
      "content": "FlightDelay",
      "pos": [
        36979,
        36990
      ]
    },
    {
      "content": "。如果需要，你可以将它重命名。",
      "pos": [
        36994,
        37009
      ]
    },
    {
      "pos": [
        37016,
        37102
      ],
      "content": "<bpt id=\"p1\">**</bpt>$sqlDatabaseMaxSizeGB<ept id=\"p1\">**</ept> - 只有在创建新的 Azure SQL 数据库服务器时才会使用此值。默认值为 10GB。10GB 对于本教程来说已足够。"
    },
    {
      "pos": [
        37109,
        37220
      ],
      "content": "<bpt id=\"p1\">**</bpt>$sqlDatabaseName<ept id=\"p1\">**</ept> - 只有在创建新的 Azure SQL 数据库时才会使用此值。默认值为 HDISqoop。如果将它重命名，则必须相应地更新 Sqoop Windows PowerShell 脚本。"
    },
    {
      "pos": [
        37225,
        37239
      ],
      "content": "按 <bpt id=\"p1\">**</bpt>F5<ept id=\"p1\">**</ept> 运行脚本。"
    },
    {
      "content": "验证脚本输出。确保已成功运行脚本。",
      "pos": [
        37243,
        37260
      ]
    },
    {
      "content": "<ph id=\"ph1\">&lt;a id=\"nextsteps\"&gt;</ph><ph id=\"ph2\">&lt;/a&gt;</ph>后续步骤",
      "pos": [
        37264,
        37290
      ]
    },
    {
      "content": "现在你已了解如何执行以下操作：将文件上载到 Azure Blob 存储、使用 Azure Blob 存储中的数据填充 Hive 表、运行 Hive 查询以及使用 Sqoop 将数据从 HDFS 导出到 Azure SQL 数据库。若要了解更多信息，请参阅下列文章：",
      "pos": [
        37291,
        37423
      ]
    },
    {
      "content": "HDInsight 入门",
      "pos": [
        37428,
        37440
      ]
    },
    {
      "content": "将 Hive 与 HDInsight 配合使用",
      "pos": [
        37468,
        37491
      ]
    },
    {
      "content": "将 Oozie 与 HDInsight 配合使用",
      "pos": [
        37516,
        37540
      ]
    },
    {
      "content": "将 Sqoop 与 HDInsight 配合使用",
      "pos": [
        37566,
        37590
      ]
    },
    {
      "content": "将 Pig 与 HDInsight 配合使用",
      "pos": [
        37616,
        37638
      ]
    },
    {
      "content": "为 HDInsight 开发 Java MapReduce 程序",
      "pos": [
        37662,
        37694
      ]
    },
    {
      "content": "为 HDInsight 开发 C# Hadoop 流式处理程序",
      "pos": [
        37728,
        37759
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"使用 HDInsight 中的 Hadoop 分析航班延误数据 | Azure\"\n    description=\"了解如何使用一个 Windows PowerShell 脚本来创建 HDInsight 群集、运行 Hive 作业、运行 Sqoop 作业和删除群集。\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"mumian\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.date=\"12/01/2015\"\n    wacn.date=\"01/14/2016\"/>\n\n#使用 HDInsight 中的 Hive 分析航班延误数据\n\nHive 提供了通过类似 SQL 的脚本语言（称为 *[HiveQL][hadoop-hiveql]*）运行 Hadoop MapReduce 作业的方法，此方法可用于对大量数据进行汇总、查询和分析。\n\nAzure HDInsight 的主要优势之一就是隔离数据存储和计算。HDInsight 将 Azure Blob 存储用于数据存储。典型的作业包含 3 部分：\n\n1. **将数据存储在 Azure Blob 存储中。** 这可能是一个连续的过程。例如，将天气数据、传感器数据、Web 日志以及此示例中的航班延误数据保存到 Azure Blob 存储中。\n2. **运行作业。** 该处理数据时，你可以运行 Windows PowerShell 脚本（或客户端应用程序）以创建 HDInsight 群集、运行作业，然后删除该群集。作业将输出数据保存到 Azure Blob 存储。甚至在删除该群集后，输出数据也会保留。这样，你仅为已使用的内容付费。\n3. **从 Azure Blob 存储检索输出**，或在此教程中将数据导出到 Azure SQL 数据库。\n\n下图演示了本教程的方案和结构：\n\n![HDI.FlightDelays.flow][img-hdi-flightdelays-flow]\n\n**注意**：图中的编号对应于章节标题。**M** 代表主进程。**A** 代表附录中的内容。\n\n教程的主要部分说明如何使用一个 PowerShell 脚本来执行以下操作：\n\n- 创建 HDInsight 群集。\n- 在群集上运行 Hive 作业，以计算机场的平均延迟。航班延误数据会存储在 Azure Blob 存储帐户中。\n- 运行 Sqoop 作业将 Hive 作业输出导出至 Azure SQL 数据库。\n- 删除 HDInsight 群集。\n\n在附录中，你可以找到有关上载航班延误数据、创建/上载 Hive 查询字符串和针对 Sqoop 作业准备 Azure SQL 数据库的说明。\n<a id=\"prerequisite\"> </a>\n###先决条件\n\n在开始阅读本教程前，你必须具有：\n\n- **一个 Azure 订阅**。请参阅[获取 Azure 试用版](/pricing/1rmb-trial/)。\n\n- **配备 Azure PowerShell 的工作站**。请参阅[安装和使用 Azure PowerShell](/documentation/articles/powershell-install-configure)。\n\n**本教程中使用的文件**\n\n本教程将使用来自[美国研究与技术创新管理部门 - 运输统计局或 RITA][rita-website] 的航班准时表现数据。数据的副本已上载至具有公共 Blob 访问权限的 Azure Blob 存储容器。PowerShell 脚本的一部分将数据从公共 blob 容器复制到你的群集的默认 blob 容器。HiveQL 脚本也会复制到同一 Blob 容器。如果想要了解如何将数据获取/上载到你自己的存储帐户，以及如何创建/上载 HiveQL 脚本文件，请参阅[附录 A](#appendix-a) 和[附录 B](#appendix-b)。\n\n下表列出了本教程中使用的文件：\n\n<table border=\"1\">\n<tr><th>文件</th><th>说明</th></tr>\n<tr><td>wasb://flightdelay@hditutorialdata.blob.core.windows.net/flightdelays.hql</td><td>你要运行的 Hive 作业所用的 HiveQL 脚本文件。此脚本已上载到具有公共访问权限的 Azure Blob 存储帐户。<a href=\"#appendix-b\">附录 B</a> 提供了有关准备此文件以及将其上载到你自己的 Azure Blob 存储帐户的说明。</td></tr>\n<tr><td>wasb://flightdelay@hditutorialdata.blob.core.windows.net/2013Data</td><td>Hive 作业的输入的数据。这些数据已上载到具有公共访问权限的 Azure Blob 存储帐户。<a href=\"#appendix-a\">附录 A</a> 提供了有关获取数据以及将数据上载到你自己的 Azure Blob 存储帐户的说明。</td></tr>\n<tr><td>\\tutorials\\flightdelays\\output</td><td>Hive 作业的输出路径。默认容器用于存储输出数据。</td></tr>\n<tr><td>\\tutorials\\flightdelays\\jobstatus</td><td>默认容器上的 Hive 作业状态文件夹。</td></tr>\n</table>\n\n\n##创建群集并运行 Hive/Sqoop 作业\n\nHadoop MapReduce 属于批处理。运行 Hive 作业时，最具成本效益的方法是为作业创建群集，并在作业完成之后删除作业。以下脚本覆盖了整个过程。有关创建 HDInsight 群集和运行 Hive 作业的详细信息，请参阅[在 HDInsight 中创建 Hadoop 群集][hdinsight-provision]和[将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]。\n\n**使用 Azure PowerShell 运行 Hive 查询**\n\n1. 按照[附录 C](#appendix-c) 中的说明，为 Sqoop 作业输出创建 Azure SQL 数据库和表。\n2. 准备参数：\n\n    <table border=\"1\">\n<tr><th>变量名</th><th>说明</th></tr>\n<tr><td>$hdinsightClusterName</td><td>HDInsight 群集名称。如果群集不存在，脚本会使用输入的名称创建一个群集。</td></tr>\n<tr><td>$storageAccountName</td><td>将用作默认存储帐户的 Azure 存储帐户。只有在脚本需要创建 HDInsight 群集时才需要此值。如果你已为 $hdinsightClusterName 指定了现有的 HDInsight 群集名称，请保留空白。如果具有输入值的存储帐户不存在，则脚本会使用该名称创建帐户。</td></tr>\n<tr><td>$blobContainerName</td><td>将用于默认文件系统的 Blob 容器。如果你将它保留空白，将使用 $hdinsightClusterName 值。</td></tr>\n<tr><td>$sqlDatabaseServerName</td><td>Azure SQL 数据库服务器名称。必须是现有的服务器。有关创建服务器的信息，请参阅<a href=\"#appendix-c\">附录 C</a>。</td></tr>\n<tr><td>$sqlDatabaseUsername</td><td>Azure SQL 数据库服务器登录名。</td></tr>\n<tr><td>$sqlDatabasePassword</td><td>Azure SQL 数据库服务器登录密码。</td></tr>\n<tr><td>$sqlDatabaseName</td><td>Sqoop 将数据导出到的 SQL 数据库。默认名称是 HDISqoop。Sqooop 作业输出的表名称为 AvgDelays。</td></tr>\n</table>\n3. 打开 Windows PowerShell 集成脚本环境 (ISE)。\n4. 将以下脚本复制并粘贴到脚本窗格中：\n\n        [CmdletBinding()]\n        Param(\n\n            # HDInsight cluster variables\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the HDInsight cluster name. If the cluster doesn't exist, the script will create one.\")]\n            [String]$hdinsightClusterName,\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure storage account name for creating a new HDInsight cluster. If the account doesn't exist, the script will create one.\")]\n            [AllowEmptyString()]\n            [String]$storageAccountName,\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure blob container name for creating a new HDInsight cluster. If not specified, the HDInsight cluster name will be used.\")]\n            [AllowEmptyString()]\n            [String]$blobContainerName,\n\n            #SQL database server variables\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure SQL Database Server Name where to export data.\")]\n            [String]$sqlDatabaseServerName,  # specify the Azure SQL database server name where you want to export data to.\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure SQL Database login username.\")]\n            [String]$sqlDatabaseUsername,\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure SQL Database login user password.\")]\n            [String]$sqlDatabasePassword,\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the database name where data will be exported to.\")]\n            [String]$sqlDatabaseName  # the default value is HDISqoop\n        )\n\n        # Treat all errors as terminating\n        $ErrorActionPreference = \"Stop\"\n\n        #region - HDInsight cluster variables\n        [int]$clusterSize = 1                # One data node is sufficient for this tutorial\n        [String]$location = \"China North\"     # For better performance, choose a datacenter near you\n        [String]$hadoopUserLogin = \"admin\"   # Use \"admin\" as the Hadoop login name\n        [String]$hadoopUserpw = \"Pass@word1\" # Use \"Pass@word1\" as the Hadoop login password\n\n        [Bool]$isNewCluster = $false      # Indicates whether a new HDInsight cluster is created by the script  \n                                          # If this variable is true, then the script can optionally delete the cluster after running the Hive and Sqoop jobs\n\n        [Bool]$isNewStorageAccount = $false\n\n        $storageAccountName = $storageAccountName.ToLower() # Storage account names must be between 3 and 24 characters in length and use numbers and lower-case letters only.\n        #endregion\n\n        #region - Hive job variables\n        [String]$hqlScriptFile = \"wasb://flightdelay@hditutorialdata.blob.core.windows.net/flightdelays.hql\" # The HiveQL script is located in a public Blob container. Update this URI if you want to use your own script file.\n\n        [String]$jobStatusFolder = \"/tutorials/flightdelays/jobstatus\" # The script saves both the output data and the job status file to the default container.\n                                                                       # The output data path is set in the HiveQL file.\n\n        #[String]$jobOutputBlobName = \"tutorials/flightdelays/output/000000_0\" # This is the output file of the Hive job. The path is set in the HiveQL script.\n        #endregion\n\n        #region - Sqoop job variables\n        [String]$sqlDatabaseTableName = \"AvgDelays\"\n        [String]$sqlDatabaseConnectionString = \"jdbc:sqlserver://$sqlDatabaseServerName.database.chinacloudapi.cn;user=$sqlDatabaseUserName@$sqlDatabaseServerName;password=$sqlDatabasePassword;database=$sqlDatabaseName\"\n        #endregion Constants and variables\n\n        #region - Connect to Azure subscription\n        Write-Host \"`nConnecting to your Azure subscription ...\" -ForegroundColor Green\n        Write-Host \"`tCurrent system time: \" (get-date) -ForegroundColor Yellow\n        if (-not (Get-AzureAccount)){ Add-AzureAccount -Environment AzureChinaCloud}\n        #endregion\n\n        #region - Validate user input, and provision HDInsight cluster if needed\n        Write-Host \"`nValidating user input ...\" -ForegroundColor Green\n\n        # Both the Azure SQL database server and database must exist\n        if (-not (Get-AzureSqlDatabaseServer|Where-Object{$_.ServerName -eq $sqlDatabaseServerName})){\n            Write-host \"The Azure SQL database server, $sqlDatabaseServerName doesn't exist.\" -ForegroundColor Red\n            Exit\n        }\n        else\n        {\n            if (-not ((Get-AzureSqlDatabase -ServerName $sqlDatabaseServerName)|Where-Object{$_.Name -eq $sqlDatabaseName})){\n                Write-host \"The Azure SQL database, $sqlDatabaseName doesn't exist.\" -ForegroundColor Red\n                Exit\n            }\n        }\n\n        if (Test-AzureName -Service -Name $hdinsightClusterName)     # If it is an existing HDInsight cluster ...\n        {\n            Write-Host \"`tThe HDInsight cluster, $hdinsightClusterName, exists. This cluster will be used to run the Hive job.\" -ForegroundColor Cyan\n\n            #region - Retrieve the default Storage account/container names if the cluster exists\n            # The Hive job output will be stored in the default container. The\n            # information is used to download a copy of the output file from\n            # Blob storage to workstation for the validation purpose.\n            Write-Host \"`nRetrieving the HDInsight cluster default storage account information ...\" `\n                        -ForegroundColor Green\n\n            $hdi = Get-AzureHDInsightCluster -Name $HDInsightClusterName\n\n            # Use the default Storage account and the default container even if the names are different from the user input\n            $storageAccountName = $hdi.DefaultStorageAccount.StorageAccountName `\n                                    -replace \".blob.core.chinacloudapi.cn\"\n            $blobContainerName = $hdi.DefaultStorageAccount.StorageContainerName\n\n            Write-Host \"`tThe default storage account for the cluster is $storageAccountName.\" `\n                        -ForegroundColor Cyan\n            Write-Host \"`tThe default Blob container for the cluster is $blobContainerName.\" `\n                        -ForegroundColor Cyan\n            #endregion\n        }\n        else     #If the cluster doesn't exist, a new one will be provisioned\n        {\n            if ([string]::IsNullOrEmpty($storageAccountName))\n            {\n                Write-Host \"You must provide a storage account name\" -ForegroundColor Red\n                EXit\n            }\n            else\n            {\n                # If the container name is not specified, use the cluster name as the container name\n                if ([string]::IsNullOrEmpty($blobContainerName))\n                {\n                    $blobContainerName = $hdinsightClusterName\n                }\n                $blobContainerName = $blobContainerName.ToLower()\n\n                #region - Provision HDInsight cluster\n                # Create an Azure Storage account if it doesn't exist\n                if (-not (Get-AzureStorageAccount|Where-Object{$_.Label -eq $storageAccountName}))\n                {\n                    Write-Host \"`nCreating the Azure storage account, $storageAccountName ...\" -ForegroundColor Green\n                    if (-not (New-AzureStorageAccount -StorageAccountName $storageAccountName.ToLower() -Location $location)){\n                        Write-Host \"Error creating the storage account, $storageAccountName\" -ForegroundColor Red\n                        Exit\n                    }\n                    $isNewStorageAccount = $True\n                }\n\n                # Create a Blob container used as the default container\n                $storageAccountKey = get-azurestoragekey -StorageAccountName $storageAccountName | %{$_.Primary}\n                $storageContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey\n\n                if (-not (Get-AzureStorageContainer -Context $storageContext |Where-Object{$_.Name -eq $blobContainerName}))\n                {\n                    Write-Host \"`nCreating the Azure Blob container, $blobContainerName ...\" -ForegroundColor Green\n                    if (-not (New-AzureStorageContainer -name $blobContainerName -Context $storageContext)){\n                        Write-Host \"Error creating the Blob container, $blobContainerName\" -ForegroundColor Red\n                        Exit\n                    }\n                }\n\n                # Create a new HDInsight cluster\n                Write-Host \"`nProvisioning the HDInsight cluster, $hdinsightClusterName ...\" -ForegroundColor Green\n                Write-Host \"`tCurrent system time: \" (get-date) -ForegroundColor Yellow\n                $hadoopUserPassword = ConvertTo-SecureString -String $hadoopUserpw -AsPlainText -Force\n                $credential = New-Object System.Management.Automation.PSCredential($hadoopUserLogin,$hadoopUserPassword)\n                if (-not $credential)\n                {\n                    Write-Host \"Error creating the PSCredential object\" -ForegroundColor Red\n                    Exit\n                }\n\n                if (-not (New-AzureHDInsightCluster -Name $hdinsightClusterName -Location $location -Credential $credential -DefaultStorageAccountName \"$storageAccountName.blob.core.chinacloudapi.cn\" -DefaultStorageAccountKey $storageAccountKey -DefaultStorageContainerName $blobContainerName -ClusterSizeInNodes $clusterSize)){\n                    Write-Host \"Error provisioning the cluster, $hdinsightClusterName.\" -ForegroundColor Red\n                    Exit\n                }\n                Else\n                {\n                    $isNewCluster = $True\n                }\n                #endregion\n            }\n        }\n        #endregion\n\n        #region - Submit Hive job\n        Write-Host \"`nSubmitting the Hive job ...\" -ForegroundColor Green\n        Write-Host \"`tCurrent system time: \" (get-date) -ForegroundColor Yellow\n\n        Use-AzureHDInsightCluster $HDInsightClusterName\n        $response = Invoke-Hive –File $hqlScriptFile -StatusFolder $jobStatusFolder\n\n        Write-Host \"`nThe Hive job status\" -ForegroundColor Cyan\n        Write-Host \"---------------------------------------------------------\" -ForegroundColor Cyan\n        write-Host $response\n        Write-Host \"---------------------------------------------------------\" -ForegroundColor Cyan\n        #endregion\n\n        #region - Run Sqoop job\n        Write-Host \"`nSubmitting the Sqoop job ...\" -ForegroundColor Green\n        Write-Host \"`tCurrent system time: \" (get-date) -ForegroundColor Yellow\n\n        [String]$exportDir = \"wasb://$blobContainerName@$storageAccountName.blob.core.chinacloudapi.cn/tutorials/flightdelays/output\"\n\n\n        $sqoopDef = New-AzureHDInsightSqoopJobDefinition -Command \"export --connect $sqlDatabaseConnectionString --table $sqlDatabaseTableName --export-dir $exportDir --fields-terminated-by \\001 \"\n        $sqoopJob = Start-AzureHDInsightJob -Cluster $hdinsightClusterName -JobDefinition $sqoopDef #-Debug -Verbose\n        Wait-AzureHDInsightJob -WaitTimeoutInSeconds 3600 -Job $sqoopJob\n\n        Write-Host \"Standard Error\" -BackgroundColor Green\n        Get-AzureHDInsightJobOutput -Cluster $hdinsightClusterName -JobId $sqoopJob.JobId -StandardError\n        Write-Host \"Standard Output\" -BackgroundColor Green\n        Get-AzureHDInsightJobOutput -Cluster $hdinsightClusterName -JobId $sqoopJob.JobId -StandardOutput\n        #endregion\n\n        #region - Delete the HDInsight cluster\n        if ($isNewCluster -eq $True)\n        {\n            $isDelete = Read-Host 'Do you want to delete the HDInsight Hadoop cluster ' $hdinsightClusterName '? (Y/N)'\n\n            if ($isDelete.ToLower() -eq \"y\")\n            {\n                Write-Host \"`nDeleting the HDInsight cluster ...\" -ForegroundColor Green\n                Write-Host \"`tCurrent system time: \" (get-date) -ForegroundColor Yellow\n                Remove-AzureHDInsightCluster -Name $hdinsightClusterName\n            }\n        }\n        #endregion\n\n        #region - Delete the Storage account\n        if ($isNewStorageAccount -eq $True)\n        {\n            $isDelete = Read-Host 'Do you want to delete the Azure storage account ' $storageAccountName '? (Y/N)'\n\n            if ($isDelete.ToLower() -eq \"y\")\n            {\n                Write-Host \"`nDeleting the Azure storage account ...\" -ForegroundColor Green\n                Write-Host \"`tCurrent system time: \" (get-date) -ForegroundColor Yellow\n                Remove-AzureStorageAccount -StorageAccountName $storageAccountName\n            }\n        }\n        #endregion\n\n        Write-Host \"End of the PowerShell script\" -ForegroundColor Green\n        Write-Host \"`tCurrent system time: \" (get-date) -ForegroundColor Yellow\n\n5. 按 **F5** 运行脚本。输出应如下所示：\n\n    ![HDI.FlightDelays.RunHiveJob.output][img-hdi-flightdelays-run-hive-job-output]\n\n6. 连接到 SQL 数据库，并在 AvgDelays 表中按城市查看平均航班延迟：\n\n    ![HDI.FlightDelays.AvgDelays.Dataset][image-hdi-flightdelays-avgdelays-dataset]\n\n\n\n---\n##<a id=\"appendix-a\"></a>附录 A - 将航班延误数据上载到 Azure Blob 存储\n上载数据文件和 HiveQL 脚本文件（请参阅[附录 B](#appendix-b)）需要进行规划。思路是在创建 HDInsight 群集和运行 Hive 作业之前存储数据文件和 HiveQL 文件。可以使用两个选项：\n\n- **使用将由 HDInsight 群集用作默认文件系统的同一 Azure 存储帐户。** 由于 HDInsight 群集将具有存储帐户访问密钥，因此你无需进行任何其他更改。\n- **使用与 HDInsight 群集默认文件系统不同的 Azure 存储帐户。** 如果选择了此项，你必须修改[创建 HDInsight 群集和运行 Hive/Sqoop 作业](#runjob)中的 Windows PowerShell 脚本的创建部分，以链接该存储帐户作为额外的存储帐户。有关说明，请参阅[在 HDInsight 中创建 Hadoop 群集][hdinsight-provision]。这样，HDInsight 群集就会知道存储帐户的访问密钥。\n\n>[AZURE.NOTE]数据文件的 WASB 路径会在 HiveQL 脚本文件中进行硬编码。你必须相应地更新该路径。\n\n**下载航班数据**\n\n1. 浏览到[美国研究与技术创新管理部门 - 运输统计局][rita-website]。\n2. 在该页面上，选择以下值：\n\n    <table border=\"1\">\n    <tr><th>Name</th><th>值</th></tr>\n    <tr><td>筛选年份</td><td>2013 </td></tr>\n    <tr><td>筛选期间</td><td>1 月</td></tr>\n    <tr><td>字段</td><td>*Year*、*FlightDate*、*UniqueCarrier*、*Carrier*、*FlightNum*、*OriginAirportID*、*Origin*、*OriginCityName*、*OriginState*、*DestAirportID*、*Dest*、*DestCityName*、*DestState*、*DepDelayMinutes*、*ArrDelay*、*ArrDelayMinutes*、*CarrierDelay*、*WeatherDelay*、*NASDelay*、*SecurityDelay*、*LateAircraftDelay*（清除其他所有字段）</td></tr>\n    </table>\n\n3. 单击“下载”。\n4. 将文件解压缩到 **C:\\\\Tutorials\\\\FlightDelays\\\\Data** 文件夹。每个文件均为 CSV 文件且大小约为 60GB。\n5.  将文件重命名为其包含的数据所对应的月份的名称。例如，将包含 1 月份数据的文件命名为 *January.csv*。\n6. 重复步骤 2 和步骤 5 为 2013 年中的 12 个月分别下载一个对应的文件。完成本教程到少要有一个文件。  \n\n**将航班延迟数据上载到 Azure Blob 存储**\n\n1. 准备参数：\n\n    <table border=\"1\">\n    <tr><th>变量名</th><th>说明</th></tr>\n    <tr><td>$storageAccountName</td><td>要将数据上载到的 Azure 存储帐户。</td></tr>\n    <tr><td>$blobContainerName</td><td>要将数据上载到的 Blob 容器。</td></tr>\n    </table>\n2. 打开 Azure PowerShell ISE。\n3. 将以下脚本粘贴到脚本窗格中：\n\n        [CmdletBinding()]\n        Param(\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure storage account name for creating a new HDInsight cluster. If the account doesn't exist, the script will create one.\")]\n            [String]$storageAccountName,\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure blob container name for creating a new HDInsight cluster. If not specified, the HDInsight cluster name will be used.\")]\n            [String]$blobContainerName\n        )\n\n        #Region - Variables\n        $localFolder = \"C:\\Tutorials\\FlightDelays\\Data\"  # The source folder\n        $destFolder = \"tutorials/flightdelays/data\"     #The blob name prefix for the files to be uploaded\n        #EndRegion\n\n        #Region - Connect to Azure subscription\n        Write-Host \"`nConnecting to your Azure subscription ...\" -ForegroundColor Green\n        if (-not (Get-AzureAccount)){ Add-AzureAccount -Environment AzureChinaCloud}\n        #EndRegion\n\n        #Region - Validate user input\n        # Validate the Storage account\n        if (-not (Get-AzureStorageAccount|Where-Object{$_.Label -eq $storageAccountName}))\n        {\n            Write-Host \"The storage account, $storageAccountName, doesn't exist.\" -ForegroundColor Red\n            exit\n        }\n\n        # Validate the container\n        $storageAccountKey = get-azurestoragekey -StorageAccountName $storageAccountName | %{$_.Primary}\n        $storageContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey\n\n        if (-not (Get-AzureStorageContainer -Context $storageContext |Where-Object{$_.Name -eq $blobContainerName}))\n        {\n            Write-Host \"The Blob container, $blobContainerName, doesn't exist\" -ForegroundColor Red\n            Exit\n        }\n        #EngRegion\n\n        #Region - Copy the file from local workstation to Azure Blob storage  \n        if (test-path -Path $localFolder)\n        {\n            foreach ($item in Get-ChildItem -Path $localFolder){\n                $fileName = \"$localFolder\\$item\"\n                $blobName = \"$destFolder/$item\"\n\n                Write-Host \"Copying $fileName to $blobName\" -ForegroundColor Green\n\n                Set-AzureStorageBlobContent -File $fileName -Container $blobContainerName -Blob $blobName -Context $storageContext\n            }\n        }\n        else\n        {\n            Write-Host \"The source folder on the workstation doesn't exist\" -ForegroundColor Red\n        }\n\n        # List the uploaded files on HDInsight\n        Get-AzureStorageBlob -Container $blobContainerName  -Context $storageContext -Prefix $destFolder\n        #EndRegion\n\n\n\n\n4. 按 **F5** 运行脚本。\n\n如果你选择使用其他方法上载文件，请确保文件路径是 tutorials/flightdelay/data。用于访问文件的语法是：\n\n    wasb://<ContainerName>@<StorageAccountName>.blob.core.chinacloudapi.cn/tutorials/flightdelay/data\n\n路径 tutorials/flightdelay/data 是你在上载文件时创建的虚拟文件夹。验证是否有 12 个文件，每个月对应一个文件。\n\n>[AZURE.NOTE]你必须更新 Hive 查询，以从新位置读取。\n\n> 你必须配置容器访问权限，使其成为公用，或者将存储帐户绑定到 HDInsight 群集。否则，Hive 查询字符串将无法访问数据文件。\n\n---\n##<a id=\"appendix-b\"></a>附录 B - 创建并上载 HiveQL 脚本\n\n使用 Azure PowerShell，你可以一次运行多个 HiveQL 语句，或者将 HiveQL 语句打包到一个脚本文件中。本部分说明如何创建 HiveQL 脚本，以及使用 Azure PowerShell 将脚本上载到 Azure Blob 存储。Hive 要求 HiveQL 脚本必须存储在 Azure Blob 存储中。\n\nHiveQL 脚本将执行以下操作：\n\n1. **删除 delays\\_raw 表**（如果该表已存在）。\n2. **创建 delays\\_raw 外部 Hive 表**，并将该表指向航班延误文件所在的 Blob 存储位置。此查询指定用“,”分隔字段并用“\\\\n”终止行。这在字段值包含逗号时将导致出现问题，因为 Hive 无法区分逗号是字段分隔符还是字段值的一部分（在 ORIGIN\\_CITY\\_NAME 和 DEST\\_CITY\\_NAME 的字段值中属于此情况）。为了解决此问题，此查询将创建 TEMP 列来保存未正确拆分到列中的数据。  \n3. **删除 delays 表**（如果该表已存在）。\n4. **创建 delays 表**。这适用于在进一步处理前清理数据。此查询将从 delays\\_raw 表创建一个新表 *delays*。请注意，将不会复制 TEMP 列（如前所述），并且将使用 **substring** 函数从数据中删除引号标记。\n5. **计算平均天气延迟，并按城市名对结果进行分组。** 它还会将结果输出到 Blob 存储。请注意，查询将从数据中删除撇号，并且将排除 **weather\\_delay** 的值为 null 的行。由于本教程中稍后使用的 Sqoop 在默认情况下无法适当地处理这些值，因此这是必要的。\n\n如需 HiveQL 命令的完整列表，请参阅 [Hive 数据定义语言][hadoop-hiveql]。每条 HiveQL 命令必须以分号结尾。\n\n**创建 HiveQL 脚本文件**\n\n1. 准备参数：\n\n    <table border=\"1\">\n    <tr><th>变量名</th><th>说明</th></tr>\n    <tr><td>$storageAccountName</td><td>要将 HiveQL 脚本上载到的 Azure 存储帐户。</td></tr>\n    <tr><td>$blobContainerName</td><td>要将 HiveQL 脚本上载到的 Blob 容器。</td></tr>\n    </table>\n2. 打开 Azure PowerShell ISE。\n\n3. 将以下脚本复制并粘贴到脚本窗格中：\n\n        [CmdletBinding()]\n        Param(\n\n            # Azure Blob storage variables\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure storage account name for creating a new HDInsight cluster. If the account doesn't exist, the script will create one.\")]\n            [String]$storageAccountName,\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure blob container name for creating a new HDInsight cluster. If not specified, the HDInsight cluster name will be used.\")]\n            [String]$blobContainerName\n\n        )\n\n        #region - Define variables\n        # Treat all errors as terminating\n        $ErrorActionPreference = \"Stop\"\n\n        # The HiveQL script file is exported as this file before it's uploaded to Blob storage\n        $hqlLocalFileName = \"C:\\tutorials\\flightdelays\\flightdelays.hql\"\n\n        # The HiveQL script file will be uploaded to Blob storage as this blob name\n        $hqlBlobName = \"tutorials/flightdelays/flightdelays.hql\"\n\n        # These two constants are used by the HiveQL script file\n        #$srcDataFolder = \"tutorials/flightdelays/data\"\n        $dstDataFolder = \"/tutorials/flightdelays/output\"\n        #endregion\n\n        #region - Validate the file and file path\n\n        # Check if a file with the same file name already exists on the workstation\n        Write-Host \"`nvalidating the folder structure on the workstation for saving the HQL script file ...\"  -ForegroundColor Green\n        if (test-path $hqlLocalFileName){\n\n            $isDelete = Read-Host 'The file, ' $hqlLocalFileName ', exists.  Do you want to overwirte it? (Y/N)'\n\n            if ($isDelete.ToLower() -ne \"y\")\n            {\n                Exit\n            }\n        }\n\n        # Create the folder if it doesn't exist\n        $folder = split-path $hqlLocalFileName\n        if (-not (test-path $folder))\n        {\n            Write-Host \"`nCreating folder, $folder ...\" -ForegroundColor Green\n\n            new-item $folder -ItemType directory  \n        }\n        #end region\n\n        #region - Add the Azure account\n        Write-Host \"`nConnecting to your Azure subscription ...\" -ForegroundColor Green\n        $azureAccounts= Get-AzureAccount\n        if (! $azureAccounts)\n        {\n            Add-AzureAccount -Environment AzureChinaCloud\n        }\n        #endregion\n\n        #region - Write the Hive script into a local file\n        Write-Host \"`nWriting the Hive script into a file on your workstation ...\" `\n                    -ForegroundColor Green\n\n        $hqlDropDelaysRaw = \"DROP TABLE delays_raw;\"\n\n        $hqlCreateDelaysRaw = \"CREATE EXTERNAL TABLE delays_raw (\" +\n                \"YEAR string, \" +\n                \"FL_DATE string, \" +\n                \"UNIQUE_CARRIER string, \" +\n                \"CARRIER string, \" +\n                \"FL_NUM string, \" +\n                \"ORIGIN_AIRPORT_ID string, \" +\n                \"ORIGIN string, \" +\n                \"ORIGIN_CITY_NAME string, \" +\n                \"ORIGIN_CITY_NAME_TEMP string, \" +\n                \"ORIGIN_STATE_ABR string, \" +\n                \"DEST_AIRPORT_ID string, \" +\n                \"DEST string, \" +\n                \"DEST_CITY_NAME string, \" +\n                \"DEST_CITY_NAME_TEMP string, \" +\n                \"DEST_STATE_ABR string, \" +\n                \"DEP_DELAY_NEW float, \" +\n                \"ARR_DELAY_NEW float, \" +\n                \"CARRIER_DELAY float, \" +\n                \"WEATHER_DELAY float, \" +\n                \"NAS_DELAY float, \" +\n                \"SECURITY_DELAY float, \" +\n                \"LATE_AIRCRAFT_DELAY float) \" +\n            \"ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \" +\n            \"LINES TERMINATED BY '\\n' \" +\n            \"STORED AS TEXTFILE \" +\n            \"LOCATION 'wasb://flightdelay@hditutorialdata.blob.core.windows.net/2013Data';\"\n\n        $hqlDropDelays = \"DROP TABLE delays;\"\n\n        $hqlCreateDelays = \"CREATE TABLE delays AS \" +\n            \"SELECT YEAR AS year, \" +\n                \"FL_DATE AS flight_date, \" +\n                \"substring(UNIQUE_CARRIER, 2, length(UNIQUE_CARRIER) -1) AS unique_carrier, \" +\n                \"substring(CARRIER, 2, length(CARRIER) -1) AS carrier, \" +\n                \"substring(FL_NUM, 2, length(FL_NUM) -1) AS flight_num, \" +\n                \"ORIGIN_AIRPORT_ID AS origin_airport_id, \" +\n                \"substring(ORIGIN, 2, length(ORIGIN) -1) AS origin_airport_code, \" +\n                \"substring(ORIGIN_CITY_NAME, 2) AS origin_city_name, \" +\n                \"substring(ORIGIN_STATE_ABR, 2, length(ORIGIN_STATE_ABR) -1)  AS origin_state_abr, \" +\n                \"DEST_AIRPORT_ID AS dest_airport_id, \" +\n                \"substring(DEST, 2, length(DEST) -1) AS dest_airport_code, \" +\n                \"substring(DEST_CITY_NAME,2) AS dest_city_name, \" +\n                \"substring(DEST_STATE_ABR, 2, length(DEST_STATE_ABR) -1) AS dest_state_abr, \" +\n                \"DEP_DELAY_NEW AS dep_delay_new, \" +\n                \"ARR_DELAY_NEW AS arr_delay_new, \" +\n                \"CARRIER_DELAY AS carrier_delay, \" +\n                \"WEATHER_DELAY AS weather_delay, \" +\n                \"NAS_DELAY AS nas_delay, \" +\n                \"SECURITY_DELAY AS security_delay, \" +\n                \"LATE_AIRCRAFT_DELAY AS late_aircraft_delay \" +\n            \"FROM delays_raw;\"\n\n        $hqlInsertLocal = \"INSERT OVERWRITE DIRECTORY '$dstDataFolder' \" +\n            \"SELECT regexp_replace(origin_city_name, '''', ''), \" +\n                \"avg(weather_delay) \" +\n            \"FROM delays \" +\n            \"WHERE weather_delay IS NOT NULL \" +\n            \"GROUP BY origin_city_name;\"\n\n        $hqlScript = $hqlDropDelaysRaw + $hqlCreateDelaysRaw + $hqlDropDelays + $hqlCreateDelays + $hqlInsertLocal\n\n        $hqlScript | Out-File $hqlLocalFileName -Encoding ascii -Force\n        #endregion\n\n        #region - Upload the Hive script to the default Blob container\n        Write-Host \"`nUploading the Hive script to the default Blob container ...\" -ForegroundColor Green\n\n        # Create a storage context object\n        $storageAccountKey = get-azurestoragekey $storageAccountName | %{$_.Primary}\n        $destContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey\n\n        # Upload the file from local workstation to Blob storage\n        Set-AzureStorageBlobContent -File $hqlLocalFileName -Container $blobContainerName -Blob $hqlBlobName -Context $destContext\n        #endregion\n\n        Write-host \"`nEnd of the PowerShell script\" -ForegroundColor Green\n\n    该脚本中使用了以下变量：\n\n    - **$hqlLocalFileName** - 该脚本会先将 HiveQL 脚本文件保存在本地，然后才上载到 Blob 存储。这是文件名。默认值是 <u>C:\\\\tutorials\\\\flightdelays\\\\flightdelays.hql</u>。\n    - **$hqlBlobName** - 这是 Azure Blob 存储中使用的 HiveQL 脚本文件 Blob 名称。默认值是 tutorials/flightdelay/flightdelays.hql。因为文件会直接写入 Azure Blob 存储，所以 Blob 名称的开头不是“/”。如果你要从 Blob 存储访问文件，必须在文件名的开头添加“/”。\n    - **$srcDataFolder** 和 **$dstDataFolder** - = \"tutorials/flightdelay/data\" \n    = \"tutorials/flightdelays/output\"\n\n\n---\n##<a id=\"appendix-c\"></a>附录 C - 针对 Sqoop 作业输出准备 Azure SQL 数据库\n**准备 SQL 数据库（将此部分与 Sqoop 脚本合并）**\n\n1. 准备参数：\n\n    <table border=\"1\">\n    <tr><th>变量名</th><th>说明</th></tr>\n    <tr><td>$sqlDatabaseServerName</td><td>Azure SQL 数据库服务器的名称。不输入任何值会创建新的服务器。</td></tr>\n    <tr><td>$sqlDatabaseUsername</td><td>Azure SQL 数据库服务器登录名。如果 $sqlDatabaseServerName 是现有的服务器，登录名和登录密码将用来向服务器进行身份验证。否则会创建新的服务器。</td></tr>\n    <tr><td>$sqlDatabasePassword</td><td>Azure SQL 数据库服务器登录密码。</td></tr>\n    <tr><td>$sqlDatabaseLocation</td><td>只有在创建新的 Azure 数据库服务器时才会使用此值。</td></tr>\n    <tr><td>$sqlDatabaseName</td><td>Sqoop 作业的 AvgDelays 表的 SQL 数据库。保留空白会创建名为 HDISqoop 的数据库。Sqooop 作业输出的表名称为 AvgDelays。</td></tr>\n    </table>\n2. 打开 Azure PowerShell ISE。\n3. 将以下脚本复制并粘贴到脚本窗格中：\n\n        [CmdletBinding()]\n        Param(\n\n            # SQL database server variables\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure SQL Database Server Name to use an existing one. Enter nothing to create a new one.\")]\n            [AllowEmptyString()]\n            [String]$sqlDatabaseServer,  # Specify the Azure SQL database server name if you have one created. Otherwise use \"\".\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure SQL Database admin user.\")]\n            [String]$sqlDatabaseUsername,\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the Azure SQL Database admin user password.\")]\n            [String]$sqlDatabasePassword,\n\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the region to create the Database in.\")]\n            [AllowEmptyString()]\n            [String]$sqlDatabaseLocation,   #For example, China North.\n\n            # SQL database variables\n            [Parameter(Mandatory=$True,\n                       HelpMessage=\"Enter the database name if you have created one. Enter nothing to create one.\")]\n            [AllowEmptyString()]\n            [String]$sqlDatabaseName # specify the database name if you have one created. Otherwise use \"\" to have the script create one for you.\n        )\n\n        # Treat all errors as terminating\n        $ErrorActionPreference = \"Stop\"\n\n        #region - Constants and variables\n\n        # IP address REST service used for retrieving external IP address and creating firewall rules\n        [String]$ipAddressRestService = \"http://bot.whatismyipaddress.com\"\n        [String]$fireWallRuleName = \"FlightDelay\"\n\n        # SQL database variables\n        [String]$sqlDatabaseMaxSizeGB = 10\n\n        #SQL query string for creating AvgDelays table\n        [String]$sqlDatabaseTableName = \"AvgDelays\"\n        [String]$sqlCreateAvgDelaysTable = \" CREATE TABLE [dbo].[$sqlDatabaseTableName](\n                    [origin_city_name] [nvarchar](50) NOT NULL,\n                    [weather_delay] float,\n                CONSTRAINT [PK_$sqlDatabaseTableName] PRIMARY KEY CLUSTERED\n                (\n                    [origin_city_name] ASC\n                )\n                )\"\n        #endregion\n\n        #region - Add the Azure account\n        Write-Host \"`nConnecting to your Azure subscription ...\" -ForegroundColor Green\n        $azureAccounts= Get-AzureAccount\n        if (! $azureAccounts)\n        {\n        Add-AzureAccount -Environment AzureChinaCloud\n        }\n        #endregion\n\n        #region - Create and validate Azure SQL database server\n        if ([string]::IsNullOrEmpty($sqlDatabaseServer))\n        {\n            Write-Host \"`nCreating SQL Database server ...\"  -ForegroundColor Green\n            $sqlDatabaseServer = (New-AzureSqlDatabaseServer -AdministratorLogin $sqlDatabaseUsername -AdministratorLoginPassword $sqlDatabasePassword -Location $sqlDatabaseLocation).ServerName\n            Write-Host \"`tThe new SQL database server name is $sqlDatabaseServer.\" -ForegroundColor Cyan\n\n            Write-Host \"`nCreating firewall rule, $fireWallRuleName ...\" -ForegroundColor Green\n            $workstationIPAddress = Invoke-RestMethod $ipAddressRestService\n            New-AzureSqlDatabaseServerFirewallRule -ServerName $sqlDatabaseServer -RuleName \"$fireWallRuleName-workstation\" -StartIpAddress $workstationIPAddress -EndIpAddress $workstationIPAddress\n            New-AzureSqlDatabaseServerFirewallRule -ServerName $sqlDatabaseServer -RuleName \"$fireWallRuleName-Azureservices\" -AllowAllAzureServices\n        }\n        else\n        {\n            $dbServer = Get-AzureSqlDatabaseServer -ServerName $sqlDatabaseServer\n            if (! $dbServer)\n            {\n                throw \"The Azure SQL database server, $sqlDatabaseServer, doesn't exist!\"\n            }\n            else\n            {\n                Write-Host \"`nUse an existing SQL Database server, $sqlDatabaseServer\" -ForegroundColor Green\n            }\n        }\n        #endregion\n\n        #region - Create and validate Azure SQL database\n        if ([string]::IsNullOrEmpty($sqlDatabaseName))\n        {\n            Write-Host \"`nCreating SQL Database, HDISqoop ...\"  -ForegroundColor Green\n\n            $sqlDatabaseName = \"HDISqoop\"\n            $sqlDatabaseServerCredential = new-object System.Management.Automation.PSCredential($sqlDatabaseUsername, ($sqlDatabasePassword  | ConvertTo-SecureString -asPlainText -Force))\n\n            $sqlDatabaseServerConnectionContext = New-AzureSqlDatabaseServerContext -ServerName $sqlDatabaseServer -Credential $sqlDatabaseServerCredential\n\n            $sqlDatabase = New-AzureSqlDatabase -ConnectionContext $sqlDatabaseServerConnectionContext -DatabaseName $sqlDatabaseName -MaxSizeGB $sqlDatabaseMaxSizeGB\n        }\n        else\n        {\n            $db = Get-AzureSqlDatabase -ServerName $sqlDatabaseServer -DatabaseName $sqlDatabaseName\n            if (! $db)\n            {\n                throw \"The Azure SQL database server, $sqlDatabaseServer, doesn't exist!\"\n            }\n            else\n            {\n                Write-Host \"`nUse an existing SQL Database, $sqlDatabaseName\" -ForegroundColor Green\n            }\n        }\n        #endregion\n\n        #region -  Execute an SQL command to create the AvgDelays table\n\n        Write-Host \"`nCreating SQL Database table ...\"  -ForegroundColor Green\n        $conn = New-Object System.Data.SqlClient.SqlConnection\n        $conn.ConnectionString = \"Data Source=$sqlDatabaseServer.database.chinacloudapi.cn;Initial Catalog=$sqlDatabaseName;User ID=$sqlDatabaseUsername;Password=$sqlDatabasePassword;Encrypt=true;Trusted_Connection=false;\"\n        $conn.open()\n        $cmd = New-Object System.Data.SqlClient.SqlCommand\n        $cmd.connection = $conn\n        $cmd.commandtext = $sqlCreateAvgDelaysTable\n        $cmd.executenonquery()\n\n        $conn.close()\n\n        Write-host \"`nEnd of the PowerShell script\" -ForegroundColor Green\n\n    >[AZURE.NOTE]该脚本使用具象状态传输 (REST) 服务 http://bot.whatismyipaddress.com 来检索外部 IP 地址。IP 地址用于创建 SQL 数据库服务器的防火墙规则。\n\n    该脚本中使用的某些变量：\n\n    - **$ipAddressRestService** - 默认值为 http://bot.whatismyipaddress.com。这是用来获取外部 IP 地址的公共 IP 地址 REST 服务。如果需要，你可以使用其他服务。使用此服务检索的外部 IP 地址将用于创建 Azure SQL 数据库服务器的防火墙规则，使你能够从工作站访问数据库（通过 Windows PowerShell 脚本）。\n    - **$fireWallRuleName** - 这是 Azure SQL 数据库服务器的防火墙规则名称。默认名称为 <u>FlightDelay</u>。如果需要，你可以将它重命名。\n    - **$sqlDatabaseMaxSizeGB** - 只有在创建新的 Azure SQL 数据库服务器时才会使用此值。默认值为 10GB。10GB 对于本教程来说已足够。\n    - **$sqlDatabaseName** - 只有在创建新的 Azure SQL 数据库时才会使用此值。默认值为 HDISqoop。如果将它重命名，则必须相应地更新 Sqoop Windows PowerShell 脚本。\n\n4. 按 **F5** 运行脚本。\n5. 验证脚本输出。确保已成功运行脚本。\n\n##<a id=\"nextsteps\"></a>后续步骤\n现在你已了解如何执行以下操作：将文件上载到 Azure Blob 存储、使用 Azure Blob 存储中的数据填充 Hive 表、运行 Hive 查询以及使用 Sqoop 将数据从 HDFS 导出到 Azure SQL 数据库。若要了解更多信息，请参阅下列文章：\n\n* [HDInsight 入门][hdinsight-get-started]\n* [将 Hive 与 HDInsight 配合使用][hdinsight-use-hive]\n* [将 Oozie 与 HDInsight 配合使用][hdinsight-use-oozie]\n* [将 Sqoop 与 HDInsight 配合使用][hdinsight-use-sqoop]\n* [将 Pig 与 HDInsight 配合使用][hdinsight-use-pig]\n* [为 HDInsight 开发 Java MapReduce 程序][hdinsight-develop-mapreduce]\n* [为 HDInsight 开发 C# Hadoop 流式处理程序][hdinsight-develop-streaming]\n\n\n\n[azure-purchase-options]: /pricing/overview/\n[azure-member-offers]: /pricing/member-offers/\n[azure-trial]: /pricing/1rmb-trial/\n\n\n[rita-website]: http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time\n[powershell-install-configure]: /documentation/articles/powershell-install-configure\n\n[hdinsight-use-oozie]: /documentation/articles/hdinsight-use-oozie\n[hdinsight-use-hive]: /documentation/articles/hdinsight-use-hive\n[hdinsight-provision]: /documentation/articles/hdinsight-provision-clusters-v1\n[hdinsight-storage]: /documentation/articles/hdinsight-hadoop-use-blob-storage\n[hdinsight-upload-data]: /documentation/articles/hdinsight-upload-data\n[hdinsight-get-started]: /documentation/articles/hdinsight-hadoop-tutorial-get-started-windows-v1\n[hdinsight-use-sqoop]: /documentation/articles/hdinsight-use-sqoop\n[hdinsight-use-pig]: /documentation/articles/hdinsight-use-pig\n[hdinsight-develop-streaming]: /documentation/articles/hdinsight-hadoop-develop-deploy-streaming-jobs\n[hdinsight-develop-mapreduce]: /documentation/articles/hdinsight-develop-deploy-java-mapreduce\n\n[hadoop-hiveql]: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL\n[hadoop-shell-commands]: http://hadoop.apache.org/docs/r0.18.3/hdfs_shell.html\n\n[technetwiki-hive-error]: http://social.technet.microsoft.com/wiki/contents/articles/23047.hdinsight-hive-error-unable-to-rename.aspx\n\n[image-hdi-flightdelays-avgdelays-dataset]: ./media/hdinsight-analyze-flight-delay-data/HDI.FlightDelays.AvgDelays.DataSet.png\n[img-hdi-flightdelays-run-hive-job-output]: ./media/hdinsight-analyze-flight-delay-data/HDI.FlightDelays.RunHiveJob.Output.png\n[img-hdi-flightdelays-flow]: ./media/hdinsight-analyze-flight-delay-data/HDI.FlightDelays.Flow.png\n\n<!---HONumber=Mooncake_1207_2015-->"
}